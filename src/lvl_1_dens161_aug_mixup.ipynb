{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import typing as tp\n",
    "import pathlib\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_speech_features as psf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"../data\")\n",
    "audios_path = data_path / \"all_audio_resampled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path/'Train.csv')\n",
    "train_extra = pd.read_csv(data_path/'train_add.csv')\n",
    "train_extra_2 = pd.read_csv(data_path/'train_add_20201029.csv')\n",
    "\n",
    "label2code = {word: idx for idx, word in enumerate(train.label.unique().tolist())}\n",
    "code2label = {v:k for k,v in label2code.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(audio_path):\n",
    "    file_name = audio_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    ip = str(audios_path.resolve() / f\"{file_name}.wav\")\n",
    "    return ip\n",
    "\n",
    "train[\"image_fn\"] = train.fn.apply(get_image_path)\n",
    "train_extra[\"image_fn\"] = train_extra.fn.apply(get_image_path)\n",
    "train_extra_2[\"image_fn\"] = train_extra_2.fn.apply(get_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train, train_extra, train_extra_2], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build validation that includes all classes:\n",
    "\n",
    "\n",
    "vcs = train_df.label.value_counts()\n",
    "\n",
    "## possible schema:\n",
    "# 25+ - take 3\n",
    "# 12-25 - take 2\n",
    "# 12- - take 1\n",
    "\n",
    "def num_for_val(num_examples):\n",
    "    if num_examples >= 25:\n",
    "        return 3\n",
    "    if num_examples >= 12:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "train_df[\"num_examples\"] = train_df.label.map(vcs.to_dict())\n",
    "train_df[\"num_for_val\"] = train_df.num_examples.apply(num_for_val)\n",
    "\n",
    "random.seed(12)\n",
    "train_df_new = pd.DataFrame()\n",
    "for label in train_df.label.unique():\n",
    "    tmp = train_df.loc[train_df.label == label].copy()\n",
    "    tmp[\"dummy\"] = tmp.label.apply(lambda _: random.random())\n",
    "    tmp.sort_values(by=\"dummy\", inplace=True)\n",
    "    tmp[\"rank\"] = range(tmp.shape[0])\n",
    "    train_df_new = pd.concat([train_df_new, tmp])\n",
    "\n",
    "train_df_new.reset_index(drop=True, inplace=True)\n",
    "train_df_new[\"val_subset\"] = train_df_new.num_for_val > train_df_new[\"rank\"]\n",
    "train_df_new.drop(\"dummy\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    set(train_df_new.loc[train_df_new.val_subset].label.unique()) == \n",
    "    set(train_df_new.loc[~train_df_new.val_subset].label.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new[\"val_fold\"] = train_df_new[\"rank\"] // train_df_new.num_for_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 451\n",
      "1 451\n",
      "2 451\n",
      "3 430\n",
      "4 413\n",
      "5 410\n",
      "6 407\n",
      "7 391\n",
      "8 381\n",
      "9 337\n"
     ]
    }
   ],
   "source": [
    "for f in range(10):\n",
    "    print(f, (train_df_new.val_fold == f).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 3\n",
    "train_folds = list()\n",
    "val_folds = list()\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    valf = train_df_new.loc[train_df_new.val_fold == i].copy()\n",
    "    trf = train_df_new.loc[train_df_new.val_fold != i].copy()\n",
    "    \n",
    "    train_folds.append(trf)\n",
    "    val_folds.append(valf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import new_generate_spec, new_build_image, normalize, MEAN, STD\n",
    "from transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    n_fft = 512\n",
    "    hop_size = 32\n",
    "    pad_center = True\n",
    "    trim = True\n",
    "    max_len_sec = 2.6\n",
    "    sr = 22050\n",
    "    img_size = 299\n",
    "    \n",
    "conf = AudioConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_noise = AddNoise(0, 0.07)\n",
    "# aug_ts = TimeStretch((0.5, 2))\n",
    "aug_pitch = PitchShift((-5, 5), sr=conf.sr)\n",
    "\n",
    "train_transforms = UseWithProb(\n",
    "    OneOf([\n",
    "        aug_noise,\n",
    "        aug_pitch\n",
    "    ]),\n",
    "    prob=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: tp.List[tp.List[str]],\n",
    "        config,\n",
    "        transform=None,\n",
    "        normalize=True\n",
    "    ):\n",
    "        self.file_list = file_list  # list of list: [file_path, label]\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn, word = self.file_list[idx]\n",
    "        audio, _ = librosa.core.load(fn, sr=SR)\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        image = new_build_image(audio, self.config)\n",
    "        \n",
    "        if self.normalize:\n",
    "            norm_image = normalize(np.array(image), mean=MEAN, std=STD)\n",
    "        else:\n",
    "            norm_image = image\n",
    "        \n",
    "        return np.moveaxis(norm_image, 2, 0), label2code[word]\n",
    "    \n",
    "    \n",
    "class SpectrogramTestDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: tp.List[tp.List[str]],\n",
    "        config,\n",
    "        transform=None,\n",
    "        normalize=True\n",
    "    ):\n",
    "        self.file_list = file_list  # list of list: [file_path, label]\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn, word = self.file_list[idx]\n",
    "        audio, _ = librosa.core.load(fn, sr=SR)\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        image = new_build_image(audio, self.config)\n",
    "        \n",
    "        if self.normalize:\n",
    "            norm_image = normalize(np.array(image), mean=MEAN, std=STD)\n",
    "        else:\n",
    "            norm_image = image\n",
    "        \n",
    "        return np.moveaxis(norm_image, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258\n",
      "4258\n",
      "4258\n",
      "=========================\n",
      "451\n",
      "451\n",
      "451\n"
     ]
    }
   ],
   "source": [
    "sdf_train_list = [\n",
    "    SpectrogramDataset(t[[\"image_fn\", \"label\"]].values.tolist(), conf,\n",
    "                       transform=train_transforms, normalize=True)\n",
    "    for t in train_folds\n",
    "]\n",
    "\n",
    "sdf_val_list = [\n",
    "    SpectrogramDataset(v[[\"image_fn\", \"label\"]].values.tolist(), conf, normalize=True)\n",
    "    for v in val_folds\n",
    "]\n",
    "\n",
    "for s in sdf_train_list:\n",
    "    print(len(s))\n",
    "    \n",
    "print(\"=========================\")\n",
    "\n",
    "for s in sdf_val_list:\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def train_mixup_epoch(log_interval, mixup_prob, model, device, criterion, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        use_mixup = False\n",
    "        if random.random() < mixup_prob:\n",
    "            use_mixup = True\n",
    "        data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "        \n",
    "        if use_mixup:\n",
    "            data, y_a, y_b, lam = mixup_data(data, target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        if use_mixup:\n",
    "            loss = mixup_criterion(criterion, output, y_a, y_b, lam) #criterion(output, target)\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "\n",
    "            \n",
    "def test(model, device, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def train_on_fold(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    fold,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    results_folder,\n",
    "    epochs\n",
    "):\n",
    "    seed_dict = {\n",
    "        0: 9,\n",
    "        1: 99,\n",
    "        2: 999\n",
    "    }\n",
    "    set_seed(seed_dict[fold])\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    best_loss = 1e5\n",
    "    best_acc = 0\n",
    "\n",
    "    max_patience = 20\n",
    "    patience = 0\n",
    "\n",
    "    train_loss_hist = list()\n",
    "    val_loss_hist = list()\n",
    "    val_acc_hist = list()\n",
    "\n",
    "    save_each_epoch = False\n",
    "\n",
    "    for ep in range(65):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.667, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{fold}_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "\n",
    "        lr_scheduler.step(cur_loss) \n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "        \n",
    "    for ep in range(13):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.0, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{fold}_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "\n",
    "        lr_scheduler.step(cur_loss)\n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "\n",
    "    print(\"time spent training: {} minutes\".format((time.time() - t0) / 60))\n",
    "    print(\"BEST LOSS:\", best_loss)\n",
    "    print(\"BEST ACC:\", best_acc)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"tmp/{results_folder}/best_run_{fold}.pth\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_on_val(\n",
    "    model,\n",
    "    val_loader,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, (inputs, _) in enumerate(val_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs))\n",
    "\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_val_outputs(model, test_loader, device=\"cuda\"):\n",
    "    outputs_list = list()\n",
    "    for batch_idx, (inputs, target) in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            outputs_list.append(outputs)\n",
    "    outputs_list = np.concatenate([t.cpu().numpy() for t in outputs_list])\n",
    "    return outputs_list\n",
    "\n",
    "def get_predictions(model, test_loader, device=\"cuda\"):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, inputs in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs)) ## ADD SOFTMAX\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft.load_state_dict(torch.load(f\"tmp/{tmp_folder_name}/best_run_{FOLD}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FOLD 0\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 84.513947\n",
      "\n",
      "Test set: Average loss: 5.1194, Accuracy: 7/451 (2%)\n",
      "\n",
      "Training so far 2.05168004433314 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 77.594330\n",
      "\n",
      "Test set: Average loss: 4.9568, Accuracy: 10/451 (2%)\n",
      "\n",
      "Training so far 4.0801282326380415 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 76.812241\n",
      "\n",
      "Test set: Average loss: 3.8566, Accuracy: 55/451 (12%)\n",
      "\n",
      "Training so far 6.1255564053853355 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 62.665424\n",
      "\n",
      "Test set: Average loss: 2.8535, Accuracy: 146/451 (32%)\n",
      "\n",
      "Training so far 8.157116504510244 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 45.454788\n",
      "\n",
      "Test set: Average loss: 2.4045, Accuracy: 184/451 (41%)\n",
      "\n",
      "Training so far 10.184798630078634 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 29.335335\n",
      "\n",
      "Test set: Average loss: 1.9216, Accuracy: 244/451 (54%)\n",
      "\n",
      "Training so far 12.212976503372193 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 65.264496\n",
      "\n",
      "Test set: Average loss: 1.6596, Accuracy: 264/451 (59%)\n",
      "\n",
      "Training so far 14.238748053709665 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 49.639431\n",
      "\n",
      "Test set: Average loss: 1.4577, Accuracy: 286/451 (63%)\n",
      "\n",
      "Training so far 16.266295731067657 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 22.618101\n",
      "\n",
      "Test set: Average loss: 1.3686, Accuracy: 298/451 (66%)\n",
      "\n",
      "Training so far 18.2956493973732 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 17.790592\n",
      "\n",
      "Test set: Average loss: 1.1476, Accuracy: 335/451 (74%)\n",
      "\n",
      "Training so far 20.320795758565268 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 17.853928\n",
      "\n",
      "Test set: Average loss: 0.9977, Accuracy: 335/451 (74%)\n",
      "\n",
      "Training so far 22.347669275601707 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 28.843874\n",
      "\n",
      "Test set: Average loss: 0.9400, Accuracy: 349/451 (77%)\n",
      "\n",
      "Training so far 24.368058021863302 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 16.102177\n",
      "\n",
      "Test set: Average loss: 0.9368, Accuracy: 354/451 (78%)\n",
      "\n",
      "Training so far 26.394407490889233 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 44.028294\n",
      "\n",
      "Test set: Average loss: 0.8158, Accuracy: 363/451 (80%)\n",
      "\n",
      "Training so far 28.426644353071847 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 58.970184\n",
      "\n",
      "Test set: Average loss: 0.7794, Accuracy: 372/451 (82%)\n",
      "\n",
      "Training so far 30.45402261018753 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 45.545422\n",
      "\n",
      "Test set: Average loss: 0.8445, Accuracy: 361/451 (80%)\n",
      "\n",
      "Training so far 32.47611858844757 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 38.190235\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 370/451 (82%)\n",
      "\n",
      "Training so far 34.53548266887665 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 35.809071\n",
      "\n",
      "Test set: Average loss: 0.7121, Accuracy: 373/451 (83%)\n",
      "\n",
      "Training so far 36.56951361497243 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 11.493342\n",
      "\n",
      "Test set: Average loss: 0.6303, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 38.59114790757497 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 34.721443\n",
      "\n",
      "Test set: Average loss: 0.6093, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 40.60746099948883 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 45.861008\n",
      "\n",
      "Test set: Average loss: 0.6560, Accuracy: 375/451 (83%)\n",
      "\n",
      "Training so far 42.6234935760498 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 51.185097\n",
      "\n",
      "Test set: Average loss: 0.6497, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 44.64631262222926 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 50.500221\n",
      "\n",
      "Test set: Average loss: 0.6424, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 46.6739900747935 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 30.552151\n",
      "\n",
      "Test set: Average loss: 0.6360, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 48.693272189299265 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 27.467640\n",
      "\n",
      "Test set: Average loss: 0.6096, Accuracy: 383/451 (85%)\n",
      "\n",
      "Epoch    25: reducing learning rate of group 0 to 1.5000e-04.\n",
      "Training so far 50.716145988305406 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 3.582231\n",
      "\n",
      "Test set: Average loss: 0.5577, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 52.74596153100332 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 46.608879\n",
      "\n",
      "Test set: Average loss: 0.5449, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 54.75903237660726 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 35.433430\n",
      "\n",
      "Test set: Average loss: 0.6449, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 56.78046742280324 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 2.235106\n",
      "\n",
      "Test set: Average loss: 0.5240, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 58.820919088522594 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 9.304804\n",
      "\n",
      "Test set: Average loss: 0.5325, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 60.88946971098582 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 32.962753\n",
      "\n",
      "Test set: Average loss: 0.5519, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 62.95982434352239 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 35.275040\n",
      "\n",
      "Test set: Average loss: 0.5669, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 65.0226529677709 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 29.159946\n",
      "\n",
      "Test set: Average loss: 0.6158, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 67.1016347805659 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 5.015431\n",
      "\n",
      "Test set: Average loss: 0.5119, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 69.17864552736282 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 13.431585\n",
      "\n",
      "Test set: Average loss: 0.6029, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 71.24896233876547 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 0.802454\n",
      "\n",
      "Test set: Average loss: 0.5647, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 73.31317706108094 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 1.380364\n",
      "\n",
      "Test set: Average loss: 0.5465, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 75.36318885087967 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 32.016167\n",
      "\n",
      "Test set: Average loss: 0.5466, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 77.41172734896342 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 14.076872\n",
      "\n",
      "Test set: Average loss: 0.5939, Accuracy: 393/451 (87%)\n",
      "\n",
      "Epoch    39: reducing learning rate of group 0 to 7.5000e-05.\n",
      "Training so far 79.48595025539399 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 34.042900\n",
      "\n",
      "Test set: Average loss: 0.5671, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 81.55019572575887 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 26.452131\n",
      "\n",
      "Test set: Average loss: 0.5474, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 83.6088506658872 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 0.229455\n",
      "\n",
      "Test set: Average loss: 0.5063, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 85.6577811996142 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 35.946354\n",
      "\n",
      "Test set: Average loss: 0.5627, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 87.68796321948369 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 0.179872\n",
      "\n",
      "Test set: Average loss: 0.5516, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 89.70782757600149 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 0.162563\n",
      "\n",
      "Test set: Average loss: 0.6370, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 91.73161877393723 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 1.639184\n",
      "\n",
      "Test set: Average loss: 0.5286, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 93.75798076788584 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 0.379607\n",
      "\n",
      "Test set: Average loss: 0.5332, Accuracy: 398/451 (88%)\n",
      "\n",
      "Epoch    47: reducing learning rate of group 0 to 3.7500e-05.\n",
      "Training so far 95.79923612674078 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 30.913609\n",
      "\n",
      "Test set: Average loss: 0.5343, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 97.85009686946869 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 19.123293\n",
      "\n",
      "Test set: Average loss: 0.5301, Accuracy: 404/451 (90%)\n",
      "\n",
      "Training so far 99.90884263515473 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 39.048981\n",
      "\n",
      "Test set: Average loss: 0.4968, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 101.9752784371376 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 0.398329\n",
      "\n",
      "Test set: Average loss: 0.5307, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 104.04954788684844 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 41.120525\n",
      "\n",
      "Test set: Average loss: 0.5057, Accuracy: 401/451 (89%)\n",
      "\n",
      "Training so far 106.12460040648779 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 0.134623\n",
      "\n",
      "Test set: Average loss: 0.5114, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 108.21576320330301 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 4.236512\n",
      "\n",
      "Test set: Average loss: 0.5162, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 110.28395759661993 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 19.618124\n",
      "\n",
      "Test set: Average loss: 0.5136, Accuracy: 403/451 (89%)\n",
      "\n",
      "Epoch    55: reducing learning rate of group 0 to 1.8750e-05.\n",
      "Training so far 112.33400932947795 minutes\n",
      "====================\n",
      "Train Epoch: 55 [0/4258 (0%)]\tLoss: 19.805288\n",
      "\n",
      "Test set: Average loss: 0.4966, Accuracy: 404/451 (90%)\n",
      "\n",
      "Training so far 114.37134552399317 minutes\n",
      "====================\n",
      "Train Epoch: 56 [0/4258 (0%)]\tLoss: 33.314045\n",
      "\n",
      "Test set: Average loss: 0.5146, Accuracy: 405/451 (90%)\n",
      "\n",
      "Training so far 116.41065649191539 minutes\n",
      "====================\n",
      "Train Epoch: 57 [0/4258 (0%)]\tLoss: 10.945335\n",
      "\n",
      "Test set: Average loss: 0.4896, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 118.46916250785192 minutes\n",
      "====================\n",
      "Train Epoch: 58 [0/4258 (0%)]\tLoss: 0.196527\n",
      "\n",
      "Test set: Average loss: 0.5008, Accuracy: 404/451 (90%)\n",
      "\n",
      "Training so far 120.5004044731458 minutes\n",
      "====================\n",
      "Train Epoch: 59 [0/4258 (0%)]\tLoss: 25.503864\n",
      "\n",
      "Test set: Average loss: 0.4887, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 122.534583902359 minutes\n",
      "====================\n",
      "Train Epoch: 60 [0/4258 (0%)]\tLoss: 24.446430\n",
      "\n",
      "Test set: Average loss: 0.5008, Accuracy: 401/451 (89%)\n",
      "\n",
      "Training so far 124.56227591435115 minutes\n",
      "====================\n",
      "Train Epoch: 61 [0/4258 (0%)]\tLoss: 0.233645\n",
      "\n",
      "Test set: Average loss: 0.4913, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 126.58368581533432 minutes\n",
      "====================\n",
      "Train Epoch: 62 [0/4258 (0%)]\tLoss: 28.892624\n",
      "\n",
      "Test set: Average loss: 0.5010, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 128.61023501952488 minutes\n",
      "====================\n",
      "Train Epoch: 63 [0/4258 (0%)]\tLoss: 22.156767\n",
      "\n",
      "Test set: Average loss: 0.5030, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 130.63590906858445 minutes\n",
      "====================\n",
      "Train Epoch: 64 [0/4258 (0%)]\tLoss: 23.116663\n",
      "\n",
      "Test set: Average loss: 0.5246, Accuracy: 399/451 (88%)\n",
      "\n",
      "Epoch    65: reducing learning rate of group 0 to 1.5000e-05.\n",
      "Training so far 132.65655082464218 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.631363\n",
      "\n",
      "Test set: Average loss: 0.4771, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 134.6826119184494 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 0.260781\n",
      "\n",
      "Test set: Average loss: 0.4641, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 136.70790056387582 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.904509\n",
      "\n",
      "Test set: Average loss: 0.4595, Accuracy: 404/451 (90%)\n",
      "\n",
      "Training so far 138.72668938239414 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.150473\n",
      "\n",
      "Test set: Average loss: 0.4717, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 140.74257893959683 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 3.556612\n",
      "\n",
      "Test set: Average loss: 0.4723, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 142.76336737076443 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.123135\n",
      "\n",
      "Test set: Average loss: 0.4644, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 144.78946920235953 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.164531\n",
      "\n",
      "Test set: Average loss: 0.4645, Accuracy: 401/451 (89%)\n",
      "\n",
      "Training so far 146.8216034889221 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.169460\n",
      "\n",
      "Test set: Average loss: 0.4805, Accuracy: 401/451 (89%)\n",
      "\n",
      "Training so far 148.8562312444051 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.254746\n",
      "\n",
      "Test set: Average loss: 0.4705, Accuracy: 401/451 (89%)\n",
      "\n",
      "Training so far 150.88605847756068 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.399669\n",
      "\n",
      "Test set: Average loss: 0.4803, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 152.90917832454045 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 0.092031\n",
      "\n",
      "Test set: Average loss: 0.4886, Accuracy: 406/451 (90%)\n",
      "\n",
      "Training so far 154.92439352671306 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 0.306622\n",
      "\n",
      "Test set: Average loss: 0.4809, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 156.94477043151855 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 0.424944\n",
      "\n",
      "Test set: Average loss: 0.4741, Accuracy: 405/451 (90%)\n",
      "\n",
      "Training so far 158.96915738185245 minutes\n",
      "====================\n",
      "time spent training: 158.96915771166485 minutes\n",
      "BEST LOSS: 0.4594666478110523\n",
      "BEST ACC: 89.57871396895787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/letfoolsdie/virtual_envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/letfoolsdie/virtual_envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:134: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FOLD 1\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 83.752060\n",
      "\n",
      "Test set: Average loss: 4.8791, Accuracy: 13/451 (3%)\n",
      "\n",
      "Training so far 2.0217489361763 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 82.367775\n",
      "\n",
      "Test set: Average loss: 3.9367, Accuracy: 48/451 (11%)\n",
      "\n",
      "Training so far 4.046665239334106 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 67.364754\n",
      "\n",
      "Test set: Average loss: 2.9569, Accuracy: 109/451 (24%)\n",
      "\n",
      "Training so far 6.073913860321045 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 73.416534\n",
      "\n",
      "Test set: Average loss: 2.3332, Accuracy: 210/451 (47%)\n",
      "\n",
      "Training so far 8.099962306022643 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 33.652168\n",
      "\n",
      "Test set: Average loss: 1.9101, Accuracy: 259/451 (57%)\n",
      "\n",
      "Training so far 10.123463082313538 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 35.991966\n",
      "\n",
      "Test set: Average loss: 2.0869, Accuracy: 232/451 (51%)\n",
      "\n",
      "Training so far 12.145927826563517 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 28.120262\n",
      "\n",
      "Test set: Average loss: 1.4968, Accuracy: 302/451 (67%)\n",
      "\n",
      "Training so far 14.166549062728881 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 52.414391\n",
      "\n",
      "Test set: Average loss: 1.5435, Accuracy: 292/451 (65%)\n",
      "\n",
      "Training so far 16.185982330640158 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 55.779404\n",
      "\n",
      "Test set: Average loss: 1.2962, Accuracy: 311/451 (69%)\n",
      "\n",
      "Training so far 18.21221826473872 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 22.484814\n",
      "\n",
      "Test set: Average loss: 1.1159, Accuracy: 332/451 (74%)\n",
      "\n",
      "Training so far 20.2381529211998 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 52.402187\n",
      "\n",
      "Test set: Average loss: 1.1079, Accuracy: 332/451 (74%)\n",
      "\n",
      "Training so far 22.264372404416402 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 30.201843\n",
      "\n",
      "Test set: Average loss: 1.0237, Accuracy: 344/451 (76%)\n",
      "\n",
      "Training so far 24.293047058582307 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 47.210770\n",
      "\n",
      "Test set: Average loss: 1.0349, Accuracy: 357/451 (79%)\n",
      "\n",
      "Training so far 26.31544247070948 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 6.199940\n",
      "\n",
      "Test set: Average loss: 0.9661, Accuracy: 347/451 (77%)\n",
      "\n",
      "Training so far 28.338641544183094 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 48.722359\n",
      "\n",
      "Test set: Average loss: 0.9392, Accuracy: 354/451 (78%)\n",
      "\n",
      "Training so far 30.363254328568775 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 7.704682\n",
      "\n",
      "Test set: Average loss: 0.8578, Accuracy: 363/451 (80%)\n",
      "\n",
      "Training so far 32.39307967027028 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 4.087886\n",
      "\n",
      "Test set: Average loss: 0.9074, Accuracy: 361/451 (80%)\n",
      "\n",
      "Training so far 34.41169286966324 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 1.851863\n",
      "\n",
      "Test set: Average loss: 0.8201, Accuracy: 362/451 (80%)\n",
      "\n",
      "Training so far 36.43447527885437 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 28.474121\n",
      "\n",
      "Test set: Average loss: 0.7917, Accuracy: 374/451 (83%)\n",
      "\n",
      "Training so far 38.46105833848318 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 17.502619\n",
      "\n",
      "Test set: Average loss: 0.7438, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 40.48427631855011 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 27.733994\n",
      "\n",
      "Test set: Average loss: 0.8325, Accuracy: 366/451 (81%)\n",
      "\n",
      "Training so far 42.510159154733024 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 44.421951\n",
      "\n",
      "Test set: Average loss: 0.8152, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 44.52000959714254 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 34.458153\n",
      "\n",
      "Test set: Average loss: 0.7794, Accuracy: 375/451 (83%)\n",
      "\n",
      "Training so far 46.53377230962118 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 47.887329\n",
      "\n",
      "Test set: Average loss: 0.7927, Accuracy: 372/451 (82%)\n",
      "\n",
      "Training so far 48.55084065993627 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 40.223141\n",
      "\n",
      "Test set: Average loss: 0.7154, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 50.58021051088969 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 19.243149\n",
      "\n",
      "Test set: Average loss: 0.7723, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 52.5941481312116 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 5.479860\n",
      "\n",
      "Test set: Average loss: 0.6998, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 54.608156534036 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 2.661150\n",
      "\n",
      "Test set: Average loss: 0.7363, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 56.63311589956284 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 37.842899\n",
      "\n",
      "Test set: Average loss: 0.8034, Accuracy: 375/451 (83%)\n",
      "\n",
      "Training so far 58.65029018322627 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 19.972725\n",
      "\n",
      "Test set: Average loss: 0.7584, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 60.66432066361109 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 36.809982\n",
      "\n",
      "Test set: Average loss: 0.8119, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 62.68520758946737 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 0.870624\n",
      "\n",
      "Test set: Average loss: 0.6969, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 64.70033651987711 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 7.057161\n",
      "\n",
      "Test set: Average loss: 0.7071, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 66.72012865146002 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 48.498653\n",
      "\n",
      "Test set: Average loss: 0.7292, Accuracy: 379/451 (84%)\n",
      "\n",
      "Training so far 68.73529040416082 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 2.174268\n",
      "\n",
      "Test set: Average loss: 0.7032, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 70.76482606331507 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 44.246422\n",
      "\n",
      "Test set: Average loss: 0.7880, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 72.78642828067144 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 2.776739\n",
      "\n",
      "Test set: Average loss: 0.7985, Accuracy: 377/451 (84%)\n",
      "\n",
      "Epoch    37: reducing learning rate of group 0 to 1.5000e-04.\n",
      "Training so far 74.80785477956137 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 3.450446\n",
      "\n",
      "Test set: Average loss: 0.7137, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 76.82885672648747 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 3.843274\n",
      "\n",
      "Test set: Average loss: 0.6999, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 78.85214524269104 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 30.964441\n",
      "\n",
      "Test set: Average loss: 0.7077, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 80.8667659163475 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 3.796134\n",
      "\n",
      "Test set: Average loss: 0.6938, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 82.87922639846802 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 30.580837\n",
      "\n",
      "Test set: Average loss: 0.7747, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 84.90067853530248 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 17.219219\n",
      "\n",
      "Test set: Average loss: 0.7422, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 86.91464337905248 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 37.999195\n",
      "\n",
      "Test set: Average loss: 0.7924, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 88.93893071810405 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 37.248642\n",
      "\n",
      "Test set: Average loss: 0.7131, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 90.95652054548263 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 43.372780\n",
      "\n",
      "Test set: Average loss: 0.7368, Accuracy: 389/451 (86%)\n",
      "\n",
      "Epoch    46: reducing learning rate of group 0 to 7.5000e-05.\n",
      "Training so far 92.97078356742858 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 26.530537\n",
      "\n",
      "Test set: Average loss: 0.7424, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 94.99617705345153 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 34.087646\n",
      "\n",
      "Test set: Average loss: 0.7595, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 97.01324499845505 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 7.874869\n",
      "\n",
      "Test set: Average loss: 0.6793, Accuracy: 394/451 (87%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training so far 99.03923189242681 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 8.997572\n",
      "\n",
      "Test set: Average loss: 0.7523, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 101.05741116205851 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 0.204779\n",
      "\n",
      "Test set: Average loss: 0.7019, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 103.08222225109736 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 35.280273\n",
      "\n",
      "Test set: Average loss: 0.6980, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 105.10726848045985 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 12.349792\n",
      "\n",
      "Test set: Average loss: 0.7615, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 107.12340687115987 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 0.096382\n",
      "\n",
      "Test set: Average loss: 0.6592, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 109.15424285332362 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 0.337754\n",
      "\n",
      "Test set: Average loss: 0.6812, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 111.17146476507187 minutes\n",
      "====================\n",
      "Train Epoch: 55 [0/4258 (0%)]\tLoss: 0.431167\n",
      "\n",
      "Test set: Average loss: 0.6886, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 113.18970869382223 minutes\n",
      "====================\n",
      "Train Epoch: 56 [0/4258 (0%)]\tLoss: 13.584216\n",
      "\n",
      "Test set: Average loss: 0.7045, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 115.21259454488754 minutes\n",
      "====================\n",
      "Train Epoch: 57 [0/4258 (0%)]\tLoss: 2.935764\n",
      "\n",
      "Test set: Average loss: 0.7213, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 117.23500219186147 minutes\n",
      "====================\n",
      "Train Epoch: 58 [0/4258 (0%)]\tLoss: 30.176842\n",
      "\n",
      "Test set: Average loss: 0.7456, Accuracy: 389/451 (86%)\n",
      "\n",
      "Epoch    59: reducing learning rate of group 0 to 3.7500e-05.\n",
      "Training so far 119.2539215405782 minutes\n",
      "====================\n",
      "Train Epoch: 59 [0/4258 (0%)]\tLoss: 5.916347\n",
      "\n",
      "Test set: Average loss: 0.7647, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 121.26197150945663 minutes\n",
      "====================\n",
      "Train Epoch: 60 [0/4258 (0%)]\tLoss: 21.783731\n",
      "\n",
      "Test set: Average loss: 0.6555, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 123.28540510733923 minutes\n",
      "====================\n",
      "Train Epoch: 61 [0/4258 (0%)]\tLoss: 0.101268\n",
      "\n",
      "Test set: Average loss: 0.7466, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 125.30520261128744 minutes\n",
      "====================\n",
      "Train Epoch: 62 [0/4258 (0%)]\tLoss: 15.648458\n",
      "\n",
      "Test set: Average loss: 0.6864, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 127.32809895674387 minutes\n",
      "====================\n",
      "Train Epoch: 63 [0/4258 (0%)]\tLoss: 4.087198\n",
      "\n",
      "Test set: Average loss: 0.6520, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 129.34224344491957 minutes\n",
      "====================\n",
      "Train Epoch: 64 [0/4258 (0%)]\tLoss: 25.168667\n",
      "\n",
      "Test set: Average loss: 0.6256, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 131.36653343439102 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.064427\n",
      "\n",
      "Test set: Average loss: 0.6354, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 133.3861536304156 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 2.115660\n",
      "\n",
      "Test set: Average loss: 0.6645, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 135.4074770530065 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 2.000403\n",
      "\n",
      "Test set: Average loss: 0.6231, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 137.43260521888732 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.140406\n",
      "\n",
      "Test set: Average loss: 0.6734, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 139.44409677584966 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.110993\n",
      "\n",
      "Test set: Average loss: 0.6849, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 141.46346865495045 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.222377\n",
      "\n",
      "Test set: Average loss: 0.6726, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 143.48681512276332 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.077904\n",
      "\n",
      "Test set: Average loss: 0.6970, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 145.50419605970382 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.042724\n",
      "\n",
      "Test set: Average loss: 0.6785, Accuracy: 399/451 (88%)\n",
      "\n",
      "Epoch    73: reducing learning rate of group 0 to 1.8750e-05.\n",
      "Training so far 147.5162816286087 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.157823\n",
      "\n",
      "Test set: Average loss: 0.6844, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 149.54210956494015 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.049775\n",
      "\n",
      "Test set: Average loss: 0.6990, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 151.55498272975285 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 1.324579\n",
      "\n",
      "Test set: Average loss: 0.7092, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 153.56839013894398 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 0.445502\n",
      "\n",
      "Test set: Average loss: 0.6924, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 155.58886730273565 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 0.052704\n",
      "\n",
      "Test set: Average loss: 0.6976, Accuracy: 398/451 (88%)\n",
      "\n",
      "Epoch    78: reducing learning rate of group 0 to 1.5000e-05.\n",
      "Training so far 157.61032441854476 minutes\n",
      "====================\n",
      "time spent training: 157.61032471259435 minutes\n",
      "BEST LOSS: 0.6230971125303509\n",
      "BEST ACC: 88.470066518847\n",
      "PROCESSING FOLD 2\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 84.305298\n",
      "\n",
      "Test set: Average loss: 5.2821, Accuracy: 6/451 (1%)\n",
      "\n",
      "Training so far 2.0131044665972393 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 78.807884\n",
      "\n",
      "Test set: Average loss: 4.1648, Accuracy: 40/451 (9%)\n",
      "\n",
      "Training so far 4.0425327777862545 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 74.694580\n",
      "\n",
      "Test set: Average loss: 3.1329, Accuracy: 122/451 (27%)\n",
      "\n",
      "Training so far 6.072503276666006 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 55.050354\n",
      "\n",
      "Test set: Average loss: 2.4658, Accuracy: 197/451 (44%)\n",
      "\n",
      "Training so far 8.088485006491343 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 40.738045\n",
      "\n",
      "Test set: Average loss: 2.0428, Accuracy: 264/451 (59%)\n",
      "\n",
      "Training so far 10.112334887186686 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 27.005226\n",
      "\n",
      "Test set: Average loss: 1.5911, Accuracy: 302/451 (67%)\n",
      "\n",
      "Training so far 12.145156915982565 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 18.505949\n",
      "\n",
      "Test set: Average loss: 1.4116, Accuracy: 314/451 (70%)\n",
      "\n",
      "Training so far 14.170808887481689 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 37.483147\n",
      "\n",
      "Test set: Average loss: 1.2955, Accuracy: 323/451 (72%)\n",
      "\n",
      "Training so far 16.19719905455907 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 8.493700\n",
      "\n",
      "Test set: Average loss: 1.2485, Accuracy: 330/451 (73%)\n",
      "\n",
      "Training so far 18.219221568107606 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 25.504978\n",
      "\n",
      "Test set: Average loss: 1.1349, Accuracy: 339/451 (75%)\n",
      "\n",
      "Training so far 20.247611848513284 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 21.521374\n",
      "\n",
      "Test set: Average loss: 1.0282, Accuracy: 347/451 (77%)\n",
      "\n",
      "Training so far 22.276876986026764 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 18.391037\n",
      "\n",
      "Test set: Average loss: 1.0835, Accuracy: 349/451 (77%)\n",
      "\n",
      "Training so far 24.2951180378596 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 40.567936\n",
      "\n",
      "Test set: Average loss: 0.9921, Accuracy: 353/451 (78%)\n",
      "\n",
      "Training so far 26.31449464162191 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 46.057045\n",
      "\n",
      "Test set: Average loss: 0.9483, Accuracy: 366/451 (81%)\n",
      "\n",
      "Training so far 28.332726812362672 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 20.268909\n",
      "\n",
      "Test set: Average loss: 0.8701, Accuracy: 367/451 (81%)\n",
      "\n",
      "Training so far 30.361097367604575 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 12.319081\n",
      "\n",
      "Test set: Average loss: 0.8815, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 32.38610376119614 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 7.139819\n",
      "\n",
      "Test set: Average loss: 0.9845, Accuracy: 353/451 (78%)\n",
      "\n",
      "Training so far 34.403864828745526 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 27.860308\n",
      "\n",
      "Test set: Average loss: 0.8934, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 36.427821477254234 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 28.473270\n",
      "\n",
      "Test set: Average loss: 0.8277, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 38.45468186934789 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 42.717319\n",
      "\n",
      "Test set: Average loss: 0.9610, Accuracy: 366/451 (81%)\n",
      "\n",
      "Training so far 40.47860901355743 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 5.064660\n",
      "\n",
      "Test set: Average loss: 0.9020, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 42.499258784453076 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 24.017784\n",
      "\n",
      "Test set: Average loss: 1.0058, Accuracy: 364/451 (81%)\n",
      "\n",
      "Training so far 44.51802523136139 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 11.644071\n",
      "\n",
      "Test set: Average loss: 0.8661, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 46.53637586037318 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 18.437473\n",
      "\n",
      "Test set: Average loss: 0.8900, Accuracy: 370/451 (82%)\n",
      "\n",
      "Epoch    24: reducing learning rate of group 0 to 1.5000e-04.\n",
      "Training so far 48.55705139239629 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 51.228752\n",
      "\n",
      "Test set: Average loss: 0.7960, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 50.5784632841746 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 22.258379\n",
      "\n",
      "Test set: Average loss: 0.8221, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 52.591485973199205 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 49.769218\n",
      "\n",
      "Test set: Average loss: 0.8665, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 54.6091385046641 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 2.442124\n",
      "\n",
      "Test set: Average loss: 0.8257, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 56.632274969418845 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 35.226974\n",
      "\n",
      "Test set: Average loss: 0.8057, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 58.65496665636699 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 10.543875\n",
      "\n",
      "Test set: Average loss: 0.8225, Accuracy: 382/451 (85%)\n",
      "\n",
      "Epoch    30: reducing learning rate of group 0 to 7.5000e-05.\n",
      "Training so far 60.674287887414295 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 13.453794\n",
      "\n",
      "Test set: Average loss: 0.8100, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 62.688416163126625 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 29.952971\n",
      "\n",
      "Test set: Average loss: 0.8179, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 64.71036793788274 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 0.473409\n",
      "\n",
      "Test set: Average loss: 0.8440, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 66.73205196062723 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 30.061829\n",
      "\n",
      "Test set: Average loss: 0.7681, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 68.76131600141525 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 41.220871\n",
      "\n",
      "Test set: Average loss: 0.8428, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 70.77471403678258 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 38.617455\n",
      "\n",
      "Test set: Average loss: 0.8339, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 72.79742681582769 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 43.193108\n",
      "\n",
      "Test set: Average loss: 0.8636, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 74.81772698163986 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 33.934502\n",
      "\n",
      "Test set: Average loss: 0.8074, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 76.84356218973795 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 34.583187\n",
      "\n",
      "Test set: Average loss: 0.8207, Accuracy: 386/451 (86%)\n",
      "\n",
      "Epoch    39: reducing learning rate of group 0 to 3.7500e-05.\n",
      "Training so far 78.85132589737574 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 8.829213\n",
      "\n",
      "Test set: Average loss: 0.8251, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 80.87468875646591 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 45.158569\n",
      "\n",
      "Test set: Average loss: 0.8248, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 82.89345773855845 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 23.237041\n",
      "\n",
      "Test set: Average loss: 0.8452, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 84.91454889774323 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 41.266304\n",
      "\n",
      "Test set: Average loss: 0.8063, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 86.93272338708242 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 0.324540\n",
      "\n",
      "Test set: Average loss: 0.8452, Accuracy: 388/451 (86%)\n",
      "\n",
      "Epoch    44: reducing learning rate of group 0 to 1.8750e-05.\n",
      "Training so far 88.9557757973671 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 14.348397\n",
      "\n",
      "Test set: Average loss: 0.8793, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 90.98044495185216 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 34.825542\n",
      "\n",
      "Test set: Average loss: 0.8245, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 93.00751455624898 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 19.187305\n",
      "\n",
      "Test set: Average loss: 0.8675, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 95.02239179611206 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 0.139651\n",
      "\n",
      "Test set: Average loss: 0.8315, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 97.03072589635849 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 7.756687\n",
      "\n",
      "Test set: Average loss: 0.9517, Accuracy: 389/451 (86%)\n",
      "\n",
      "Epoch    49: reducing learning rate of group 0 to 1.5000e-05.\n",
      "Training so far 99.0566936214765 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 5.779510\n",
      "\n",
      "Test set: Average loss: 0.8845, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 101.0779415011406 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 18.980049\n",
      "\n",
      "Test set: Average loss: 0.8207, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 103.10114815632502 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 23.825447\n",
      "\n",
      "Test set: Average loss: 0.8041, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 105.12185408671697 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 20.136106\n",
      "\n",
      "Test set: Average loss: 0.8001, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 107.1366080125173 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 22.773819\n",
      "\n",
      "Test set: Average loss: 0.8474, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 109.16305624246597 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 36.508583\n",
      "\n",
      "Test set: Average loss: 0.9226, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 111.18999402125677 minutes\n",
      "====================\n",
      "Train Epoch: 55 [0/4258 (0%)]\tLoss: 15.584847\n",
      "\n",
      "Test set: Average loss: 0.8361, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 113.20998977422714 minutes\n",
      "====================\n",
      "Train Epoch: 56 [0/4258 (0%)]\tLoss: 0.404978\n",
      "\n",
      "Test set: Average loss: 0.9103, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 115.23288175264994 minutes\n",
      "====================\n",
      "Train Epoch: 57 [0/4258 (0%)]\tLoss: 36.037308\n",
      "\n",
      "Test set: Average loss: 0.8577, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 117.24486143191656 minutes\n",
      "====================\n",
      "Train Epoch: 58 [0/4258 (0%)]\tLoss: 30.997715\n",
      "\n",
      "Test set: Average loss: 0.8571, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 119.26583343744278 minutes\n",
      "====================\n",
      "Train Epoch: 59 [0/4258 (0%)]\tLoss: 34.769878\n",
      "\n",
      "Test set: Average loss: 0.8607, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 121.28718043963114 minutes\n",
      "====================\n",
      "Train Epoch: 60 [0/4258 (0%)]\tLoss: 35.127979\n",
      "\n",
      "Test set: Average loss: 0.8514, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 123.30854816834132 minutes\n",
      "====================\n",
      "Train Epoch: 61 [0/4258 (0%)]\tLoss: 32.400566\n",
      "\n",
      "Test set: Average loss: 0.9127, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 125.32706691821416 minutes\n",
      "====================\n",
      "Train Epoch: 62 [0/4258 (0%)]\tLoss: 0.817272\n",
      "\n",
      "Test set: Average loss: 0.8613, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 127.35061047871908 minutes\n",
      "====================\n",
      "Train Epoch: 63 [0/4258 (0%)]\tLoss: 27.357018\n",
      "\n",
      "Test set: Average loss: 0.9527, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 129.36677259206772 minutes\n",
      "====================\n",
      "Train Epoch: 64 [0/4258 (0%)]\tLoss: 0.254700\n",
      "\n",
      "Test set: Average loss: 0.8269, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 131.38602590958277 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.989877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.8104, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 133.41190232833227 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 0.151084\n",
      "\n",
      "Test set: Average loss: 0.7948, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 135.43503175179163 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.132398\n",
      "\n",
      "Test set: Average loss: 0.7858, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 137.46165594259898 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 7.533592\n",
      "\n",
      "Test set: Average loss: 0.7897, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 139.48603877226512 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.533404\n",
      "\n",
      "Test set: Average loss: 0.8108, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 141.50764235655467 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.410390\n",
      "\n",
      "Test set: Average loss: 0.8072, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 143.53094732761383 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.128480\n",
      "\n",
      "Test set: Average loss: 0.8239, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 145.54829260905584 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.226704\n",
      "\n",
      "Test set: Average loss: 0.8355, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 147.56217298905054 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 1.722588\n",
      "\n",
      "Test set: Average loss: 0.8236, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 149.5838489731153 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 4.226353\n",
      "\n",
      "Test set: Average loss: 0.8432, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 151.60710186560948 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 1.489074\n",
      "\n",
      "Test set: Average loss: 0.8146, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 153.62733678420383 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 0.662439\n",
      "\n",
      "Test set: Average loss: 0.8178, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 155.63935655752817 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 0.228523\n",
      "\n",
      "Test set: Average loss: 0.8422, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 157.6566353201866 minutes\n",
      "====================\n",
      "time spent training: 157.6566381374995 minutes\n",
      "BEST LOSS: 0.7681048052538261\n",
      "BEST ACC: 86.0310421286031\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(label2code)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "results_folder = \"dens161_aug_mixup_repr_folds\"\n",
    "os.mkdir(f\"tmp/{results_folder}\")\n",
    "BS = 16\n",
    "for FOLD in range(N_FOLDS):\n",
    "    print(\"PROCESSING FOLD\", FOLD)\n",
    "    \n",
    "    model_ft = models.densenet161(pretrained=True)\n",
    "    model_ft.classifier = nn.Sequential(\n",
    "        nn.Linear(model_ft.classifier.in_features, n_classes)\n",
    "    )\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=3e-4)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer_ft, 'min', patience=4, factor=0.5, verbose=True, min_lr=1.5e-5)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(sdf_train_list[FOLD], batch_size=BS, shuffle=True,\n",
    "                                               num_workers=6, drop_last=False, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(sdf_val_list[FOLD], batch_size=BS, drop_last=False, pin_memory=True,\n",
    "                                        num_workers=6)\n",
    "    \n",
    "    # full train cycle with one fold for cross-val:\n",
    "    model_ft = train_on_fold(\n",
    "        model_ft,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        FOLD,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        lr_scheduler,\n",
    "        results_folder,\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    # predict from one model:\n",
    "    preds = predict_on_val(\n",
    "        model_ft,\n",
    "        val_loader\n",
    "    )\n",
    "    val_folds[FOLD][\"preds\"] = [code2label[c] for c in preds]\n",
    "    \n",
    "    # Save val predictions from one model:\n",
    "    val_out = get_val_outputs(model_ft, val_loader)\n",
    "    gt = val_folds[FOLD].label.map(label2code).values\n",
    "    val_losses = list()\n",
    "    \n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([val_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        val_losses.append(item_loss)\n",
    "    val_losses = np.array(val_losses)\n",
    "    val_folds[FOLD][\"loss\"] = val_losses\n",
    "    val_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=True).to_csv(\n",
    "        f\"tmp/{results_folder}/val_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## for debug, save predictions for train as well:\n",
    "    ### debug train audios:\n",
    "    train_debug_loader = torch.utils.data.DataLoader(sdf_train_list[FOLD], batch_size=BS, shuffle=False,\n",
    "                                               num_workers=6, drop_last=False, pin_memory=True)\n",
    "    train_out = get_val_outputs(model_ft, train_debug_loader)\n",
    "    gt = train_folds[FOLD].label.map(label2code).values\n",
    "    train_losses = list()\n",
    "\n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([train_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        train_losses.append(item_loss)\n",
    "\n",
    "    train_losses = np.array(train_losses)\n",
    "    train_folds[FOLD][\"loss\"] = train_losses\n",
    "    train_folds[FOLD][\"preds\"] = [code2label[c] for c in np.argmax(train_out, axis=1)]\n",
    "    train_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=False).to_csv(\n",
    "        f\"tmp/{results_folder}/train_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## MAKE SUBMISSION (from one model):\n",
    "    sample_subm = pd.read_csv(\"Submission1.csv\")\n",
    "    sample_subm[\"image_fn\"] = sample_subm.fn.apply(get_image_path)\n",
    "    subm_dataset = SpectrogramTestDataset([[path, None] for path in sample_subm.image_fn.values ], conf)\n",
    "    subm_loader = torch.utils.data.DataLoader(subm_dataset, batch_size=16)\n",
    "\n",
    "    preds = get_predictions(model_ft, subm_loader)\n",
    "\n",
    "    for c in sample_subm.columns:\n",
    "        if c in {\"fn\", \"image_fn\"}:\n",
    "            continue\n",
    "        c_idx = label2code[c]\n",
    "        sample_subm[c] = preds[:, c_idx]\n",
    "\n",
    "    sample_subm.drop(\"image_fn\", axis=1).to_csv(f'tmp/{results_folder}/subm_{FOLD}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8603104212860311\n",
      "0.88470066518847\n",
      "0.8603104212860311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i/100) for i in [86.0310421286031,88.470066518847,86.0310421286031]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAKE ONE SUBMISSION FROM ALL SUBS:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s = f'tmp/{results_folder}/'\n",
    "preds_to_average = [\n",
    "    f\"subm_{f}.csv\"\n",
    "    for f in range(N_FOLDS)\n",
    "]\n",
    "\n",
    "all_subs = list()\n",
    "source_pred = pd.read_csv(s + preds_to_average[0])\n",
    "pred_cols = source_pred.drop(\"fn\", axis=1).columns.values\n",
    "for file in preds_to_average[1:]:\n",
    "    tmp = pd.read_csv(s + file)\n",
    "    source_pred[pred_cols] += tmp[pred_cols]\n",
    "\n",
    "source_pred[pred_cols] /= len(preds_to_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isclose((source_pred[pred_cols].sum(axis=1)).values, np.ones(source_pred.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_pred.to_csv(f'tmp/{results_folder}/{results_folder}_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
