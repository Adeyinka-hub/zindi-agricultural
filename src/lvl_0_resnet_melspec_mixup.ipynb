{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/letfoolsdie/virtual_envs/ml/lib/python3.6/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import typing as tp\n",
    "import pathlib\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_speech_features as psf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"../data\")\n",
    "audios_path = data_path / \"all_audio_resampled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path/'Train.csv')\n",
    "train_extra = pd.read_csv(data_path/'train_add.csv')\n",
    "train_extra_2 = pd.read_csv(data_path/'train_add_20201029.csv')\n",
    "\n",
    "label2code = {word: idx for idx, word in enumerate(train.label.unique().tolist())}\n",
    "code2label = {v:k for k,v in label2code.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(audio_path):\n",
    "    file_name = audio_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    ip = str(audios_path.resolve() / f\"{file_name}.wav\")\n",
    "    return ip\n",
    "\n",
    "train[\"image_fn\"] = train.fn.apply(get_image_path)\n",
    "train_extra[\"image_fn\"] = train_extra.fn.apply(get_image_path)\n",
    "train_extra_2[\"image_fn\"] = train_extra_2.fn.apply(get_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train, train_extra, train_extra_2], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build validation that includes all classes:\n",
    "\n",
    "\n",
    "vcs = train_df.label.value_counts()\n",
    "\n",
    "## possible schema:\n",
    "# 25+ - take 3\n",
    "# 12-25 - take 2\n",
    "# 12- - take 1\n",
    "\n",
    "def num_for_val(num_examples):\n",
    "    if num_examples >= 25:\n",
    "        return 3\n",
    "    if num_examples >= 12:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "train_df[\"num_examples\"] = train_df.label.map(vcs.to_dict())\n",
    "train_df[\"num_for_val\"] = train_df.num_examples.apply(num_for_val)\n",
    "\n",
    "random.seed(12)\n",
    "train_df_new = pd.DataFrame()\n",
    "for label in train_df.label.unique():\n",
    "    tmp = train_df.loc[train_df.label == label].copy()\n",
    "    tmp[\"dummy\"] = tmp.label.apply(lambda _: random.random())\n",
    "    tmp.sort_values(by=\"dummy\", inplace=True)\n",
    "    tmp[\"rank\"] = range(tmp.shape[0])\n",
    "    train_df_new = pd.concat([train_df_new, tmp])\n",
    "\n",
    "train_df_new.reset_index(drop=True, inplace=True)\n",
    "train_df_new[\"val_subset\"] = train_df_new.num_for_val > train_df_new[\"rank\"]\n",
    "train_df_new.drop(\"dummy\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    set(train_df_new.loc[train_df_new.val_subset].label.unique()) == \n",
    "    set(train_df_new.loc[~train_df_new.val_subset].label.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new[\"val_fold\"] = train_df_new[\"rank\"] // train_df_new.num_for_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 451\n",
      "1 451\n",
      "2 451\n",
      "3 430\n",
      "4 413\n",
      "5 410\n",
      "6 407\n",
      "7 391\n",
      "8 381\n",
      "9 337\n"
     ]
    }
   ],
   "source": [
    "for f in range(10):\n",
    "    print(f, (train_df_new.val_fold == f).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 3\n",
    "train_folds = list()\n",
    "val_folds = list()\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    valf = train_df_new.loc[train_df_new.val_fold == i].copy()\n",
    "    trf = train_df_new.loc[train_df_new.val_fold != i].copy()\n",
    "    \n",
    "    train_folds.append(trf)\n",
    "    val_folds.append(valf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reproduce.src.data_processing import new_generate_spec, new_build_image, normalize, MEAN, STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpecDataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 audio_paths,\n",
    "                 labels,\n",
    "                 config,\n",
    "                 transforms=None,\n",
    "                 is_test=False\n",
    "    ):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.config = config\n",
    "        self.tr = transforms\n",
    "        self.is_test = is_test\n",
    "        self.n_channels = 3\n",
    "        \n",
    "        assert len(self.audio_paths) == len(self.labels)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        MEAN = torch.Tensor([0.485, 0.456, 0.406])\n",
    "        STD = torch.Tensor([0.229, 0.224, 0.225])\n",
    "        \n",
    "        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])\n",
    "        specgram = torchaudio.transforms.MelSpectrogram(\n",
    "            n_mels=self.config.n_mels, sample_rate=self.config.sr,\n",
    "            n_fft=self.config.n_fft, hop_length=self.config.hop_size\n",
    "        )(waveform[0])\n",
    "        specgram = torchaudio.transforms.AmplitudeToDB()(specgram)\n",
    "        \n",
    "        if self.tr:\n",
    "            specgram = self.tr(specgram)\n",
    "  \n",
    "        ## normalize:\n",
    "        specgram -= specgram.min()\n",
    "        specgram /= specgram.max()\n",
    "        \n",
    "        specgram = torch.stack([specgram for _ in range(self.n_channels)])\n",
    "#         return specgram, self.labels[idx]\n",
    "        if self.is_test:\n",
    "            return (specgram - MEAN.reshape(3, 1, 1)) / STD.reshape(3,1,1)\n",
    "        return (specgram - MEAN.reshape(3, 1, 1)) / STD.reshape(3,1,1), self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as tr\n",
    "\n",
    "class PadToSize:\n",
    "    \"\"\"\n",
    "    !! adds padding only to the last dimension !!\n",
    "    !! ALSO can cut image\n",
    "    \"\"\"\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def __call__(self, img2d):\n",
    "        if img2d.shape[-1] >= self.target_size[-1]:\n",
    "            return img2d[:,:self.target_size[-1]]\n",
    "\n",
    "        to_pad = self.target_size[-1] - img2d.shape[-1]\n",
    "        pad_left = to_pad // 2\n",
    "        pad_right = to_pad - pad_left\n",
    "        return torch.nn.functional.pad(img2d, pad=(pad_left, pad_right))\n",
    "\n",
    "class AudioConfig:\n",
    "    n_mels = 64\n",
    "    img_size = (n_mels, 440)\n",
    "    sr = 22050\n",
    "    n_fft = 512\n",
    "    hop_size = 128\n",
    "\n",
    "config = AudioConfig()\n",
    "\n",
    "transforms = tr.Compose([\n",
    "    PadToSize(target_size=config.img_size),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258\n",
      "4258\n",
      "4258\n",
      "=========================\n",
      "451\n",
      "451\n",
      "451\n"
     ]
    }
   ],
   "source": [
    "tr_spec_ds_list = [\n",
    "    MelSpecDataset(\n",
    "    t.image_fn.values,\n",
    "    [label2code[i] for i in t.label.values],\n",
    "    config=config,\n",
    "    transforms=transforms\n",
    ") for t in train_folds\n",
    "]\n",
    "\n",
    "val_spec_ds_list = [\n",
    "    MelSpecDataset(\n",
    "    t.image_fn.values,\n",
    "    [label2code[i] for i in t.label.values],\n",
    "    config=config,\n",
    "    transforms=transforms\n",
    "    ) for t in val_folds\n",
    "]\n",
    "\n",
    "for s in tr_spec_ds_list:\n",
    "    print(len(s))\n",
    "    \n",
    "print(\"=========================\")\n",
    "\n",
    "for s in val_spec_ds_list:\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def train_mixup_epoch(log_interval, mixup_prob, model, device, criterion, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        use_mixup = False\n",
    "        if random.random() < mixup_prob:\n",
    "            use_mixup = True\n",
    "        data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "        \n",
    "        if use_mixup:\n",
    "            data, y_a, y_b, lam = mixup_data(data, target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        if use_mixup:\n",
    "            loss = mixup_criterion(criterion, output, y_a, y_b, lam) #criterion(output, target)\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "\n",
    "            \n",
    "def test(model, device, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def train_on_fold(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    fold,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    results_folder,\n",
    "    epochs\n",
    "):\n",
    "    seed_dict = {\n",
    "        0: 9,\n",
    "        1: 99,\n",
    "        2: 999\n",
    "    }\n",
    "    set_seed(seed_dict[fold])\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    best_loss = 1e5\n",
    "    best_acc = 0\n",
    "\n",
    "    max_patience = 20\n",
    "    patience = 0\n",
    "\n",
    "    train_loss_hist = list()\n",
    "    val_loss_hist = list()\n",
    "    val_acc_hist = list()\n",
    "\n",
    "    save_each_epoch = False\n",
    "\n",
    "    for ep in range(50):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.8, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            print(\"patience:\", patience)\n",
    "\n",
    "\n",
    "        lr_scheduler.step() \n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "\n",
    "    optimizer = optim.Adam(model_ft.parameters(), lr=2e-5)\n",
    "    for ep in range(15):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.0, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            print(\"patience:\", patience)\n",
    "\n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "\n",
    "\n",
    "    print(\"time spent training: {} minutes\".format((time.time() - t0) / 60))\n",
    "    print(\"BEST LOSS:\", best_loss)\n",
    "    print(\"BEST ACC:\", best_acc)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"tmp/{results_folder}/best_run_{fold}.pth\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_on_val(\n",
    "    model,\n",
    "    val_loader,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, (inputs, _) in enumerate(val_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs))\n",
    "\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_val_outputs(model, test_loader, device=\"cuda\"):\n",
    "    outputs_list = list()\n",
    "    for batch_idx, (inputs, target) in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            outputs_list.append(outputs)\n",
    "    outputs_list = np.concatenate([t.cpu().numpy() for t in outputs_list])\n",
    "    return outputs_list\n",
    "\n",
    "def get_predictions(model, test_loader, device=\"cuda\"):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, inputs in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs)) ## ADD SOFTMAX\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft.load_state_dict(torch.load(f\"tmp/{tmp_folder_name}/best_run_{FOLD}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FOLD 1\n",
      "Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 1172.989868\n",
      "\n",
      "Test set: Average loss: 5.2415, Accuracy: 22/451 (5%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.9903e-04.\n",
      "Training so far 0.14472198486328125 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 628.256592\n",
      "\n",
      "Test set: Average loss: 3.8227, Accuracy: 94/451 (21%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.9614e-04.\n",
      "Training so far 0.28986985683441163 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 571.820190\n",
      "\n",
      "Test set: Average loss: 2.1459, Accuracy: 223/451 (49%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.9135e-04.\n",
      "Training so far 0.43508367935816444 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 442.173889\n",
      "\n",
      "Test set: Average loss: 1.9347, Accuracy: 267/451 (59%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.8470e-04.\n",
      "Training so far 0.5819736083348592 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 259.220764\n",
      "\n",
      "Test set: Average loss: 1.6516, Accuracy: 287/451 (64%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.7623e-04.\n",
      "Training so far 0.7296151002248128 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 393.364258\n",
      "\n",
      "Test set: Average loss: 1.4165, Accuracy: 322/451 (71%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.6603e-04.\n",
      "Training so far 0.8730579098065694 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 46.236897\n",
      "\n",
      "Test set: Average loss: 1.3970, Accuracy: 319/451 (71%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.5416e-04.\n",
      "Training so far 1.0180083672205607 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 337.932495\n",
      "\n",
      "Test set: Average loss: 1.2999, Accuracy: 339/451 (75%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.4074e-04.\n",
      "Training so far 1.1644542654355368 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 287.997040\n",
      "\n",
      "Test set: Average loss: 1.3178, Accuracy: 327/451 (73%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 4.2586e-04.\n",
      "Training so far 1.31305988629659 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 348.056702\n",
      "\n",
      "Test set: Average loss: 1.2524, Accuracy: 343/451 (76%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.0964e-04.\n",
      "Training so far 1.4621413111686707 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 53.976265\n",
      "\n",
      "Test set: Average loss: 1.3036, Accuracy: 338/451 (75%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 3.9222e-04.\n",
      "Training so far 1.610204784075419 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 50.235138\n",
      "\n",
      "Test set: Average loss: 1.1263, Accuracy: 350/451 (78%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 3.7373e-04.\n",
      "Training so far 1.7626075665156047 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 283.331818\n",
      "\n",
      "Test set: Average loss: 1.0337, Accuracy: 369/451 (82%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 3.5433e-04.\n",
      "Training so far 1.9152302106221517 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 9.248531\n",
      "\n",
      "Test set: Average loss: 1.0407, Accuracy: 362/451 (80%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 3.3416e-04.\n",
      "Training so far 2.0613103032112123 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 120.786011\n",
      "\n",
      "Test set: Average loss: 1.1121, Accuracy: 361/451 (80%)\n",
      "\n",
      "patience: 2\n",
      "Adjusting learning rate of group 0 to 3.1341e-04.\n",
      "Training so far 2.208114679654439 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 322.376251\n",
      "\n",
      "Test set: Average loss: 1.0345, Accuracy: 358/451 (79%)\n",
      "\n",
      "patience: 3\n",
      "Adjusting learning rate of group 0 to 2.9222e-04.\n",
      "Training so far 2.3517293373743695 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 10.791942\n",
      "\n",
      "Test set: Average loss: 1.0723, Accuracy: 360/451 (80%)\n",
      "\n",
      "patience: 4\n",
      "Adjusting learning rate of group 0 to 2.7077e-04.\n",
      "Training so far 2.4975834091504416 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 15.783144\n",
      "\n",
      "Test set: Average loss: 1.0319, Accuracy: 366/451 (81%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 2.4923e-04.\n",
      "Training so far 2.6460744857788088 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 279.195740\n",
      "\n",
      "Test set: Average loss: 1.0557, Accuracy: 360/451 (80%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 2.2778e-04.\n",
      "Training so far 2.7938658396402993 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 249.642700\n",
      "\n",
      "Test set: Average loss: 1.1355, Accuracy: 364/451 (81%)\n",
      "\n",
      "patience: 2\n",
      "Adjusting learning rate of group 0 to 2.0659e-04.\n",
      "Training so far 2.941870566209157 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 9.399887\n",
      "\n",
      "Test set: Average loss: 1.0242, Accuracy: 366/451 (81%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 1.8584e-04.\n",
      "Training so far 3.089594892660777 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 241.913986\n",
      "\n",
      "Test set: Average loss: 0.9517, Accuracy: 376/451 (83%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 1.6567e-04.\n",
      "Training so far 3.238573495546977 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 234.636688\n",
      "\n",
      "Test set: Average loss: 1.0441, Accuracy: 369/451 (82%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 1.4627e-04.\n",
      "Training so far 3.3870071371396384 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 6.886384\n",
      "\n",
      "Test set: Average loss: 0.9241, Accuracy: 372/451 (82%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 1.2778e-04.\n",
      "Training so far 3.534129595756531 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 91.483055\n",
      "\n",
      "Test set: Average loss: 1.0563, Accuracy: 369/451 (82%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 1.1036e-04.\n",
      "Training so far 3.6822811166445413 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 170.646729\n",
      "\n",
      "Test set: Average loss: 1.1373, Accuracy: 357/451 (79%)\n",
      "\n",
      "patience: 2\n",
      "Adjusting learning rate of group 0 to 9.4145e-05.\n",
      "Training so far 3.830804705619812 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 243.377411\n",
      "\n",
      "Test set: Average loss: 0.9623, Accuracy: 370/451 (82%)\n",
      "\n",
      "patience: 3\n",
      "Adjusting learning rate of group 0 to 7.9263e-05.\n",
      "Training so far 3.973119783401489 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 249.855377\n",
      "\n",
      "Test set: Average loss: 1.0310, Accuracy: 364/451 (81%)\n",
      "\n",
      "patience: 4\n",
      "Adjusting learning rate of group 0 to 6.5836e-05.\n",
      "Training so far 4.118408441543579 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 256.245422\n",
      "\n",
      "Test set: Average loss: 0.9821, Accuracy: 374/451 (83%)\n",
      "\n",
      "patience: 5\n",
      "Adjusting learning rate of group 0 to 5.3972e-05.\n",
      "Training so far 4.265244587262472 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 278.908264\n",
      "\n",
      "Test set: Average loss: 1.0083, Accuracy: 372/451 (82%)\n",
      "\n",
      "patience: 6\n",
      "Adjusting learning rate of group 0 to 4.3767e-05.\n",
      "Training so far 4.41254981358846 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 128.352295\n",
      "\n",
      "Test set: Average loss: 1.0395, Accuracy: 372/451 (82%)\n",
      "\n",
      "patience: 7\n",
      "Adjusting learning rate of group 0 to 3.5304e-05.\n",
      "Training so far 4.559168283144633 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 51.640717\n",
      "\n",
      "Test set: Average loss: 0.9806, Accuracy: 373/451 (83%)\n",
      "\n",
      "patience: 8\n",
      "Adjusting learning rate of group 0 to 2.8649e-05.\n",
      "Training so far 4.706799550851186 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 217.293060\n",
      "\n",
      "Test set: Average loss: 1.0382, Accuracy: 368/451 (82%)\n",
      "\n",
      "patience: 9\n",
      "Adjusting learning rate of group 0 to 2.3857e-05.\n",
      "Training so far 4.8535348773002625 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 60.160919\n",
      "\n",
      "Test set: Average loss: 0.9683, Accuracy: 372/451 (82%)\n",
      "\n",
      "patience: 10\n",
      "Adjusting learning rate of group 0 to 2.0966e-05.\n",
      "Training so far 5.000022506713867 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 4.816093\n",
      "\n",
      "Test set: Average loss: 1.0129, Accuracy: 367/451 (81%)\n",
      "\n",
      "patience: 11\n",
      "Adjusting learning rate of group 0 to 2.0000e-05.\n",
      "Training so far 5.1429730693499245 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 165.297409\n",
      "\n",
      "Test set: Average loss: 1.0084, Accuracy: 366/451 (81%)\n",
      "\n",
      "patience: 12\n",
      "Adjusting learning rate of group 0 to 2.0966e-05.\n",
      "Training so far 5.289679392178853 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 196.373474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0221, Accuracy: 369/451 (82%)\n",
      "\n",
      "patience: 13\n",
      "Adjusting learning rate of group 0 to 2.3857e-05.\n",
      "Training so far 5.437586236000061 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 97.510727\n",
      "\n",
      "Test set: Average loss: 0.9459, Accuracy: 370/451 (82%)\n",
      "\n",
      "patience: 14\n",
      "Adjusting learning rate of group 0 to 2.8649e-05.\n",
      "Training so far 5.584317914644877 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 179.279556\n",
      "\n",
      "Test set: Average loss: 1.0058, Accuracy: 370/451 (82%)\n",
      "\n",
      "patience: 15\n",
      "Adjusting learning rate of group 0 to 3.5304e-05.\n",
      "Training so far 5.730286939938863 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 170.361801\n",
      "\n",
      "Test set: Average loss: 1.0060, Accuracy: 375/451 (83%)\n",
      "\n",
      "patience: 16\n",
      "Adjusting learning rate of group 0 to 4.3767e-05.\n",
      "Training so far 5.877453859647115 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 231.114349\n",
      "\n",
      "Test set: Average loss: 0.9930, Accuracy: 367/451 (81%)\n",
      "\n",
      "patience: 17\n",
      "Adjusting learning rate of group 0 to 5.3972e-05.\n",
      "Training so far 6.023726634184519 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 228.934235\n",
      "\n",
      "Test set: Average loss: 1.0120, Accuracy: 372/451 (82%)\n",
      "\n",
      "patience: 18\n",
      "Adjusting learning rate of group 0 to 6.5836e-05.\n",
      "Training so far 6.166677272319793 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 245.593048\n",
      "\n",
      "Test set: Average loss: 1.0523, Accuracy: 373/451 (83%)\n",
      "\n",
      "patience: 19\n",
      "Adjusting learning rate of group 0 to 7.9263e-05.\n",
      "Training so far 6.313595362504324 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 66.981308\n",
      "\n",
      "Test set: Average loss: 1.0479, Accuracy: 370/451 (82%)\n",
      "\n",
      "patience: 20\n",
      "Adjusting learning rate of group 0 to 9.4145e-05.\n",
      "Training so far 6.457609752813975 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 257.240417\n",
      "\n",
      "Test set: Average loss: 0.9732, Accuracy: 370/451 (82%)\n",
      "\n",
      "patience: 21\n",
      "Adjusting learning rate of group 0 to 1.1036e-04.\n",
      "Training so far 6.60431973139445 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 4.679770\n",
      "\n",
      "Test set: Average loss: 1.0430, Accuracy: 371/451 (82%)\n",
      "\n",
      "patience: 22\n",
      "Adjusting learning rate of group 0 to 1.2778e-04.\n",
      "Training so far 6.746300109227499 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 7.329773\n",
      "\n",
      "Test set: Average loss: 1.0248, Accuracy: 367/451 (81%)\n",
      "\n",
      "patience: 23\n",
      "Adjusting learning rate of group 0 to 1.4627e-04.\n",
      "Training so far 6.890763707955679 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 249.334595\n",
      "\n",
      "Test set: Average loss: 1.1328, Accuracy: 358/451 (79%)\n",
      "\n",
      "patience: 24\n",
      "Adjusting learning rate of group 0 to 1.6567e-04.\n",
      "Training so far 7.031191432476044 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 6.157371\n",
      "\n",
      "Test set: Average loss: 1.0597, Accuracy: 375/451 (83%)\n",
      "\n",
      "patience: 25\n",
      "Adjusting learning rate of group 0 to 1.8584e-04.\n",
      "Training so far 7.178716286023458 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 4.710654\n",
      "\n",
      "Test set: Average loss: 1.0424, Accuracy: 362/451 (80%)\n",
      "\n",
      "patience: 26\n",
      "Adjusting learning rate of group 0 to 2.0659e-04.\n",
      "Training so far 7.320920701821645 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 4.870022\n",
      "\n",
      "Test set: Average loss: 0.7339, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 7.471199381351471 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 0.651922\n",
      "\n",
      "Test set: Average loss: 0.7290, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 7.620240104198456 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.370013\n",
      "\n",
      "Test set: Average loss: 0.7222, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 7.768991001447042 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.261799\n",
      "\n",
      "Test set: Average loss: 0.7315, Accuracy: 386/451 (86%)\n",
      "\n",
      "patience: 1\n",
      "Training so far 7.91742970943451 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.210119\n",
      "\n",
      "Test set: Average loss: 0.7301, Accuracy: 386/451 (86%)\n",
      "\n",
      "patience: 2\n",
      "Training so far 8.064245927333832 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.187833\n",
      "\n",
      "Test set: Average loss: 0.7311, Accuracy: 387/451 (86%)\n",
      "\n",
      "patience: 3\n",
      "Training so far 8.20795552333196 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.139691\n",
      "\n",
      "Test set: Average loss: 0.7391, Accuracy: 387/451 (86%)\n",
      "\n",
      "patience: 4\n",
      "Training so far 8.354461479187012 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.101043\n",
      "\n",
      "Test set: Average loss: 0.7378, Accuracy: 387/451 (86%)\n",
      "\n",
      "patience: 5\n",
      "Training so far 8.501594440142314 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.109356\n",
      "\n",
      "Test set: Average loss: 0.7371, Accuracy: 388/451 (86%)\n",
      "\n",
      "patience: 6\n",
      "Training so far 8.649008071422577 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.089162\n",
      "\n",
      "Test set: Average loss: 0.7414, Accuracy: 389/451 (86%)\n",
      "\n",
      "patience: 7\n",
      "Training so far 8.795632453759511 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 0.067202\n",
      "\n",
      "Test set: Average loss: 0.7492, Accuracy: 387/451 (86%)\n",
      "\n",
      "patience: 8\n",
      "Training so far 8.942366508642833 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 0.063537\n",
      "\n",
      "Test set: Average loss: 0.7455, Accuracy: 388/451 (86%)\n",
      "\n",
      "patience: 9\n",
      "Training so far 9.088811727364858 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 0.054103\n",
      "\n",
      "Test set: Average loss: 0.7555, Accuracy: 387/451 (86%)\n",
      "\n",
      "patience: 10\n",
      "Training so far 9.235533392429351 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 0.061747\n",
      "\n",
      "Test set: Average loss: 0.7560, Accuracy: 386/451 (86%)\n",
      "\n",
      "patience: 11\n",
      "Training so far 9.382739981015524 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 0.059121\n",
      "\n",
      "Test set: Average loss: 0.7602, Accuracy: 387/451 (86%)\n",
      "\n",
      "patience: 12\n",
      "Training so far 9.529406531651814 minutes\n",
      "====================\n",
      "time spent training: 9.529406805833181 minutes\n",
      "BEST LOSS: 0.72223811350482\n",
      "BEST ACC: 85.36585365853658\n",
      "PROCESSING FOLD 2\n",
      "Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 1166.544678\n",
      "\n",
      "Test set: Average loss: 5.1797, Accuracy: 12/451 (3%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.9903e-04.\n",
      "Training so far 0.1445004979769389 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 624.953796\n",
      "\n",
      "Test set: Average loss: 4.1747, Accuracy: 75/451 (17%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.9614e-04.\n",
      "Training so far 0.2929155667622884 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 278.278290\n",
      "\n",
      "Test set: Average loss: 2.8927, Accuracy: 161/451 (36%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.9135e-04.\n",
      "Training so far 0.43930407365163165 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 245.674728\n",
      "\n",
      "Test set: Average loss: 1.9751, Accuracy: 250/451 (55%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.8470e-04.\n",
      "Training so far 0.5920591672261556 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 243.573730\n",
      "\n",
      "Test set: Average loss: 1.5937, Accuracy: 314/451 (70%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.7623e-04.\n",
      "Training so far 0.7422528386116027 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 104.567230\n",
      "\n",
      "Test set: Average loss: 1.4088, Accuracy: 320/451 (71%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.6603e-04.\n",
      "Training so far 0.8902856548627217 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 129.537933\n",
      "\n",
      "Test set: Average loss: 1.3687, Accuracy: 341/451 (76%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.5416e-04.\n",
      "Training so far 1.0385952750841776 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 171.963898\n",
      "\n",
      "Test set: Average loss: 1.4397, Accuracy: 326/451 (72%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 4.4074e-04.\n",
      "Training so far 1.1851789712905885 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 30.723083\n",
      "\n",
      "Test set: Average loss: 1.2609, Accuracy: 338/451 (75%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.2586e-04.\n",
      "Training so far 1.3352192918459573 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 352.283752\n",
      "\n",
      "Test set: Average loss: 1.3638, Accuracy: 333/451 (74%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 4.0964e-04.\n",
      "Training so far 1.4794620553652444 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 350.988647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.2861, Accuracy: 340/451 (75%)\n",
      "\n",
      "patience: 2\n",
      "Adjusting learning rate of group 0 to 3.9222e-04.\n",
      "Training so far 1.62764630317688 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 25.693024\n",
      "\n",
      "Test set: Average loss: 1.1338, Accuracy: 354/451 (78%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 3.7373e-04.\n",
      "Training so far 1.7753538052241007 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 194.970261\n",
      "\n",
      "Test set: Average loss: 1.2333, Accuracy: 360/451 (80%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 3.5433e-04.\n",
      "Training so far 1.9240917046864827 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 336.747192\n",
      "\n",
      "Test set: Average loss: 1.1633, Accuracy: 366/451 (81%)\n",
      "\n",
      "patience: 2\n",
      "Adjusting learning rate of group 0 to 3.3416e-04.\n",
      "Training so far 2.0672542532285054 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 239.625015\n",
      "\n",
      "Test set: Average loss: 1.0300, Accuracy: 368/451 (82%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 3.1341e-04.\n",
      "Training so far 2.216125778357188 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 247.405548\n",
      "\n",
      "Test set: Average loss: 1.1000, Accuracy: 363/451 (80%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 2.9222e-04.\n",
      "Training so far 2.3597185055414838 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 307.136475\n",
      "\n",
      "Test set: Average loss: 1.1131, Accuracy: 366/451 (81%)\n",
      "\n",
      "patience: 2\n",
      "Adjusting learning rate of group 0 to 2.7077e-04.\n",
      "Training so far 2.5040247241655984 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 12.044646\n",
      "\n",
      "Test set: Average loss: 1.1235, Accuracy: 365/451 (81%)\n",
      "\n",
      "patience: 3\n",
      "Adjusting learning rate of group 0 to 2.4923e-04.\n",
      "Training so far 2.652909489472707 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 322.221313\n",
      "\n",
      "Test set: Average loss: 1.0414, Accuracy: 367/451 (81%)\n",
      "\n",
      "patience: 4\n",
      "Adjusting learning rate of group 0 to 2.2778e-04.\n",
      "Training so far 2.7993825038274127 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 294.685577\n",
      "\n",
      "Test set: Average loss: 1.1386, Accuracy: 359/451 (80%)\n",
      "\n",
      "patience: 5\n",
      "Adjusting learning rate of group 0 to 2.0659e-04.\n",
      "Training so far 2.9460369348526 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 8.498919\n",
      "\n",
      "Test set: Average loss: 1.1429, Accuracy: 378/451 (84%)\n",
      "\n",
      "patience: 6\n",
      "Adjusting learning rate of group 0 to 1.8584e-04.\n",
      "Training so far 3.0930394013722737 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 96.079819\n",
      "\n",
      "Test set: Average loss: 1.1599, Accuracy: 375/451 (83%)\n",
      "\n",
      "patience: 7\n",
      "Adjusting learning rate of group 0 to 1.6567e-04.\n",
      "Training so far 3.2361360748608905 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 242.054626\n",
      "\n",
      "Test set: Average loss: 0.9967, Accuracy: 374/451 (83%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 1.4627e-04.\n",
      "Training so far 3.38116250038147 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 231.591873\n",
      "\n",
      "Test set: Average loss: 1.0501, Accuracy: 373/451 (83%)\n",
      "\n",
      "patience: 1\n",
      "Adjusting learning rate of group 0 to 1.2778e-04.\n",
      "Training so far 3.527932933966319 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 281.037994\n",
      "\n",
      "Test set: Average loss: 1.1104, Accuracy: 373/451 (83%)\n",
      "\n",
      "patience: 2\n",
      "Adjusting learning rate of group 0 to 1.1036e-04.\n",
      "Training so far 3.6755205432573956 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 276.489197\n",
      "\n",
      "Test set: Average loss: 1.0673, Accuracy: 376/451 (83%)\n",
      "\n",
      "patience: 3\n",
      "Adjusting learning rate of group 0 to 9.4145e-05.\n",
      "Training so far 3.8211456179618835 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 247.084564\n",
      "\n",
      "Test set: Average loss: 1.0281, Accuracy: 376/451 (83%)\n",
      "\n",
      "patience: 4\n",
      "Adjusting learning rate of group 0 to 7.9263e-05.\n",
      "Training so far 3.966990029811859 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 136.437546\n",
      "\n",
      "Test set: Average loss: 1.0376, Accuracy: 379/451 (84%)\n",
      "\n",
      "patience: 5\n",
      "Adjusting learning rate of group 0 to 6.5836e-05.\n",
      "Training so far 4.108335689703623 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 201.530502\n",
      "\n",
      "Test set: Average loss: 1.0658, Accuracy: 376/451 (83%)\n",
      "\n",
      "patience: 6\n",
      "Adjusting learning rate of group 0 to 5.3972e-05.\n",
      "Training so far 4.2562551379203795 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 159.695740\n",
      "\n",
      "Test set: Average loss: 1.0282, Accuracy: 375/451 (83%)\n",
      "\n",
      "patience: 7\n",
      "Adjusting learning rate of group 0 to 4.3767e-05.\n",
      "Training so far 4.4042425354321795 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 5.348192\n",
      "\n",
      "Test set: Average loss: 1.0358, Accuracy: 377/451 (84%)\n",
      "\n",
      "patience: 8\n",
      "Adjusting learning rate of group 0 to 3.5304e-05.\n",
      "Training so far 4.545599238077799 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 158.996460\n",
      "\n",
      "Test set: Average loss: 1.0730, Accuracy: 375/451 (83%)\n",
      "\n",
      "patience: 9\n",
      "Adjusting learning rate of group 0 to 2.8649e-05.\n",
      "Training so far 4.693915605545044 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 64.091507\n",
      "\n",
      "Test set: Average loss: 1.1573, Accuracy: 372/451 (82%)\n",
      "\n",
      "patience: 10\n",
      "Adjusting learning rate of group 0 to 2.3857e-05.\n",
      "Training so far 4.837891022364299 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 7.144300\n",
      "\n",
      "Test set: Average loss: 1.0803, Accuracy: 378/451 (84%)\n",
      "\n",
      "patience: 11\n",
      "Adjusting learning rate of group 0 to 2.0966e-05.\n",
      "Training so far 4.9850744565327965 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 75.341316\n",
      "\n",
      "Test set: Average loss: 1.0926, Accuracy: 377/451 (84%)\n",
      "\n",
      "patience: 12\n",
      "Adjusting learning rate of group 0 to 2.0000e-05.\n",
      "Training so far 5.131054917971293 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 98.614204\n",
      "\n",
      "Test set: Average loss: 1.0719, Accuracy: 377/451 (84%)\n",
      "\n",
      "patience: 13\n",
      "Adjusting learning rate of group 0 to 2.0966e-05.\n",
      "Training so far 5.2782103816668196 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 80.498856\n",
      "\n",
      "Test set: Average loss: 1.0676, Accuracy: 379/451 (84%)\n",
      "\n",
      "patience: 14\n",
      "Adjusting learning rate of group 0 to 2.3857e-05.\n",
      "Training so far 5.421314279238383 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 4.784780\n",
      "\n",
      "Test set: Average loss: 1.0702, Accuracy: 377/451 (84%)\n",
      "\n",
      "patience: 15\n",
      "Adjusting learning rate of group 0 to 2.8649e-05.\n",
      "Training so far 5.5688017964363095 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 250.824677\n",
      "\n",
      "Test set: Average loss: 1.0572, Accuracy: 377/451 (84%)\n",
      "\n",
      "patience: 16\n",
      "Adjusting learning rate of group 0 to 3.5304e-05.\n",
      "Training so far 5.715144844849904 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 19.535767\n",
      "\n",
      "Test set: Average loss: 1.0293, Accuracy: 376/451 (83%)\n",
      "\n",
      "patience: 17\n",
      "Adjusting learning rate of group 0 to 4.3767e-05.\n",
      "Training so far 5.863821109135945 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 261.472198\n",
      "\n",
      "Test set: Average loss: 1.0615, Accuracy: 379/451 (84%)\n",
      "\n",
      "patience: 18\n",
      "Adjusting learning rate of group 0 to 5.3972e-05.\n",
      "Training so far 6.011247416337331 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 234.384644\n",
      "\n",
      "Test set: Average loss: 1.1181, Accuracy: 374/451 (83%)\n",
      "\n",
      "patience: 19\n",
      "Adjusting learning rate of group 0 to 6.5836e-05.\n",
      "Training so far 6.158772889773051 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 146.012817\n",
      "\n",
      "Test set: Average loss: 1.1850, Accuracy: 369/451 (82%)\n",
      "\n",
      "patience: 20\n",
      "Adjusting learning rate of group 0 to 7.9263e-05.\n",
      "Training so far 6.302819859981537 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 7.559216\n",
      "\n",
      "Test set: Average loss: 1.0105, Accuracy: 375/451 (83%)\n",
      "\n",
      "patience: 21\n",
      "Adjusting learning rate of group 0 to 9.4145e-05.\n",
      "Training so far 6.449043850104014 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 4.525948\n",
      "\n",
      "Test set: Average loss: 1.1202, Accuracy: 371/451 (82%)\n",
      "\n",
      "patience: 22\n",
      "Adjusting learning rate of group 0 to 1.1036e-04.\n",
      "Training so far 6.595041545232137 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 250.432098\n",
      "\n",
      "Test set: Average loss: 1.1453, Accuracy: 371/451 (82%)\n",
      "\n",
      "patience: 23\n",
      "Adjusting learning rate of group 0 to 1.2778e-04.\n",
      "Training so far 6.742533453305563 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 230.720673\n",
      "\n",
      "Test set: Average loss: 1.1290, Accuracy: 371/451 (82%)\n",
      "\n",
      "patience: 24\n",
      "Adjusting learning rate of group 0 to 1.4627e-04.\n",
      "Training so far 6.888757638136545 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 217.974091\n",
      "\n",
      "Test set: Average loss: 1.2979, Accuracy: 352/451 (78%)\n",
      "\n",
      "patience: 25\n",
      "Adjusting learning rate of group 0 to 1.6567e-04.\n",
      "Training so far 7.0360921422640486 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 267.507477\n",
      "\n",
      "Test set: Average loss: 1.1617, Accuracy: 364/451 (81%)\n",
      "\n",
      "patience: 26\n",
      "Adjusting learning rate of group 0 to 1.8584e-04.\n",
      "Training so far 7.185854025681814 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 183.906708\n",
      "\n",
      "Test set: Average loss: 1.1923, Accuracy: 364/451 (81%)\n",
      "\n",
      "patience: 27\n",
      "Adjusting learning rate of group 0 to 2.0659e-04.\n",
      "Training so far 7.330053508281708 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 6.768074\n",
      "\n",
      "Test set: Average loss: 0.8193, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 7.478242917855581 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 1.098606\n",
      "\n",
      "Test set: Average loss: 0.8111, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 7.629748845100403 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.556756\n",
      "\n",
      "Test set: Average loss: 0.8179, Accuracy: 383/451 (85%)\n",
      "\n",
      "patience: 1\n",
      "Training so far 7.776867059866587 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.401210\n",
      "\n",
      "Test set: Average loss: 0.8161, Accuracy: 384/451 (85%)\n",
      "\n",
      "patience: 2\n",
      "Training so far 7.92684314250946 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.361872\n",
      "\n",
      "Test set: Average loss: 0.8218, Accuracy: 384/451 (85%)\n",
      "\n",
      "patience: 3\n",
      "Training so far 8.070065784454346 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.226433\n",
      "\n",
      "Test set: Average loss: 0.8275, Accuracy: 386/451 (86%)\n",
      "\n",
      "patience: 4\n",
      "Training so far 8.216143151124319 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.227305\n",
      "\n",
      "Test set: Average loss: 0.8315, Accuracy: 385/451 (85%)\n",
      "\n",
      "patience: 5\n",
      "Training so far 8.364058168729146 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.145903\n",
      "\n",
      "Test set: Average loss: 0.8359, Accuracy: 385/451 (85%)\n",
      "\n",
      "patience: 6\n",
      "Training so far 8.508063995838166 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.145387\n",
      "\n",
      "Test set: Average loss: 0.8376, Accuracy: 384/451 (85%)\n",
      "\n",
      "patience: 7\n",
      "Training so far 8.651182917753856 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.126518\n",
      "\n",
      "Test set: Average loss: 0.8386, Accuracy: 386/451 (86%)\n",
      "\n",
      "patience: 8\n",
      "Training so far 8.797491836547852 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 0.113108\n",
      "\n",
      "Test set: Average loss: 0.8469, Accuracy: 385/451 (85%)\n",
      "\n",
      "patience: 9\n",
      "Training so far 8.944658875465393 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 0.083831\n",
      "\n",
      "Test set: Average loss: 0.8503, Accuracy: 386/451 (86%)\n",
      "\n",
      "patience: 10\n",
      "Training so far 9.087392222881316 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 0.084675\n",
      "\n",
      "Test set: Average loss: 0.8513, Accuracy: 385/451 (85%)\n",
      "\n",
      "patience: 11\n",
      "Training so far 9.235419420401255 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 0.070567\n",
      "\n",
      "Test set: Average loss: 0.8592, Accuracy: 388/451 (86%)\n",
      "\n",
      "patience: 12\n",
      "Training so far 9.381655895709992 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 0.060591\n",
      "\n",
      "Test set: Average loss: 0.8605, Accuracy: 386/451 (86%)\n",
      "\n",
      "patience: 13\n",
      "Training so far 9.527932453155518 minutes\n",
      "====================\n",
      "time spent training: 9.527932743231455 minutes\n",
      "BEST LOSS: 0.8110809410754963\n",
      "BEST ACC: 85.14412416851441\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(label2code)\n",
    "device = torch.device(\"cuda\")\n",
    "BS = 128\n",
    "\n",
    "results_folder = \"melspec_resnet34_repr\"\n",
    "os.mkdir(f\"tmp/{results_folder}\")\n",
    "\n",
    "for FOLD in range(3):\n",
    "    print(\"PROCESSING FOLD\", FOLD)\n",
    "    model_ft = models.resnet34(pretrained=True)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=5e-4)\n",
    "    cosine_lrsche = CosineAnnealingLR(optimizer_ft, T_max=35, eta_min=2e-5, verbose=True)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(tr_spec_ds_list[FOLD], batch_size=BS, shuffle=True,\n",
    "                                               num_workers=6, drop_last=False, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_spec_ds_list[FOLD], batch_size=BS, drop_last=False, pin_memory=True,\n",
    "                                        num_workers=6)\n",
    "    \n",
    "    # full train cycle with one fold for cross-val:\n",
    "    model_ft = train_on_fold(\n",
    "        model_ft,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        FOLD,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        cosine_lrsche,\n",
    "        results_folder,\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    # predict from one model:\n",
    "    preds = predict_on_val(\n",
    "        model_ft,\n",
    "        val_loader\n",
    "    )\n",
    "    val_folds[FOLD][\"preds\"] = [code2label[c] for c in preds]\n",
    "    \n",
    "    # Save val predictions from one model:\n",
    "    val_out = get_val_outputs(model_ft, val_loader)\n",
    "    gt = val_folds[FOLD].label.map(label2code).values\n",
    "    val_losses = list()\n",
    "    \n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([val_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        val_losses.append(item_loss)\n",
    "    val_losses = np.array(val_losses)\n",
    "    val_folds[FOLD][\"loss\"] = val_losses\n",
    "    val_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=True).to_csv(\n",
    "        f\"tmp/{results_folder}/val_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## for debug, save predictions for train as well:\n",
    "    ### debug train audios:\n",
    "    train_debug_loader = torch.utils.data.DataLoader(tr_spec_ds_list[FOLD], batch_size=BS, shuffle=False,\n",
    "                                               num_workers=4, drop_last=False, pin_memory=True)\n",
    "    train_out = get_val_outputs(model_ft, train_debug_loader)\n",
    "    gt = train_folds[FOLD].label.map(label2code).values\n",
    "    train_losses = list()\n",
    "\n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([train_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        train_losses.append(item_loss)\n",
    "\n",
    "    train_losses = np.array(train_losses)\n",
    "    train_folds[FOLD][\"loss\"] = train_losses\n",
    "    train_folds[FOLD][\"preds\"] = [code2label[c] for c in np.argmax(train_out, axis=1)]\n",
    "    train_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=False).to_csv(\n",
    "        f\"tmp/{results_folder}/train_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## MAKE SUBMISSION (from one model):\n",
    "    sample_subm = pd.read_csv(\"Submission1.csv\")\n",
    "    sample_subm[\"image_fn\"] = sample_subm.fn.apply(get_image_path)\n",
    "#     subm_dataset = SpectrogramTestDataset([[path, None] for path in sample_subm.image_fn.values ], conf)\n",
    "#     subm_loader = torch.utils.data.DataLoader(subm_dataset, batch_size=16)\n",
    "    subm_dataset = MelSpecDataset(\n",
    "        sample_subm.image_fn.values,\n",
    "        [None for _ in sample_subm.image_fn.values],\n",
    "        config=config,\n",
    "        transforms=transforms,\n",
    "        is_test=True\n",
    "    )\n",
    "    subm_loader = torch.utils.data.DataLoader(subm_dataset, batch_size=BS)\n",
    "\n",
    "    preds = get_predictions(model_ft, subm_loader)\n",
    "\n",
    "    for c in sample_subm.columns:\n",
    "        if c in {\"fn\", \"image_fn\"}:\n",
    "            continue\n",
    "        c_idx = label2code[c]\n",
    "        sample_subm[c] = preds[:, c_idx]\n",
    "\n",
    "    sample_subm.drop(\"image_fn\", axis=1).to_csv(f'tmp/{results_folder}/subm_{FOLD}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAKE ONE SUBMISSION FROM ALL SUBS:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s = f'tmp/{results_folder}/'\n",
    "preds_to_average = [\n",
    "    f\"subm_{f}.csv\"\n",
    "    for f in range(N_FOLDS)\n",
    "]\n",
    "\n",
    "all_subs = list()\n",
    "source_pred = pd.read_csv(s + preds_to_average[0])\n",
    "pred_cols = source_pred.drop(\"fn\", axis=1).columns.values\n",
    "for file in preds_to_average[1:]:\n",
    "    tmp = pd.read_csv(s + file)\n",
    "    source_pred[pred_cols] += tmp[pred_cols]\n",
    "\n",
    "source_pred[pred_cols] /= len(preds_to_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_pred.to_csv(f'tmp/{results_folder}/{results_folder}_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
