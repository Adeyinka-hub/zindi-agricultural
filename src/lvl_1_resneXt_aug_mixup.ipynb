{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import typing as tp\n",
    "import pathlib\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_speech_features as psf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"../data\")\n",
    "audios_path = data_path / \"all_audio_resampled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path/'Train.csv')\n",
    "train_extra = pd.read_csv(data_path/'train_add.csv')\n",
    "train_extra_2 = pd.read_csv(data_path/'train_add_20201029.csv')\n",
    "\n",
    "label2code = {word: idx for idx, word in enumerate(train.label.unique().tolist())}\n",
    "code2label = {v:k for k,v in label2code.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(audio_path):\n",
    "    file_name = audio_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    ip = str(audios_path.resolve() / f\"{file_name}.wav\")\n",
    "    return ip\n",
    "\n",
    "train[\"image_fn\"] = train.fn.apply(get_image_path)\n",
    "train_extra[\"image_fn\"] = train_extra.fn.apply(get_image_path)\n",
    "train_extra_2[\"image_fn\"] = train_extra_2.fn.apply(get_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train, train_extra, train_extra_2], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build validation that includes all classes:\n",
    "\n",
    "\n",
    "vcs = train_df.label.value_counts()\n",
    "\n",
    "## possible schema:\n",
    "# 25+ - take 3\n",
    "# 12-25 - take 2\n",
    "# 12- - take 1\n",
    "\n",
    "def num_for_val(num_examples):\n",
    "    if num_examples >= 25:\n",
    "        return 3\n",
    "    if num_examples >= 12:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "train_df[\"num_examples\"] = train_df.label.map(vcs.to_dict())\n",
    "train_df[\"num_for_val\"] = train_df.num_examples.apply(num_for_val)\n",
    "\n",
    "random.seed(12)\n",
    "train_df_new = pd.DataFrame()\n",
    "for label in train_df.label.unique():\n",
    "    tmp = train_df.loc[train_df.label == label].copy()\n",
    "    tmp[\"dummy\"] = tmp.label.apply(lambda _: random.random())\n",
    "    tmp.sort_values(by=\"dummy\", inplace=True)\n",
    "    tmp[\"rank\"] = range(tmp.shape[0])\n",
    "    train_df_new = pd.concat([train_df_new, tmp])\n",
    "\n",
    "train_df_new.reset_index(drop=True, inplace=True)\n",
    "train_df_new[\"val_subset\"] = train_df_new.num_for_val > train_df_new[\"rank\"]\n",
    "train_df_new.drop(\"dummy\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    set(train_df_new.loc[train_df_new.val_subset].label.unique()) == \n",
    "    set(train_df_new.loc[~train_df_new.val_subset].label.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new[\"val_fold\"] = train_df_new[\"rank\"] // train_df_new.num_for_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 451\n",
      "1 451\n",
      "2 451\n",
      "3 430\n",
      "4 413\n",
      "5 410\n",
      "6 407\n",
      "7 391\n",
      "8 381\n",
      "9 337\n"
     ]
    }
   ],
   "source": [
    "for f in range(10):\n",
    "    print(f, (train_df_new.val_fold == f).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 3\n",
    "train_folds = list()\n",
    "val_folds = list()\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    valf = train_df_new.loc[train_df_new.val_fold == i].copy()\n",
    "    trf = train_df_new.loc[train_df_new.val_fold != i].copy()\n",
    "    \n",
    "    train_folds.append(trf)\n",
    "    val_folds.append(valf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import new_generate_spec, new_build_image, normalize, MEAN, STD\n",
    "from transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    n_fft = 512\n",
    "    hop_size = 32\n",
    "    pad_center = True\n",
    "    trim = True\n",
    "    max_len_sec = 2.6\n",
    "    sr = 22050\n",
    "    img_size = 299\n",
    "    \n",
    "conf = AudioConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_noise = AddNoise(0, 0.06)\n",
    "aug_pitch = PitchShift((-7, 7), sr=conf.sr)\n",
    "\n",
    "train_transforms = UseWithProb(\n",
    "    OneOf([\n",
    "        aug_noise,\n",
    "        aug_pitch\n",
    "    ]),\n",
    "    prob=0.45\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: tp.List[tp.List[str]],\n",
    "        config,\n",
    "        transform=None,\n",
    "        normalize=True\n",
    "    ):\n",
    "        self.file_list = file_list  # list of list: [file_path, label]\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn, word = self.file_list[idx]\n",
    "        audio, _ = librosa.core.load(fn, sr=SR)\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        image = new_build_image(audio, self.config)\n",
    "        \n",
    "        if self.normalize:\n",
    "            norm_image = normalize(np.array(image), mean=MEAN, std=STD)\n",
    "        else:\n",
    "            norm_image = image\n",
    "        \n",
    "        return np.moveaxis(norm_image, 2, 0), label2code[word]\n",
    "    \n",
    "    \n",
    "class SpectrogramTestDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: tp.List[tp.List[str]],\n",
    "        config,\n",
    "        transform=None,\n",
    "        normalize=True\n",
    "    ):\n",
    "        self.file_list = file_list  # list of list: [file_path, label]\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn, word = self.file_list[idx]\n",
    "        audio, _ = librosa.core.load(fn, sr=SR)\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        image = new_build_image(audio, self.config)\n",
    "        \n",
    "        if self.normalize:\n",
    "            norm_image = normalize(np.array(image), mean=MEAN, std=STD)\n",
    "        else:\n",
    "            norm_image = image\n",
    "        \n",
    "        return np.moveaxis(norm_image, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258\n",
      "4258\n",
      "4258\n",
      "=========================\n",
      "451\n",
      "451\n",
      "451\n"
     ]
    }
   ],
   "source": [
    "sdf_train_list = [\n",
    "    SpectrogramDataset(t[[\"image_fn\", \"label\"]].values.tolist(), conf,\n",
    "                       transform=train_transforms, normalize=True)\n",
    "    for t in train_folds\n",
    "]\n",
    "\n",
    "sdf_val_list = [\n",
    "    SpectrogramDataset(v[[\"image_fn\", \"label\"]].values.tolist(), conf, normalize=True)\n",
    "    for v in val_folds\n",
    "]\n",
    "\n",
    "for s in sdf_train_list:\n",
    "    print(len(s))\n",
    "    \n",
    "print(\"=========================\")\n",
    "\n",
    "for s in sdf_val_list:\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def train_mixup_epoch(log_interval, mixup_prob, model, device, criterion, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        use_mixup = False\n",
    "        if random.random() < mixup_prob:\n",
    "            use_mixup = True\n",
    "        data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "        \n",
    "        if use_mixup:\n",
    "            data, y_a, y_b, lam = mixup_data(data, target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        if use_mixup:\n",
    "            loss = mixup_criterion(criterion, output, y_a, y_b, lam) #criterion(output, target)\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "\n",
    "            \n",
    "def test(model, device, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def train_on_fold(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    fold,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    results_folder,\n",
    "    epochs\n",
    "):\n",
    "    seed_dict = {\n",
    "        0: 9,\n",
    "        1: 99,\n",
    "        2: 999\n",
    "    }\n",
    "    set_seed(seed_dict[fold])\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    best_loss = 1e5\n",
    "    best_acc = 0\n",
    "\n",
    "    max_patience = 20\n",
    "    patience = 0\n",
    "\n",
    "    train_loss_hist = list()\n",
    "    val_loss_hist = list()\n",
    "    val_acc_hist = list()\n",
    "\n",
    "    save_each_epoch = False\n",
    "\n",
    "    for ep in range(65):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.667, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{fold}_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "\n",
    "        lr_scheduler.step(cur_loss) \n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "        \n",
    "    for ep in range(15):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.0, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{fold}_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "\n",
    "        lr_scheduler.step(cur_loss)\n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "\n",
    "    print(\"time spent training: {} minutes\".format((time.time() - t0) / 60))\n",
    "    print(\"BEST LOSS:\", best_loss)\n",
    "    print(\"BEST ACC:\", best_acc)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"tmp/{results_folder}/best_run_{fold}.pth\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_on_val(\n",
    "    model,\n",
    "    val_loader,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, (inputs, _) in enumerate(val_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs))\n",
    "\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_val_outputs(model, test_loader, device=\"cuda\"):\n",
    "    outputs_list = list()\n",
    "    for batch_idx, (inputs, target) in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            outputs_list.append(outputs)\n",
    "    outputs_list = np.concatenate([t.cpu().numpy() for t in outputs_list])\n",
    "    return outputs_list\n",
    "\n",
    "def get_predictions(model, test_loader, device=\"cuda\"):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, inputs in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs)) ## ADD SOFTMAX\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft.load_state_dict(torch.load(f\"tmp/{tmp_folder_name}/best_run_{FOLD}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FOLD 0\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 167.946594\n",
      "\n",
      "Test set: Average loss: 5.1759, Accuracy: 9/451 (2%)\n",
      "\n",
      "Training so far 1.5153255303700766 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 159.280701\n",
      "\n",
      "Test set: Average loss: 4.1223, Accuracy: 39/451 (9%)\n",
      "\n",
      "Training so far 3.0794251402219137 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 145.556137\n",
      "\n",
      "Test set: Average loss: 3.0141, Accuracy: 109/451 (24%)\n",
      "\n",
      "Training so far 4.640584393342336 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 119.891464\n",
      "\n",
      "Test set: Average loss: 2.4041, Accuracy: 187/451 (41%)\n",
      "\n",
      "Training so far 6.194824632008871 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 127.730530\n",
      "\n",
      "Test set: Average loss: 2.1588, Accuracy: 209/451 (46%)\n",
      "\n",
      "Training so far 7.736872899532318 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 48.023014\n",
      "\n",
      "Test set: Average loss: 1.7141, Accuracy: 276/451 (61%)\n",
      "\n",
      "Training so far 9.285128859678904 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 29.441177\n",
      "\n",
      "Test set: Average loss: 1.4866, Accuracy: 291/451 (65%)\n",
      "\n",
      "Training so far 10.829892679055531 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 39.083138\n",
      "\n",
      "Test set: Average loss: 1.4261, Accuracy: 302/451 (67%)\n",
      "\n",
      "Training so far 12.382770935694376 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 120.484970\n",
      "\n",
      "Test set: Average loss: 1.2637, Accuracy: 307/451 (68%)\n",
      "\n",
      "Training so far 13.93421666622162 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 88.822174\n",
      "\n",
      "Test set: Average loss: 1.1649, Accuracy: 326/451 (72%)\n",
      "\n",
      "Training so far 15.485812389850617 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 49.267761\n",
      "\n",
      "Test set: Average loss: 1.2046, Accuracy: 325/451 (72%)\n",
      "\n",
      "Training so far 17.030679146448772 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 106.311691\n",
      "\n",
      "Test set: Average loss: 1.0853, Accuracy: 345/451 (76%)\n",
      "\n",
      "Training so far 18.589395912488303 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 92.512932\n",
      "\n",
      "Test set: Average loss: 1.1169, Accuracy: 336/451 (75%)\n",
      "\n",
      "Training so far 20.13818576733271 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 14.680676\n",
      "\n",
      "Test set: Average loss: 1.0131, Accuracy: 350/451 (78%)\n",
      "\n",
      "Training so far 21.68040701945623 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 18.967157\n",
      "\n",
      "Test set: Average loss: 0.9273, Accuracy: 359/451 (80%)\n",
      "\n",
      "Training so far 23.224989998340607 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 27.401525\n",
      "\n",
      "Test set: Average loss: 0.9902, Accuracy: 355/451 (79%)\n",
      "\n",
      "Training so far 24.774685720602672 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 73.742844\n",
      "\n",
      "Test set: Average loss: 1.0478, Accuracy: 354/451 (78%)\n",
      "\n",
      "Training so far 26.32297365665436 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 14.291830\n",
      "\n",
      "Test set: Average loss: 1.0780, Accuracy: 354/451 (78%)\n",
      "\n",
      "Training so far 27.860452250639597 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 87.454117\n",
      "\n",
      "Test set: Average loss: 0.8615, Accuracy: 371/451 (82%)\n",
      "\n",
      "Training so far 29.40432438055674 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 10.687714\n",
      "\n",
      "Test set: Average loss: 0.9389, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 30.951492524147035 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 7.062159\n",
      "\n",
      "Test set: Average loss: 0.8966, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 32.50169030427933 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 46.620964\n",
      "\n",
      "Test set: Average loss: 0.8389, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 34.048859473069506 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 3.850983\n",
      "\n",
      "Test set: Average loss: 0.7213, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 35.598326969146726 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 12.092811\n",
      "\n",
      "Test set: Average loss: 0.8168, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 37.13999491930008 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 76.476791\n",
      "\n",
      "Test set: Average loss: 0.7811, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 38.681560643514 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 9.836008\n",
      "\n",
      "Test set: Average loss: 0.8425, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 40.223036245505014 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 11.884781\n",
      "\n",
      "Test set: Average loss: 0.9232, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 41.78251685698827 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 80.952316\n",
      "\n",
      "Test set: Average loss: 0.8609, Accuracy: 376/451 (83%)\n",
      "\n",
      "Epoch    28: reducing learning rate of group 0 to 1.5000e-04.\n",
      "Training so far 43.327774421374 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 1.719569\n",
      "\n",
      "Test set: Average loss: 0.6963, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 44.88435739278793 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 24.792707\n",
      "\n",
      "Test set: Average loss: 0.6871, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 46.42021559476852 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 2.449929\n",
      "\n",
      "Test set: Average loss: 0.6315, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 47.96662319501241 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 1.675677\n",
      "\n",
      "Test set: Average loss: 0.6631, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 49.5124143799146 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 78.852081\n",
      "\n",
      "Test set: Average loss: 0.6372, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 51.06178245544434 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 1.498898\n",
      "\n",
      "Test set: Average loss: 0.6088, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 52.616497846444446 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 41.608772\n",
      "\n",
      "Test set: Average loss: 0.6383, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 54.168954678376515 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 27.683403\n",
      "\n",
      "Test set: Average loss: 0.6275, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 55.714006980260216 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 67.367096\n",
      "\n",
      "Test set: Average loss: 0.6079, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 57.260147241751355 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 64.839745\n",
      "\n",
      "Test set: Average loss: 0.6126, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 58.79727387030919 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 55.956722\n",
      "\n",
      "Test set: Average loss: 0.6362, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 60.33186877171199 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 72.398224\n",
      "\n",
      "Test set: Average loss: 0.6239, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 61.86943524281184 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 80.640724\n",
      "\n",
      "Test set: Average loss: 0.5940, Accuracy: 404/451 (90%)\n",
      "\n",
      "Training so far 63.41071307659149 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 4.504585\n",
      "\n",
      "Test set: Average loss: 0.5749, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 64.96008781989416 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 21.613941\n",
      "\n",
      "Test set: Average loss: 0.6295, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 66.50438007116318 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 63.798698\n",
      "\n",
      "Test set: Average loss: 0.5740, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 68.05596530040106 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 74.979378\n",
      "\n",
      "Test set: Average loss: 0.6120, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 69.59580955902736 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 21.574112\n",
      "\n",
      "Test set: Average loss: 0.5984, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 71.14322136640548 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 70.614868\n",
      "\n",
      "Test set: Average loss: 0.6146, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 72.67973332802454 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 4.507791\n",
      "\n",
      "Test set: Average loss: 0.5824, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 74.21970599492391 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 80.952881\n",
      "\n",
      "Test set: Average loss: 0.5918, Accuracy: 397/451 (88%)\n",
      "\n",
      "Epoch    49: reducing learning rate of group 0 to 7.5000e-05.\n",
      "Training so far 75.76479982932409 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 37.576870\n",
      "\n",
      "Test set: Average loss: 0.5470, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 77.31701451539993 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 1.479512\n",
      "\n",
      "Test set: Average loss: 0.5564, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 78.87322177092234 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 7.939745\n",
      "\n",
      "Test set: Average loss: 0.5513, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 80.42570188442866 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 0.923042\n",
      "\n",
      "Test set: Average loss: 0.5759, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 81.97701182762782 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 0.220467\n",
      "\n",
      "Test set: Average loss: 0.5757, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 83.52642560799917 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 6.876816\n",
      "\n",
      "Test set: Average loss: 0.5647, Accuracy: 392/451 (87%)\n",
      "\n",
      "Epoch    55: reducing learning rate of group 0 to 3.7500e-05.\n",
      "Training so far 85.07850575049719 minutes\n",
      "====================\n",
      "Train Epoch: 55 [0/4258 (0%)]\tLoss: 0.416518\n",
      "\n",
      "Test set: Average loss: 0.5532, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 86.62722442150115 minutes\n",
      "====================\n",
      "Train Epoch: 56 [0/4258 (0%)]\tLoss: 11.927226\n",
      "\n",
      "Test set: Average loss: 0.5393, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 88.17887292702993 minutes\n",
      "====================\n",
      "Train Epoch: 57 [0/4258 (0%)]\tLoss: 59.704475\n",
      "\n",
      "Test set: Average loss: 0.5488, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 89.72539122899373 minutes\n",
      "====================\n",
      "Train Epoch: 58 [0/4258 (0%)]\tLoss: 67.397018\n",
      "\n",
      "Test set: Average loss: 0.5443, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 91.28173406521479 minutes\n",
      "====================\n",
      "Train Epoch: 59 [0/4258 (0%)]\tLoss: 38.355293\n",
      "\n",
      "Test set: Average loss: 0.5364, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 92.82976558208466 minutes\n",
      "====================\n",
      "Train Epoch: 60 [0/4258 (0%)]\tLoss: 52.240963\n",
      "\n",
      "Test set: Average loss: 0.5444, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 94.3764344771703 minutes\n",
      "====================\n",
      "Train Epoch: 61 [0/4258 (0%)]\tLoss: 23.014545\n",
      "\n",
      "Test set: Average loss: 0.5384, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 95.90552524725597 minutes\n",
      "====================\n",
      "Train Epoch: 62 [0/4258 (0%)]\tLoss: 0.287264\n",
      "\n",
      "Test set: Average loss: 0.5452, Accuracy: 405/451 (90%)\n",
      "\n",
      "Training so far 97.45589069525401 minutes\n",
      "====================\n",
      "Train Epoch: 63 [0/4258 (0%)]\tLoss: 54.350174\n",
      "\n",
      "Test set: Average loss: 0.5549, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 99.01512613693873 minutes\n",
      "====================\n",
      "Train Epoch: 64 [0/4258 (0%)]\tLoss: 65.365387\n",
      "\n",
      "Test set: Average loss: 0.5385, Accuracy: 404/451 (90%)\n",
      "\n",
      "Epoch    65: reducing learning rate of group 0 to 1.8750e-05.\n",
      "Training so far 100.5655680457751 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.246861\n",
      "\n",
      "Test set: Average loss: 0.5055, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 102.11107531785964 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 3.918965\n",
      "\n",
      "Test set: Average loss: 0.5131, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 103.66408833265305 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.154004\n",
      "\n",
      "Test set: Average loss: 0.4987, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 105.20668847958247 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 1.191742\n",
      "\n",
      "Test set: Average loss: 0.5122, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 106.75268895626068 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.996310\n",
      "\n",
      "Test set: Average loss: 0.5153, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 108.30098625818889 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 4.568752\n",
      "\n",
      "Test set: Average loss: 0.5068, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 109.84654216368993 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 1.387413\n",
      "\n",
      "Test set: Average loss: 0.5082, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 111.37871931393941 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.388017\n",
      "\n",
      "Test set: Average loss: 0.5052, Accuracy: 401/451 (89%)\n",
      "\n",
      "Epoch    73: reducing learning rate of group 0 to 1.5000e-05.\n",
      "Training so far 112.93235658407211 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.123990\n",
      "\n",
      "Test set: Average loss: 0.5190, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 114.47079257567724 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.230800\n",
      "\n",
      "Test set: Average loss: 0.5076, Accuracy: 403/451 (89%)\n",
      "\n",
      "Training so far 116.00737091700236 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 0.173778\n",
      "\n",
      "Test set: Average loss: 0.5157, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 117.55271473328273 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 0.142473\n",
      "\n",
      "Test set: Average loss: 0.5266, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 119.0983073592186 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 0.085784\n",
      "\n",
      "Test set: Average loss: 0.5300, Accuracy: 401/451 (89%)\n",
      "\n",
      "Training so far 120.62943223714828 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 0.808438\n",
      "\n",
      "Test set: Average loss: 0.5120, Accuracy: 404/451 (90%)\n",
      "\n",
      "Training so far 122.17355731725692 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 4.049104\n",
      "\n",
      "Test set: Average loss: 0.5090, Accuracy: 402/451 (89%)\n",
      "\n",
      "Training so far 123.71325188080469 minutes\n",
      "====================\n",
      "time spent training: 123.71325219869614 minutes\n",
      "BEST LOSS: 0.4987426639396177\n",
      "BEST ACC: 88.24833702882484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/letfoolsdie/virtual_envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/letfoolsdie/virtual_envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:134: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FOLD 1\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 172.283020\n",
      "\n",
      "Test set: Average loss: 4.5344, Accuracy: 25/451 (6%)\n",
      "\n",
      "Training so far 1.5400746703147887 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 138.585983\n",
      "\n",
      "Test set: Average loss: 3.7370, Accuracy: 54/451 (12%)\n",
      "\n",
      "Training so far 3.082967495918274 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 97.342072\n",
      "\n",
      "Test set: Average loss: 2.7583, Accuracy: 150/451 (33%)\n",
      "\n",
      "Training so far 4.63525630235672 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 111.559860\n",
      "\n",
      "Test set: Average loss: 2.2827, Accuracy: 199/451 (44%)\n",
      "\n",
      "Training so far 6.192296330134074 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 68.361694\n",
      "\n",
      "Test set: Average loss: 2.0577, Accuracy: 220/451 (49%)\n",
      "\n",
      "Training so far 7.736361793677012 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 55.552338\n",
      "\n",
      "Test set: Average loss: 1.6227, Accuracy: 284/451 (63%)\n",
      "\n",
      "Training so far 9.288157935937246 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 44.689812\n",
      "\n",
      "Test set: Average loss: 1.4419, Accuracy: 299/451 (66%)\n",
      "\n",
      "Training so far 10.8269167582194 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 108.601044\n",
      "\n",
      "Test set: Average loss: 1.3076, Accuracy: 320/451 (71%)\n",
      "\n",
      "Training so far 12.379441356658935 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 101.219376\n",
      "\n",
      "Test set: Average loss: 1.1965, Accuracy: 329/451 (73%)\n",
      "\n",
      "Training so far 13.920428649584453 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 85.485687\n",
      "\n",
      "Test set: Average loss: 1.1393, Accuracy: 329/451 (73%)\n",
      "\n",
      "Training so far 15.476206648349763 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 100.321655\n",
      "\n",
      "Test set: Average loss: 1.0893, Accuracy: 336/451 (75%)\n",
      "\n",
      "Training so far 17.029006493091583 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 23.083351\n",
      "\n",
      "Test set: Average loss: 1.0471, Accuracy: 352/451 (78%)\n",
      "\n",
      "Training so far 18.58229777018229 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 14.835138\n",
      "\n",
      "Test set: Average loss: 1.0906, Accuracy: 347/451 (77%)\n",
      "\n",
      "Training so far 20.119411877791087 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 88.591553\n",
      "\n",
      "Test set: Average loss: 0.9614, Accuracy: 350/451 (78%)\n",
      "\n",
      "Training so far 21.66821612517039 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 33.839806\n",
      "\n",
      "Test set: Average loss: 1.1228, Accuracy: 349/451 (77%)\n",
      "\n",
      "Training so far 23.207032465934752 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 16.536682\n",
      "\n",
      "Test set: Average loss: 1.0232, Accuracy: 356/451 (79%)\n",
      "\n",
      "Training so far 24.742409404118856 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 10.686503\n",
      "\n",
      "Test set: Average loss: 1.0573, Accuracy: 352/451 (78%)\n",
      "\n",
      "Training so far 26.27970965305964 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 8.307584\n",
      "\n",
      "Test set: Average loss: 0.8908, Accuracy: 367/451 (81%)\n",
      "\n",
      "Training so far 27.818051433563234 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 63.161369\n",
      "\n",
      "Test set: Average loss: 1.0012, Accuracy: 366/451 (81%)\n",
      "\n",
      "Training so far 29.35666275024414 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 12.682893\n",
      "\n",
      "Test set: Average loss: 0.8619, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 30.906429354349772 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 28.578459\n",
      "\n",
      "Test set: Average loss: 0.8898, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 32.43965193033218 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 113.665161\n",
      "\n",
      "Test set: Average loss: 0.9534, Accuracy: 370/451 (82%)\n",
      "\n",
      "Training so far 33.9862634340922 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 12.454817\n",
      "\n",
      "Test set: Average loss: 0.8441, Accuracy: 370/451 (82%)\n",
      "\n",
      "Training so far 35.52218623956045 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 95.767319\n",
      "\n",
      "Test set: Average loss: 0.8411, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 37.07804690202077 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 59.993729\n",
      "\n",
      "Test set: Average loss: 0.8542, Accuracy: 375/451 (83%)\n",
      "\n",
      "Training so far 38.61067606210709 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 10.455479\n",
      "\n",
      "Test set: Average loss: 0.8287, Accuracy: 379/451 (84%)\n",
      "\n",
      "Training so far 40.159954981009164 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 81.253258\n",
      "\n",
      "Test set: Average loss: 0.7913, Accuracy: 370/451 (82%)\n",
      "\n",
      "Training so far 41.7095898270607 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 55.955898\n",
      "\n",
      "Test set: Average loss: 0.8902, Accuracy: 370/451 (82%)\n",
      "\n",
      "Training so far 43.24913212855657 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 8.132092\n",
      "\n",
      "Test set: Average loss: 0.8588, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 44.80195283492406 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 7.080399\n",
      "\n",
      "Test set: Average loss: 0.8254, Accuracy: 372/451 (82%)\n",
      "\n",
      "Training so far 46.3491641720136 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 60.791519\n",
      "\n",
      "Test set: Average loss: 0.8331, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 47.89466678698857 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 52.413483\n",
      "\n",
      "Test set: Average loss: 0.8142, Accuracy: 377/451 (84%)\n",
      "\n",
      "Epoch    32: reducing learning rate of group 0 to 1.5000e-04.\n",
      "Training so far 49.44279268185298 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 74.005798\n",
      "\n",
      "Test set: Average loss: 0.7556, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 51.002018344402316 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 63.546120\n",
      "\n",
      "Test set: Average loss: 0.7707, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 52.54536991914113 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 58.235928\n",
      "\n",
      "Test set: Average loss: 0.7138, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 54.092137185732525 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 37.200970\n",
      "\n",
      "Test set: Average loss: 0.7285, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 55.62710209687551 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 59.135029\n",
      "\n",
      "Test set: Average loss: 0.7263, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 57.17452311515808 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 81.765358\n",
      "\n",
      "Test set: Average loss: 0.7268, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 58.729662950833635 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 0.532655\n",
      "\n",
      "Test set: Average loss: 0.7630, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 60.26759063402812 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 51.941364\n",
      "\n",
      "Test set: Average loss: 0.6886, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 61.81977622906367 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 0.572876\n",
      "\n",
      "Test set: Average loss: 0.7188, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 63.37247094710668 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 0.495521\n",
      "\n",
      "Test set: Average loss: 0.7071, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 64.9162659406662 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 63.793350\n",
      "\n",
      "Test set: Average loss: 0.7355, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 66.4631012002627 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 33.650944\n",
      "\n",
      "Test set: Average loss: 0.6807, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 68.00430905421575 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 39.972725\n",
      "\n",
      "Test set: Average loss: 0.7361, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 69.55060018698374 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 4.601328\n",
      "\n",
      "Test set: Average loss: 0.7693, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 71.08887913624446 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 8.890680\n",
      "\n",
      "Test set: Average loss: 0.7397, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 72.63678133885065 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 0.359860\n",
      "\n",
      "Test set: Average loss: 0.7199, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 74.17222012678782 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 1.066449\n",
      "\n",
      "Test set: Average loss: 0.6848, Accuracy: 391/451 (87%)\n",
      "\n",
      "Epoch    49: reducing learning rate of group 0 to 7.5000e-05.\n",
      "Training so far 75.73443871339163 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 9.192108\n",
      "\n",
      "Test set: Average loss: 0.6624, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 77.29701682329178 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 0.366875\n",
      "\n",
      "Test set: Average loss: 0.6607, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 78.84668300946554 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 0.193857\n",
      "\n",
      "Test set: Average loss: 0.6909, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 80.38284516334534 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 52.120880\n",
      "\n",
      "Test set: Average loss: 0.6849, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 81.9317910472552 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 67.761871\n",
      "\n",
      "Test set: Average loss: 0.7016, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 83.47660151720046 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 24.773466\n",
      "\n",
      "Test set: Average loss: 0.7016, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 85.01809465487798 minutes\n",
      "====================\n",
      "Train Epoch: 55 [0/4258 (0%)]\tLoss: 40.263321\n",
      "\n",
      "Test set: Average loss: 0.6794, Accuracy: 394/451 (87%)\n",
      "\n",
      "Epoch    56: reducing learning rate of group 0 to 3.7500e-05.\n",
      "Training so far 86.56346210241318 minutes\n",
      "====================\n",
      "Train Epoch: 56 [0/4258 (0%)]\tLoss: 0.996593\n",
      "\n",
      "Test set: Average loss: 0.6466, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 88.10194772084554 minutes\n",
      "====================\n",
      "Train Epoch: 57 [0/4258 (0%)]\tLoss: 5.498797\n",
      "\n",
      "Test set: Average loss: 0.6630, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 89.6516918738683 minutes\n",
      "====================\n",
      "Train Epoch: 58 [0/4258 (0%)]\tLoss: 0.741618\n",
      "\n",
      "Test set: Average loss: 0.6575, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 91.19285074869792 minutes\n",
      "====================\n",
      "Train Epoch: 59 [0/4258 (0%)]\tLoss: 22.007345\n",
      "\n",
      "Test set: Average loss: 0.6691, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 92.74168830315271 minutes\n",
      "====================\n",
      "Train Epoch: 60 [0/4258 (0%)]\tLoss: 0.120815\n",
      "\n",
      "Test set: Average loss: 0.6747, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 94.2895473321279 minutes\n",
      "====================\n",
      "Train Epoch: 61 [0/4258 (0%)]\tLoss: 31.204174\n",
      "\n",
      "Test set: Average loss: 0.7042, Accuracy: 393/451 (87%)\n",
      "\n",
      "Epoch    62: reducing learning rate of group 0 to 1.8750e-05.\n",
      "Training so far 95.83765074412028 minutes\n",
      "====================\n",
      "Train Epoch: 62 [0/4258 (0%)]\tLoss: 70.195312\n",
      "\n",
      "Test set: Average loss: 0.6801, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 97.37599260409674 minutes\n",
      "====================\n",
      "Train Epoch: 63 [0/4258 (0%)]\tLoss: 5.338391\n",
      "\n",
      "Test set: Average loss: 0.6843, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 98.92522834539413 minutes\n",
      "====================\n",
      "Train Epoch: 64 [0/4258 (0%)]\tLoss: 75.103050\n",
      "\n",
      "Test set: Average loss: 0.6833, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 100.47271713018418 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.190853\n",
      "\n",
      "Test set: Average loss: 0.7018, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 102.0183074037234 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 2.116725\n",
      "\n",
      "Test set: Average loss: 0.6932, Accuracy: 396/451 (88%)\n",
      "\n",
      "Epoch    67: reducing learning rate of group 0 to 1.5000e-05.\n",
      "Training so far 103.56546693245569 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 4.369259\n",
      "\n",
      "Test set: Average loss: 0.6978, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 105.12023995320003 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.285040\n",
      "\n",
      "Test set: Average loss: 0.7106, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 106.66111465295155 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 4.855248\n",
      "\n",
      "Test set: Average loss: 0.7041, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 108.21871294577916 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.166839\n",
      "\n",
      "Test set: Average loss: 0.7102, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 109.76751861174901 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.102586\n",
      "\n",
      "Test set: Average loss: 0.6366, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 111.30518593390782 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 3.714002\n",
      "\n",
      "Test set: Average loss: 0.7109, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 112.85319244464239 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.749609\n",
      "\n",
      "Test set: Average loss: 0.6941, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 114.40315291484197 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.129773\n",
      "\n",
      "Test set: Average loss: 0.7002, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 115.94841104745865 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 0.188885\n",
      "\n",
      "Test set: Average loss: 0.7015, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 117.49516626199086 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 0.379621\n",
      "\n",
      "Test set: Average loss: 0.7204, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 119.04737088680267 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 4.582103\n",
      "\n",
      "Test set: Average loss: 0.7027, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 120.59156274795532 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 0.349048\n",
      "\n",
      "Test set: Average loss: 0.7126, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 122.12967121998469 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 0.298517\n",
      "\n",
      "Test set: Average loss: 0.7050, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 123.67944646676382 minutes\n",
      "====================\n",
      "time spent training: 123.6794468084971 minutes\n",
      "BEST LOSS: 0.6365518464217429\n",
      "BEST ACC: 87.58314855875831\n",
      "PROCESSING FOLD 2\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 167.002823\n",
      "\n",
      "Test set: Average loss: 5.9868, Accuracy: 7/451 (2%)\n",
      "\n",
      "Training so far 1.5329873164494832 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 141.523987\n",
      "\n",
      "Test set: Average loss: 4.0971, Accuracy: 29/451 (6%)\n",
      "\n",
      "Training so far 3.076790726184845 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 142.665039\n",
      "\n",
      "Test set: Average loss: 3.0840, Accuracy: 101/451 (22%)\n",
      "\n",
      "Training so far 4.631178335348765 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 94.230194\n",
      "\n",
      "Test set: Average loss: 3.0805, Accuracy: 114/451 (25%)\n",
      "\n",
      "Training so far 6.183788355191549 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 137.565308\n",
      "\n",
      "Test set: Average loss: 2.1120, Accuracy: 236/451 (52%)\n",
      "\n",
      "Training so far 7.7310635526975 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 134.632690\n",
      "\n",
      "Test set: Average loss: 1.8154, Accuracy: 272/451 (60%)\n",
      "\n",
      "Training so far 9.273507050673167 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 119.449242\n",
      "\n",
      "Test set: Average loss: 1.6635, Accuracy: 283/451 (63%)\n",
      "\n",
      "Training so far 10.822893186410267 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 34.915092\n",
      "\n",
      "Test set: Average loss: 1.5187, Accuracy: 295/451 (65%)\n",
      "\n",
      "Training so far 12.355841159820557 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 30.752604\n",
      "\n",
      "Test set: Average loss: 1.3145, Accuracy: 324/451 (72%)\n",
      "\n",
      "Training so far 13.903234688440959 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 103.208832\n",
      "\n",
      "Test set: Average loss: 1.3191, Accuracy: 313/451 (69%)\n",
      "\n",
      "Training so far 15.453077868620555 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 23.821194\n",
      "\n",
      "Test set: Average loss: 1.2441, Accuracy: 321/451 (71%)\n",
      "\n",
      "Training so far 17.000293393929798 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 104.609451\n",
      "\n",
      "Test set: Average loss: 1.1390, Accuracy: 331/451 (73%)\n",
      "\n",
      "Training so far 18.548451725641886 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 56.850235\n",
      "\n",
      "Test set: Average loss: 1.0736, Accuracy: 350/451 (78%)\n",
      "\n",
      "Training so far 20.084938168525696 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 17.824188\n",
      "\n",
      "Test set: Average loss: 1.1802, Accuracy: 339/451 (75%)\n",
      "\n",
      "Training so far 21.627142147223154 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 66.315941\n",
      "\n",
      "Test set: Average loss: 1.7404, Accuracy: 290/451 (64%)\n",
      "\n",
      "Training so far 23.16703171332677 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 49.584557\n",
      "\n",
      "Test set: Average loss: 1.0196, Accuracy: 354/451 (78%)\n",
      "\n",
      "Training so far 24.719191547234853 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 97.159470\n",
      "\n",
      "Test set: Average loss: 0.9774, Accuracy: 363/451 (80%)\n",
      "\n",
      "Training so far 26.268633862336475 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 9.212070\n",
      "\n",
      "Test set: Average loss: 1.0865, Accuracy: 358/451 (79%)\n",
      "\n",
      "Training so far 27.807952133814492 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 62.970444\n",
      "\n",
      "Test set: Average loss: 0.9718, Accuracy: 370/451 (82%)\n",
      "\n",
      "Training so far 29.350396720568337 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 19.435266\n",
      "\n",
      "Test set: Average loss: 1.0613, Accuracy: 354/451 (78%)\n",
      "\n",
      "Training so far 30.901277140776315 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 53.825798\n",
      "\n",
      "Test set: Average loss: 0.9594, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 32.45164190928141 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 38.027248\n",
      "\n",
      "Test set: Average loss: 1.0916, Accuracy: 364/451 (81%)\n",
      "\n",
      "Training so far 34.00664301713308 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 6.666962\n",
      "\n",
      "Test set: Average loss: 0.9610, Accuracy: 371/451 (82%)\n",
      "\n",
      "Training so far 35.536835730075836 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 7.029746\n",
      "\n",
      "Test set: Average loss: 1.0178, Accuracy: 354/451 (78%)\n",
      "\n",
      "Training so far 37.07965727647146 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 27.879784\n",
      "\n",
      "Test set: Average loss: 0.9615, Accuracy: 366/451 (81%)\n",
      "\n",
      "Training so far 38.61759218374888 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 22.122618\n",
      "\n",
      "Test set: Average loss: 0.8939, Accuracy: 371/451 (82%)\n",
      "\n",
      "Training so far 40.17730489571889 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 31.402107\n",
      "\n",
      "Test set: Average loss: 0.9437, Accuracy: 364/451 (81%)\n",
      "\n",
      "Training so far 41.73939150571823 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 101.148758\n",
      "\n",
      "Test set: Average loss: 0.9075, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 43.286324266592665 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 41.540890\n",
      "\n",
      "Test set: Average loss: 0.8962, Accuracy: 364/451 (81%)\n",
      "\n",
      "Training so far 44.84894031683604 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 34.750889\n",
      "\n",
      "Test set: Average loss: 0.9088, Accuracy: 367/451 (81%)\n",
      "\n",
      "Training so far 46.3957098086675 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 85.863464\n",
      "\n",
      "Test set: Average loss: 0.9522, Accuracy: 374/451 (83%)\n",
      "\n",
      "Epoch    31: reducing learning rate of group 0 to 1.5000e-04.\n",
      "Training so far 47.945162955919905 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 5.629035\n",
      "\n",
      "Test set: Average loss: 0.8279, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 49.49003313382467 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 66.631744\n",
      "\n",
      "Test set: Average loss: 0.8306, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 51.019836684068046 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 20.050407\n",
      "\n",
      "Test set: Average loss: 0.7835, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 52.569135852654775 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 5.854529\n",
      "\n",
      "Test set: Average loss: 0.8235, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 54.10669167439143 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 7.101092\n",
      "\n",
      "Test set: Average loss: 0.8040, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 55.65263853470484 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 1.421013\n",
      "\n",
      "Test set: Average loss: 0.8308, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 57.20080881118774 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 84.874130\n",
      "\n",
      "Test set: Average loss: 0.7700, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 58.749729096889496 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 32.454563\n",
      "\n",
      "Test set: Average loss: 0.7891, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 60.30581248203914 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 0.700133\n",
      "\n",
      "Test set: Average loss: 0.8580, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 61.86058992942174 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 10.535961\n",
      "\n",
      "Test set: Average loss: 0.8095, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 63.404428295294444 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 55.564789\n",
      "\n",
      "Test set: Average loss: 0.7982, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 64.93686546484629 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 72.846283\n",
      "\n",
      "Test set: Average loss: 0.8424, Accuracy: 375/451 (83%)\n",
      "\n",
      "Epoch    43: reducing learning rate of group 0 to 7.5000e-05.\n",
      "Training so far 66.46833907763163 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 2.615692\n",
      "\n",
      "Test set: Average loss: 0.7732, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 68.01508171161016 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 58.266159\n",
      "\n",
      "Test set: Average loss: 0.7941, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 69.55950785477957 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 3.794609\n",
      "\n",
      "Test set: Average loss: 0.7959, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 71.10644098122914 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 1.317123\n",
      "\n",
      "Test set: Average loss: 0.7810, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 72.65097161928813 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 13.567541\n",
      "\n",
      "Test set: Average loss: 0.7891, Accuracy: 386/451 (86%)\n",
      "\n",
      "Epoch    48: reducing learning rate of group 0 to 3.7500e-05.\n",
      "Training so far 74.1923330505689 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 24.057194\n",
      "\n",
      "Test set: Average loss: 0.7979, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 75.73200056155522 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 3.045453\n",
      "\n",
      "Test set: Average loss: 0.7616, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 77.2889773885409 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 6.333744\n",
      "\n",
      "Test set: Average loss: 0.7974, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 78.83181687990825 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 35.522400\n",
      "\n",
      "Test set: Average loss: 0.7816, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 80.36713663339614 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 0.635017\n",
      "\n",
      "Test set: Average loss: 0.7820, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 81.90815714995067 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 0.232036\n",
      "\n",
      "Test set: Average loss: 0.7816, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 83.45716985066731 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 71.823883\n",
      "\n",
      "Test set: Average loss: 0.7630, Accuracy: 385/451 (85%)\n",
      "\n",
      "Epoch    55: reducing learning rate of group 0 to 1.8750e-05.\n",
      "Training so far 85.01444392601648 minutes\n",
      "====================\n",
      "Train Epoch: 55 [0/4258 (0%)]\tLoss: 0.846571\n",
      "\n",
      "Test set: Average loss: 0.7579, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 86.57065150737762 minutes\n",
      "====================\n",
      "Train Epoch: 56 [0/4258 (0%)]\tLoss: 0.179850\n",
      "\n",
      "Test set: Average loss: 0.7775, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 88.11712695360184 minutes\n",
      "====================\n",
      "Train Epoch: 57 [0/4258 (0%)]\tLoss: 54.549801\n",
      "\n",
      "Test set: Average loss: 0.7695, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 89.66156763235728 minutes\n",
      "====================\n",
      "Train Epoch: 58 [0/4258 (0%)]\tLoss: 0.210047\n",
      "\n",
      "Test set: Average loss: 0.7717, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 91.22069799502691 minutes\n",
      "====================\n",
      "Train Epoch: 59 [0/4258 (0%)]\tLoss: 68.861908\n",
      "\n",
      "Test set: Average loss: 0.7677, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 92.77653355995814 minutes\n",
      "====================\n",
      "Train Epoch: 60 [0/4258 (0%)]\tLoss: 0.155115\n",
      "\n",
      "Test set: Average loss: 0.7834, Accuracy: 388/451 (86%)\n",
      "\n",
      "Epoch    61: reducing learning rate of group 0 to 1.5000e-05.\n",
      "Training so far 94.33346704244613 minutes\n",
      "====================\n",
      "Train Epoch: 61 [0/4258 (0%)]\tLoss: 48.081165\n",
      "\n",
      "Test set: Average loss: 0.7628, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 95.87518951892852 minutes\n",
      "====================\n",
      "Train Epoch: 62 [0/4258 (0%)]\tLoss: 48.326164\n",
      "\n",
      "Test set: Average loss: 0.7581, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 97.423577272892 minutes\n",
      "====================\n",
      "Train Epoch: 63 [0/4258 (0%)]\tLoss: 0.198022\n",
      "\n",
      "Test set: Average loss: 0.7580, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 98.97038195927938 minutes\n",
      "====================\n",
      "Train Epoch: 64 [0/4258 (0%)]\tLoss: 0.447722\n",
      "\n",
      "Test set: Average loss: 0.7687, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 100.52255197366078 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.324654\n",
      "\n",
      "Test set: Average loss: 0.7449, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 102.07149009307226 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 0.243031\n",
      "\n",
      "Test set: Average loss: 0.7619, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 103.61487487951915 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.536543\n",
      "\n",
      "Test set: Average loss: 0.7679, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 105.15392274856568 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.715013\n",
      "\n",
      "Test set: Average loss: 0.7538, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 106.70159174601237 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.202419\n",
      "\n",
      "Test set: Average loss: 0.7581, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 108.25562470356623 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.434759\n",
      "\n",
      "Test set: Average loss: 0.7065, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 109.80693054199219 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.220031\n",
      "\n",
      "Test set: Average loss: 0.7894, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 111.34856045246124 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.414440\n",
      "\n",
      "Test set: Average loss: 0.7868, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 112.88322848081589 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.796618\n",
      "\n",
      "Test set: Average loss: 0.8113, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 114.42320979038874 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 1.328470\n",
      "\n",
      "Test set: Average loss: 0.8108, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 115.96167917648951 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 0.581438\n",
      "\n",
      "Test set: Average loss: 0.7993, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 117.50610983371735 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 0.207712\n",
      "\n",
      "Test set: Average loss: 0.7886, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 119.0494909008344 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 0.189190\n",
      "\n",
      "Test set: Average loss: 0.8058, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 120.60185041824977 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 0.105221\n",
      "\n",
      "Test set: Average loss: 0.8036, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 122.13234639962515 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 0.154026\n",
      "\n",
      "Test set: Average loss: 0.7991, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 123.67893286943436 minutes\n",
      "====================\n",
      "time spent training: 123.6789331873258 minutes\n",
      "BEST LOSS: 0.7064720934087579\n",
      "BEST ACC: 86.9179600886918\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(label2code)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "results_folder = \"resnext_aug_mixup_repr_folds\"\n",
    "os.mkdir(f\"tmp/{results_folder}\")\n",
    "BS = 32\n",
    "for FOLD in range(N_FOLDS):\n",
    "    print(\"PROCESSING FOLD\", FOLD)\n",
    "    \n",
    "    model_ft = models.resnext50_32x4d(pretrained=True)\n",
    "    model_ft.fc = nn.Sequential(\n",
    "        nn.Linear(model_ft.fc.in_features, n_classes)\n",
    "    )\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=3e-4)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer_ft, 'min', patience=4, factor=0.5, verbose=True, min_lr=1.5e-5)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(sdf_train_list[FOLD], batch_size=BS, shuffle=True,\n",
    "                                               num_workers=6, drop_last=False, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(sdf_val_list[FOLD], batch_size=BS, drop_last=False, pin_memory=True,\n",
    "                                        num_workers=6)\n",
    "    \n",
    "    # full train cycle with one fold for cross-val:\n",
    "    model_ft = train_on_fold(\n",
    "        model_ft,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        FOLD,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        lr_scheduler,\n",
    "        results_folder,\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    # predict from one model:\n",
    "    preds = predict_on_val(\n",
    "        model_ft,\n",
    "        val_loader\n",
    "    )\n",
    "    val_folds[FOLD][\"preds\"] = [code2label[c] for c in preds]\n",
    "    \n",
    "    # Save val predictions from one model:\n",
    "    val_out = get_val_outputs(model_ft, val_loader)\n",
    "    gt = val_folds[FOLD].label.map(label2code).values\n",
    "    val_losses = list()\n",
    "    \n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([val_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        val_losses.append(item_loss)\n",
    "    val_losses = np.array(val_losses)\n",
    "    val_folds[FOLD][\"loss\"] = val_losses\n",
    "    val_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=True).to_csv(\n",
    "        f\"tmp/{results_folder}/val_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## for debug, save predictions for train as well:\n",
    "    ### debug train audios:\n",
    "    train_debug_loader = torch.utils.data.DataLoader(sdf_train_list[FOLD], batch_size=BS, shuffle=False,\n",
    "                                               num_workers=6, drop_last=False, pin_memory=True)\n",
    "    train_out = get_val_outputs(model_ft, train_debug_loader)\n",
    "    gt = train_folds[FOLD].label.map(label2code).values\n",
    "    train_losses = list()\n",
    "\n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([train_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        train_losses.append(item_loss)\n",
    "\n",
    "    train_losses = np.array(train_losses)\n",
    "    train_folds[FOLD][\"loss\"] = train_losses\n",
    "    train_folds[FOLD][\"preds\"] = [code2label[c] for c in np.argmax(train_out, axis=1)]\n",
    "    train_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=False).to_csv(\n",
    "        f\"tmp/{results_folder}/train_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## MAKE SUBMISSION (from one model):\n",
    "    sample_subm = pd.read_csv(\"Submission1.csv\")\n",
    "    sample_subm[\"image_fn\"] = sample_subm.fn.apply(get_image_path)\n",
    "    subm_dataset = SpectrogramTestDataset([[path, None] for path in sample_subm.image_fn.values ], conf)\n",
    "    subm_loader = torch.utils.data.DataLoader(subm_dataset, batch_size=16)\n",
    "\n",
    "    preds = get_predictions(model_ft, subm_loader)\n",
    "\n",
    "    for c in sample_subm.columns:\n",
    "        if c in {\"fn\", \"image_fn\"}:\n",
    "            continue\n",
    "        c_idx = label2code[c]\n",
    "        sample_subm[c] = preds[:, c_idx]\n",
    "\n",
    "    sample_subm.drop(\"image_fn\", axis=1).to_csv(f'tmp/{results_folder}/subm_{FOLD}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8824833702882483\n",
      "0.8758314855875831\n",
      "0.869179600886918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i/100) for i in [88.24833702882484,87.58314855875831,86.9179600886918]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAKE ONE SUBMISSION FROM ALL SUBS:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s = f'tmp/{results_folder}/'\n",
    "preds_to_average = [\n",
    "    f\"subm_{f}.csv\"\n",
    "    for f in range(N_FOLDS)\n",
    "]\n",
    "\n",
    "all_subs = list()\n",
    "source_pred = pd.read_csv(s + preds_to_average[0])\n",
    "pred_cols = source_pred.drop(\"fn\", axis=1).columns.values\n",
    "for file in preds_to_average[1:]:\n",
    "    tmp = pd.read_csv(s + file)\n",
    "    source_pred[pred_cols] += tmp[pred_cols]\n",
    "\n",
    "source_pred[pred_cols] /= len(preds_to_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isclose((source_pred[pred_cols].sum(axis=1)).values, np.ones(source_pred.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_pred.to_csv(f'tmp/{results_folder}/{results_folder}_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
