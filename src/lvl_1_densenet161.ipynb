{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import typing as tp\n",
    "import pathlib\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_speech_features as psf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"../data\")\n",
    "audios_path = data_path / \"all_audio_resampled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path/'Train.csv')\n",
    "train_extra = pd.read_csv(data_path/'train_add.csv')\n",
    "train_extra_2 = pd.read_csv(data_path/'train_add_20201029.csv')\n",
    "\n",
    "label2code = {word: idx for idx, word in enumerate(train.label.unique().tolist())}\n",
    "code2label = {v:k for k,v in label2code.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(audio_path):\n",
    "    file_name = audio_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    ip = str(audios_path.resolve() / f\"{file_name}.wav\")\n",
    "    return ip\n",
    "\n",
    "train[\"image_fn\"] = train.fn.apply(get_image_path)\n",
    "train_extra[\"image_fn\"] = train_extra.fn.apply(get_image_path)\n",
    "train_extra_2[\"image_fn\"] = train_extra_2.fn.apply(get_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train, train_extra, train_extra_2], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build validation that includes all classes:\n",
    "\n",
    "\n",
    "vcs = train_df.label.value_counts()\n",
    "\n",
    "## possible schema:\n",
    "# 25+ - take 3\n",
    "# 12-25 - take 2\n",
    "# 12- - take 1\n",
    "\n",
    "def num_for_val(num_examples):\n",
    "    if num_examples >= 25:\n",
    "        return 3\n",
    "    if num_examples >= 12:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "train_df[\"num_examples\"] = train_df.label.map(vcs.to_dict())\n",
    "train_df[\"num_for_val\"] = train_df.num_examples.apply(num_for_val)\n",
    "\n",
    "random.seed(12)\n",
    "train_df_new = pd.DataFrame()\n",
    "for label in train_df.label.unique():\n",
    "    tmp = train_df.loc[train_df.label == label].copy()\n",
    "    tmp[\"dummy\"] = tmp.label.apply(lambda _: random.random())\n",
    "    tmp.sort_values(by=\"dummy\", inplace=True)\n",
    "    tmp[\"rank\"] = range(tmp.shape[0])\n",
    "    train_df_new = pd.concat([train_df_new, tmp])\n",
    "\n",
    "train_df_new.reset_index(drop=True, inplace=True)\n",
    "train_df_new[\"val_subset\"] = train_df_new.num_for_val > train_df_new[\"rank\"]\n",
    "train_df_new.drop(\"dummy\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    set(train_df_new.loc[train_df_new.val_subset].label.unique()) == \n",
    "    set(train_df_new.loc[~train_df_new.val_subset].label.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new[\"val_fold\"] = train_df_new[\"rank\"] // train_df_new.num_for_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 451\n",
      "1 451\n",
      "2 451\n",
      "3 430\n",
      "4 413\n",
      "5 410\n",
      "6 407\n",
      "7 391\n",
      "8 381\n",
      "9 337\n"
     ]
    }
   ],
   "source": [
    "for f in range(10):\n",
    "    print(f, (train_df_new.val_fold == f).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 3\n",
    "train_folds = list()\n",
    "val_folds = list()\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    valf = train_df_new.loc[train_df_new.val_fold == i].copy()\n",
    "    trf = train_df_new.loc[train_df_new.val_fold != i].copy()\n",
    "    \n",
    "    train_folds.append(trf)\n",
    "    val_folds.append(valf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproduce.src.data_processing import new_generate_spec, new_build_image, normalize, MEAN, STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: tp.List[tp.List[str]],\n",
    "        config,\n",
    "        transform=None,\n",
    "        normalize=True\n",
    "    ):\n",
    "        self.file_list = file_list  # list of list: [file_path, label]\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn, word = self.file_list[idx]\n",
    "        audio, _ = librosa.core.load(fn, sr=SR)\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        image = new_build_image(audio, self.config)\n",
    "        \n",
    "        if self.normalize:\n",
    "            norm_image = normalize(np.array(image), mean=MEAN, std=STD)\n",
    "        else:\n",
    "            norm_image = image\n",
    "        \n",
    "        return np.moveaxis(norm_image, 2, 0), label2code[word]\n",
    "    \n",
    "    \n",
    "class SpectrogramTestDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: tp.List[tp.List[str]],\n",
    "        config,\n",
    "        transform=None,\n",
    "        normalize=True\n",
    "    ):\n",
    "        self.file_list = file_list  # list of list: [file_path, label]\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn, word = self.file_list[idx]\n",
    "        audio, _ = librosa.core.load(fn, sr=SR)\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        image = new_build_image(audio, self.config)\n",
    "        \n",
    "        if self.normalize:\n",
    "            norm_image = normalize(np.array(image), mean=MEAN, std=STD)\n",
    "        else:\n",
    "            norm_image = image\n",
    "        \n",
    "        return np.moveaxis(norm_image, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    n_fft = 512\n",
    "    hop_size = 32\n",
    "    pad_center = True\n",
    "    trim = True\n",
    "    max_len_sec = 2.6\n",
    "    sr = 22050\n",
    "    img_size = 299\n",
    "    \n",
    "conf = AudioConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258\n",
      "4258\n",
      "4258\n",
      "=========================\n",
      "451\n",
      "451\n",
      "451\n"
     ]
    }
   ],
   "source": [
    "sdf_train_list = [\n",
    "    SpectrogramDataset(t[[\"image_fn\", \"label\"]].values.tolist(), conf, normalize=True)\n",
    "    for t in train_folds\n",
    "]\n",
    "\n",
    "sdf_val_list = [\n",
    "    SpectrogramDataset(v[[\"image_fn\", \"label\"]].values.tolist(), conf, normalize=True)\n",
    "    for v in val_folds\n",
    "]\n",
    "\n",
    "for s in sdf_train_list:\n",
    "    print(len(s))\n",
    "    \n",
    "print(\"=========================\")\n",
    "\n",
    "for s in sdf_val_list:\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def train_mixup_epoch(log_interval, mixup_prob, model, device, criterion, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        use_mixup = False\n",
    "        if random.random() < mixup_prob:\n",
    "            use_mixup = True\n",
    "        data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "        \n",
    "        if use_mixup:\n",
    "            data, y_a, y_b, lam = mixup_data(data, target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        if use_mixup:\n",
    "            loss = mixup_criterion(criterion, output, y_a, y_b, lam) #criterion(output, target)\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "\n",
    "            \n",
    "def test(model, device, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def train_on_fold(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    fold,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    results_folder,\n",
    "    epochs\n",
    "):\n",
    "    seed_dict = {\n",
    "        0: 9,\n",
    "        1: 99,\n",
    "        2: 999\n",
    "    }\n",
    "    set_seed(seed_dict[fold])\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    best_loss = 1e5\n",
    "    best_acc = 0\n",
    "\n",
    "    max_patience = 20\n",
    "    patience = 0\n",
    "\n",
    "    train_loss_hist = list()\n",
    "    val_loss_hist = list()\n",
    "    val_acc_hist = list()\n",
    "\n",
    "    save_each_epoch = False\n",
    "\n",
    "    for ep in range(55):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.667, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{fold}_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "\n",
    "        lr_scheduler.step(cur_loss) \n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "        \n",
    "    for ep in range(11):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.0, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{fold}_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "\n",
    "        lr_scheduler.step(cur_loss)\n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "\n",
    "    print(\"time spent training: {} minutes\".format((time.time() - t0) / 60))\n",
    "    print(\"BEST LOSS:\", best_loss)\n",
    "    print(\"BEST ACC:\", best_acc)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"tmp/{results_folder}/best_run_{fold}.pth\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_on_val(\n",
    "    model,\n",
    "    val_loader,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, (inputs, _) in enumerate(val_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs))\n",
    "\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_val_outputs(model, test_loader, device=\"cuda\"):\n",
    "    outputs_list = list()\n",
    "    for batch_idx, (inputs, target) in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            outputs_list.append(outputs)\n",
    "    outputs_list = np.concatenate([t.cpu().numpy() for t in outputs_list])\n",
    "    return outputs_list\n",
    "\n",
    "def get_predictions(model, test_loader, device=\"cuda\"):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, inputs in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs)) ## ADD SOFTMAX\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft.load_state_dict(torch.load(f\"tmp/{tmp_folder_name}/best_run_{FOLD}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FOLD 0\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 85.442596\n",
      "\n",
      "Test set: Average loss: 8.8587, Accuracy: 2/451 (0%)\n",
      "\n",
      "Training so far 1.9941056887308757 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 79.447342\n",
      "\n",
      "Test set: Average loss: 4.9973, Accuracy: 8/451 (2%)\n",
      "\n",
      "Training so far 4.0111941734949745 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 79.939110\n",
      "\n",
      "Test set: Average loss: 5.2344, Accuracy: 11/451 (2%)\n",
      "\n",
      "Training so far 6.010459589958191 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 71.284996\n",
      "\n",
      "Test set: Average loss: 7.5468, Accuracy: 9/451 (2%)\n",
      "\n",
      "Training so far 8.022417342662811 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 71.251129\n",
      "\n",
      "Test set: Average loss: 4.4163, Accuracy: 20/451 (4%)\n",
      "\n",
      "Training so far 10.032903158664704 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 61.629967\n",
      "\n",
      "Test set: Average loss: 4.4396, Accuracy: 31/451 (7%)\n",
      "\n",
      "Training so far 12.037156709035237 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 75.835556\n",
      "\n",
      "Test set: Average loss: 3.8339, Accuracy: 57/451 (13%)\n",
      "\n",
      "Training so far 14.04237103064855 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 62.599182\n",
      "\n",
      "Test set: Average loss: 2.9409, Accuracy: 113/451 (25%)\n",
      "\n",
      "Training so far 16.04464726050695 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 40.503380\n",
      "\n",
      "Test set: Average loss: 2.1370, Accuracy: 211/451 (47%)\n",
      "\n",
      "Training so far 18.05117867390315 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 26.276371\n",
      "\n",
      "Test set: Average loss: 1.6878, Accuracy: 260/451 (58%)\n",
      "\n",
      "Training so far 20.05233368476232 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 21.958988\n",
      "\n",
      "Test set: Average loss: 2.1140, Accuracy: 210/451 (47%)\n",
      "\n",
      "Training so far 22.05748963356018 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 29.417778\n",
      "\n",
      "Test set: Average loss: 1.3637, Accuracy: 293/451 (65%)\n",
      "\n",
      "Training so far 24.074398271242778 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 14.731421\n",
      "\n",
      "Test set: Average loss: 1.1849, Accuracy: 311/451 (69%)\n",
      "\n",
      "Training so far 26.08202620744705 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 40.423244\n",
      "\n",
      "Test set: Average loss: 1.1079, Accuracy: 334/451 (74%)\n",
      "\n",
      "Training so far 28.081258026758828 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 61.013428\n",
      "\n",
      "Test set: Average loss: 1.0553, Accuracy: 329/451 (73%)\n",
      "\n",
      "Training so far 30.075758508841197 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 41.681740\n",
      "\n",
      "Test set: Average loss: 1.0347, Accuracy: 331/451 (73%)\n",
      "\n",
      "Training so far 32.07835018634796 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 40.325218\n",
      "\n",
      "Test set: Average loss: 1.0632, Accuracy: 329/451 (73%)\n",
      "\n",
      "Training so far 34.07061038017273 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 32.149120\n",
      "\n",
      "Test set: Average loss: 0.9308, Accuracy: 347/451 (77%)\n",
      "\n",
      "Training so far 36.072181820869446 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 13.905400\n",
      "\n",
      "Test set: Average loss: 0.8328, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 38.09140245914459 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 33.269867\n",
      "\n",
      "Test set: Average loss: 0.7825, Accuracy: 360/451 (80%)\n",
      "\n",
      "Training so far 40.137034324804944 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 44.869896\n",
      "\n",
      "Test set: Average loss: 0.8381, Accuracy: 364/451 (81%)\n",
      "\n",
      "Training so far 42.13690920273463 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 45.794308\n",
      "\n",
      "Test set: Average loss: 0.7932, Accuracy: 362/451 (80%)\n",
      "\n",
      "Training so far 44.13635049661001 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 45.866081\n",
      "\n",
      "Test set: Average loss: 0.8978, Accuracy: 348/451 (77%)\n",
      "\n",
      "Training so far 46.130833899974824 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 35.169666\n",
      "\n",
      "Test set: Average loss: 0.9082, Accuracy: 352/451 (78%)\n",
      "\n",
      "Training so far 48.118314198652904 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 26.046474\n",
      "\n",
      "Test set: Average loss: 0.7739, Accuracy: 372/451 (82%)\n",
      "\n",
      "Training so far 50.11928298473358 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 1.925634\n",
      "\n",
      "Test set: Average loss: 0.6823, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 52.11887473662694 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 48.987579\n",
      "\n",
      "Test set: Average loss: 0.7270, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 54.10937545696894 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 35.673317\n",
      "\n",
      "Test set: Average loss: 0.7684, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 56.09910847345988 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 2.773726\n",
      "\n",
      "Test set: Average loss: 0.7748, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 58.095960013071696 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 2.107969\n",
      "\n",
      "Test set: Average loss: 0.8443, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 60.09472938776016 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 35.941715\n",
      "\n",
      "Test set: Average loss: 0.6530, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 62.09539137283961 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 37.029297\n",
      "\n",
      "Test set: Average loss: 0.7443, Accuracy: 375/451 (83%)\n",
      "\n",
      "Training so far 64.09442501465479 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 26.449203\n",
      "\n",
      "Test set: Average loss: 0.8612, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 66.09322992960612 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 5.117974\n",
      "\n",
      "Test set: Average loss: 0.8371, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 68.08954061667124 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 13.060690\n",
      "\n",
      "Test set: Average loss: 0.9268, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 70.08692047198613 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 0.630897\n",
      "\n",
      "Test set: Average loss: 0.8199, Accuracy: 368/451 (82%)\n",
      "\n",
      "Epoch    36: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Training so far 72.07763164838155 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 0.435710\n",
      "\n",
      "Test set: Average loss: 0.6628, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 74.07079584995905 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 27.759392\n",
      "\n",
      "Test set: Average loss: 0.6712, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 76.06066034634908 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 14.441333\n",
      "\n",
      "Test set: Average loss: 0.6393, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 78.06083043813706 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 32.546211\n",
      "\n",
      "Test set: Average loss: 0.7197, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 80.05232295592626 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 24.397142\n",
      "\n",
      "Test set: Average loss: 0.6839, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 82.04785379568736 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 0.287266\n",
      "\n",
      "Test set: Average loss: 0.7011, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 84.03900019725164 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 30.449097\n",
      "\n",
      "Test set: Average loss: 0.6345, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 86.0407688220342 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 0.175905\n",
      "\n",
      "Test set: Average loss: 0.6675, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 88.04001553058625 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 0.129855\n",
      "\n",
      "Test set: Average loss: 0.6884, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 90.0303827047348 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 0.309670\n",
      "\n",
      "Test set: Average loss: 0.6319, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 92.02847285270691 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 0.186352\n",
      "\n",
      "Test set: Average loss: 0.6873, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 94.02585111061732 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 35.235497\n",
      "\n",
      "Test set: Average loss: 0.6555, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 96.02330051660537 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 18.341398\n",
      "\n",
      "Test set: Average loss: 0.6202, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 98.02671030759811 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 30.209393\n",
      "\n",
      "Test set: Average loss: 0.6320, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 100.01761779387792 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 0.080884\n",
      "\n",
      "Test set: Average loss: 0.6989, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 102.0129777987798 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 29.511700\n",
      "\n",
      "Test set: Average loss: 0.6338, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 104.00855114062627 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 0.140758\n",
      "\n",
      "Test set: Average loss: 0.6974, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 106.0031378587087 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 4.511357\n",
      "\n",
      "Test set: Average loss: 0.6706, Accuracy: 384/451 (85%)\n",
      "\n",
      "Epoch    54: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Training so far 107.99689534902572 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 19.236303\n",
      "\n",
      "Test set: Average loss: 0.6447, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 110.00432965358098 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.082168\n",
      "\n",
      "Test set: Average loss: 0.6036, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 112.06036812067032 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 0.074550\n",
      "\n",
      "Test set: Average loss: 0.5924, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 114.07845033804576 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.058653\n",
      "\n",
      "Test set: Average loss: 0.5859, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 116.0679358124733 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.042634\n",
      "\n",
      "Test set: Average loss: 0.5995, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 118.13239531119665 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.040558\n",
      "\n",
      "Test set: Average loss: 0.5979, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 120.19597254991531 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.024583\n",
      "\n",
      "Test set: Average loss: 0.5798, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 122.26654456456502 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.036223\n",
      "\n",
      "Test set: Average loss: 0.6035, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 124.30488386948903 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.045092\n",
      "\n",
      "Test set: Average loss: 0.6005, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 126.37953486442566 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.015091\n",
      "\n",
      "Test set: Average loss: 0.6265, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 128.4221705039342 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.012189\n",
      "\n",
      "Test set: Average loss: 0.5920, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 130.41842317183813 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 0.013589\n",
      "\n",
      "Test set: Average loss: 0.6025, Accuracy: 395/451 (88%)\n",
      "\n",
      "Epoch    66: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Training so far 132.42950861851375 minutes\n",
      "====================\n",
      "time spent training: 132.4295089006424 minutes\n",
      "BEST LOSS: 0.5798206186611742\n",
      "BEST ACC: 87.13968957871397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/letfoolsdie/virtual_envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/letfoolsdie/virtual_envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:134: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FOLD 1\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 85.458000\n",
      "\n",
      "Test set: Average loss: 5.1605, Accuracy: 3/451 (1%)\n",
      "\n",
      "Training so far 1.989265294869741 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 88.092468\n",
      "\n",
      "Test set: Average loss: 5.0036, Accuracy: 7/451 (2%)\n",
      "\n",
      "Training so far 3.983501180013021 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 77.267563\n",
      "\n",
      "Test set: Average loss: 5.2043, Accuracy: 8/451 (2%)\n",
      "\n",
      "Training so far 5.98129030863444 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 77.231834\n",
      "\n",
      "Test set: Average loss: 4.5449, Accuracy: 19/451 (4%)\n",
      "\n",
      "Training so far 7.976518352826436 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 66.879356\n",
      "\n",
      "Test set: Average loss: 4.2600, Accuracy: 38/451 (8%)\n",
      "\n",
      "Training so far 9.96908281246821 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 61.727505\n",
      "\n",
      "Test set: Average loss: 3.5773, Accuracy: 69/451 (15%)\n",
      "\n",
      "Training so far 11.964548973242442 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 50.228035\n",
      "\n",
      "Test set: Average loss: 2.6071, Accuracy: 174/451 (39%)\n",
      "\n",
      "Training so far 13.957842381795247 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 58.329151\n",
      "\n",
      "Test set: Average loss: 2.1575, Accuracy: 226/451 (50%)\n",
      "\n",
      "Training so far 15.952881395816803 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 58.721130\n",
      "\n",
      "Test set: Average loss: 1.8382, Accuracy: 250/451 (55%)\n",
      "\n",
      "Training so far 17.9444176197052 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 27.450890\n",
      "\n",
      "Test set: Average loss: 1.7271, Accuracy: 248/451 (55%)\n",
      "\n",
      "Training so far 19.93252673546473 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 59.235104\n",
      "\n",
      "Test set: Average loss: 1.4100, Accuracy: 291/451 (65%)\n",
      "\n",
      "Training so far 21.92689959605535 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 34.285046\n",
      "\n",
      "Test set: Average loss: 1.2968, Accuracy: 313/451 (69%)\n",
      "\n",
      "Training so far 23.92144925991694 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 46.539627\n",
      "\n",
      "Test set: Average loss: 1.1753, Accuracy: 330/451 (73%)\n",
      "\n",
      "Training so far 25.908298607667287 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 5.194957\n",
      "\n",
      "Test set: Average loss: 1.4899, Accuracy: 296/451 (66%)\n",
      "\n",
      "Training so far 27.899040242036182 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 44.425583\n",
      "\n",
      "Test set: Average loss: 0.9990, Accuracy: 358/451 (79%)\n",
      "\n",
      "Training so far 29.891242106755573 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 8.355357\n",
      "\n",
      "Test set: Average loss: 1.0147, Accuracy: 352/451 (78%)\n",
      "\n",
      "Training so far 31.87299348115921 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 5.812827\n",
      "\n",
      "Test set: Average loss: 1.0249, Accuracy: 343/451 (76%)\n",
      "\n",
      "Training so far 33.864596887429556 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 3.379985\n",
      "\n",
      "Test set: Average loss: 0.9772, Accuracy: 353/451 (78%)\n",
      "\n",
      "Training so far 35.85175167719523 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 26.456062\n",
      "\n",
      "Test set: Average loss: 0.9590, Accuracy: 362/451 (80%)\n",
      "\n",
      "Training so far 37.83906336625417 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 18.866852\n",
      "\n",
      "Test set: Average loss: 0.8824, Accuracy: 370/451 (82%)\n",
      "\n",
      "Training so far 39.832934908072154 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 27.175545\n",
      "\n",
      "Test set: Average loss: 1.0554, Accuracy: 348/451 (77%)\n",
      "\n",
      "Training so far 41.85379612445831 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 41.573631\n",
      "\n",
      "Test set: Average loss: 0.9517, Accuracy: 361/451 (80%)\n",
      "\n",
      "Training so far 43.84910530646642 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 30.667274\n",
      "\n",
      "Test set: Average loss: 0.8590, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 45.839934357007344 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 46.257629\n",
      "\n",
      "Test set: Average loss: 0.9116, Accuracy: 363/451 (80%)\n",
      "\n",
      "Training so far 47.88833022117615 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 37.480499\n",
      "\n",
      "Test set: Average loss: 0.8053, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 49.875607856114705 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 20.443481\n",
      "\n",
      "Test set: Average loss: 0.9327, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 51.85965702533722 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 1.736494\n",
      "\n",
      "Test set: Average loss: 0.8327, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 53.88454175392787 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 0.455007\n",
      "\n",
      "Test set: Average loss: 0.9510, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 55.978865750630696 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 38.083576\n",
      "\n",
      "Test set: Average loss: 0.8474, Accuracy: 379/451 (84%)\n",
      "\n",
      "Training so far 58.007346038023634 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 18.552948\n",
      "\n",
      "Test set: Average loss: 0.8354, Accuracy: 361/451 (80%)\n",
      "\n",
      "Epoch    30: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Training so far 60.07104377746582 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 35.097366\n",
      "\n",
      "Test set: Average loss: 0.7266, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 62.11066626707713 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 0.515774\n",
      "\n",
      "Test set: Average loss: 0.7739, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 64.100851949056 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 7.064966\n",
      "\n",
      "Test set: Average loss: 0.7726, Accuracy: 379/451 (84%)\n",
      "\n",
      "Training so far 66.09101721843084 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 39.633636\n",
      "\n",
      "Test set: Average loss: 0.7595, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 68.08513693014781 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 0.980012\n",
      "\n",
      "Test set: Average loss: 0.7660, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 70.07765513658524 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 38.605053\n",
      "\n",
      "Test set: Average loss: 0.8222, Accuracy: 375/451 (83%)\n",
      "\n",
      "Epoch    36: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Training so far 72.06781028111776 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 0.384857\n",
      "\n",
      "Test set: Average loss: 0.7731, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 74.05636566480001 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 0.132612\n",
      "\n",
      "Test set: Average loss: 0.7509, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 76.04570478200912 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 0.304987\n",
      "\n",
      "Test set: Average loss: 0.7489, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 78.03773588339487 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 27.470564\n",
      "\n",
      "Test set: Average loss: 0.7815, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 80.02941404978434 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 0.162318\n",
      "\n",
      "Test set: Average loss: 0.7728, Accuracy: 383/451 (85%)\n",
      "\n",
      "Epoch    41: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Training so far 82.04463382959366 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 28.701277\n",
      "\n",
      "Test set: Average loss: 0.7674, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 84.03511008024216 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 15.264724\n",
      "\n",
      "Test set: Average loss: 0.7566, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 86.0224708477656 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 33.364193\n",
      "\n",
      "Test set: Average loss: 0.7440, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 88.00630640983582 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 28.475727\n",
      "\n",
      "Test set: Average loss: 0.7477, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 90.0298092285792 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 34.472610\n",
      "\n",
      "Test set: Average loss: 0.7738, Accuracy: 382/451 (85%)\n",
      "\n",
      "Epoch    46: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Training so far 92.03461987574896 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 23.708527\n",
      "\n",
      "Test set: Average loss: 0.7664, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 94.02172097762426 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 30.604618\n",
      "\n",
      "Test set: Average loss: 0.7582, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 96.00994403759638 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 7.684467\n",
      "\n",
      "Test set: Average loss: 0.7125, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 97.99973359107972 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 8.049229\n",
      "\n",
      "Test set: Average loss: 0.7292, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 99.98929142951965 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 0.089929\n",
      "\n",
      "Test set: Average loss: 0.7388, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 102.00115214188894 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 36.451164\n",
      "\n",
      "Test set: Average loss: 0.7188, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 103.98676419655482 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 12.203717\n",
      "\n",
      "Test set: Average loss: 0.7671, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 105.97477976878484 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 0.108616\n",
      "\n",
      "Test set: Average loss: 0.7247, Accuracy: 384/451 (85%)\n",
      "\n",
      "Epoch    54: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Training so far 107.96411498387654 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 0.078841\n",
      "\n",
      "Test set: Average loss: 0.6982, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 109.95785064697266 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.137820\n",
      "\n",
      "Test set: Average loss: 0.7089, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 111.94553306500117 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 0.079689\n",
      "\n",
      "Test set: Average loss: 0.7075, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 113.934801252683 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.042290\n",
      "\n",
      "Test set: Average loss: 0.7103, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 115.91967711846034 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.047165\n",
      "\n",
      "Test set: Average loss: 0.7120, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 117.9106531937917 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.057002\n",
      "\n",
      "Test set: Average loss: 0.6858, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 119.93896512190501 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.097948\n",
      "\n",
      "Test set: Average loss: 0.6969, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 121.93030901749928 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.028841\n",
      "\n",
      "Test set: Average loss: 0.7130, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 123.92426118850707 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.198862\n",
      "\n",
      "Test set: Average loss: 0.7202, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 125.90787633657456 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.041198\n",
      "\n",
      "Test set: Average loss: 0.6978, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 127.89582246144613 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.050203\n",
      "\n",
      "Test set: Average loss: 0.7183, Accuracy: 391/451 (87%)\n",
      "\n",
      "Epoch    65: reducing learning rate of group 0 to 1.5000e-05.\n",
      "Training so far 129.8818921049436 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 0.101091\n",
      "\n",
      "Test set: Average loss: 0.7141, Accuracy: 391/451 (87%)\n",
      "\n",
      "Training so far 131.87373661200206 minutes\n",
      "====================\n",
      "time spent training: 131.87373696168262 minutes\n",
      "BEST LOSS: 0.685750516581694\n",
      "BEST ACC: 86.69623059866963\n",
      "PROCESSING FOLD 2\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 85.720589\n",
      "\n",
      "Test set: Average loss: 6.7176, Accuracy: 3/451 (1%)\n",
      "\n",
      "Training so far 1.9891732454299926 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 79.018486\n",
      "\n",
      "Test set: Average loss: 4.6527, Accuracy: 10/451 (2%)\n",
      "\n",
      "Training so far 3.9962772091229755 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 78.926590\n",
      "\n",
      "Test set: Average loss: 4.1876, Accuracy: 30/451 (7%)\n",
      "\n",
      "Training so far 6.007482453187307 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 63.925125\n",
      "\n",
      "Test set: Average loss: 3.3696, Accuracy: 92/451 (20%)\n",
      "\n",
      "Training so far 8.007255788644155 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 44.769520\n",
      "\n",
      "Test set: Average loss: 2.5903, Accuracy: 165/451 (37%)\n",
      "\n",
      "Training so far 10.011926511923471 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 39.071327\n",
      "\n",
      "Test set: Average loss: 2.0717, Accuracy: 228/451 (51%)\n",
      "\n",
      "Training so far 12.013761754830679 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 28.697983\n",
      "\n",
      "Test set: Average loss: 1.8076, Accuracy: 250/451 (55%)\n",
      "\n",
      "Training so far 14.022648175557455 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 36.341202\n",
      "\n",
      "Test set: Average loss: 1.6057, Accuracy: 274/451 (61%)\n",
      "\n",
      "Training so far 16.02511862119039 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 13.113619\n",
      "\n",
      "Test set: Average loss: 1.3501, Accuracy: 307/451 (68%)\n",
      "\n",
      "Training so far 18.038240536053976 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 17.197424\n",
      "\n",
      "Test set: Average loss: 1.2287, Accuracy: 317/451 (70%)\n",
      "\n",
      "Training so far 20.04804281393687 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 20.989546\n",
      "\n",
      "Test set: Average loss: 1.3472, Accuracy: 308/451 (68%)\n",
      "\n",
      "Training so far 22.05652181307475 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 19.298082\n",
      "\n",
      "Test set: Average loss: 1.2063, Accuracy: 334/451 (74%)\n",
      "\n",
      "Training so far 24.060779368877412 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 41.776672\n",
      "\n",
      "Test set: Average loss: 1.1251, Accuracy: 332/451 (74%)\n",
      "\n",
      "Training so far 26.069758685429893 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 47.181778\n",
      "\n",
      "Test set: Average loss: 0.9503, Accuracy: 356/451 (79%)\n",
      "\n",
      "Training so far 28.099249362945557 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 17.723354\n",
      "\n",
      "Test set: Average loss: 0.9321, Accuracy: 360/451 (80%)\n",
      "\n",
      "Training so far 30.122360893090566 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 10.486294\n",
      "\n",
      "Test set: Average loss: 0.9967, Accuracy: 350/451 (78%)\n",
      "\n",
      "Training so far 32.15059901078542 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 6.334943\n",
      "\n",
      "Test set: Average loss: 1.0050, Accuracy: 350/451 (78%)\n",
      "\n",
      "Training so far 34.15896993478139 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 25.719215\n",
      "\n",
      "Test set: Average loss: 0.9801, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 36.24133274157842 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 27.842310\n",
      "\n",
      "Test set: Average loss: 1.0785, Accuracy: 347/451 (77%)\n",
      "\n",
      "Training so far 38.31332001288732 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 43.137241\n",
      "\n",
      "Test set: Average loss: 1.0928, Accuracy: 357/451 (79%)\n",
      "\n",
      "Epoch    20: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Training so far 40.43228151400884 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 1.423337\n",
      "\n",
      "Test set: Average loss: 0.8442, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 42.56905837059021 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 26.305779\n",
      "\n",
      "Test set: Average loss: 0.8802, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 44.63016722599665 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 6.341790\n",
      "\n",
      "Test set: Average loss: 0.9263, Accuracy: 364/451 (81%)\n",
      "\n",
      "Training so far 46.690187672773995 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 15.452554\n",
      "\n",
      "Test set: Average loss: 0.8965, Accuracy: 372/451 (82%)\n",
      "\n",
      "Training so far 48.74131469329198 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 41.953072\n",
      "\n",
      "Test set: Average loss: 0.9324, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 50.78972400824229 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 21.087608\n",
      "\n",
      "Test set: Average loss: 0.9553, Accuracy: 360/451 (80%)\n",
      "\n",
      "Epoch    26: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Training so far 52.78933779795965 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 39.912342\n",
      "\n",
      "Test set: Average loss: 0.8669, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 54.79962671995163 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 0.574357\n",
      "\n",
      "Test set: Average loss: 0.9166, Accuracy: 371/451 (82%)\n",
      "\n",
      "Training so far 56.803632231553394 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 33.251282\n",
      "\n",
      "Test set: Average loss: 0.8320, Accuracy: 379/451 (84%)\n",
      "\n",
      "Training so far 58.81161374648412 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 0.349782\n",
      "\n",
      "Test set: Average loss: 0.8514, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 60.811685132980344 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 10.479640\n",
      "\n",
      "Test set: Average loss: 0.8403, Accuracy: 375/451 (83%)\n",
      "\n",
      "Training so far 62.813581871986386 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 31.000462\n",
      "\n",
      "Test set: Average loss: 0.8724, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 64.81982274850209 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 0.708453\n",
      "\n",
      "Test set: Average loss: 0.9101, Accuracy: 372/451 (82%)\n",
      "\n",
      "Training so far 66.82838279803595 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 25.683498\n",
      "\n",
      "Test set: Average loss: 0.8119, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 68.8321104447047 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 33.790890\n",
      "\n",
      "Test set: Average loss: 0.8609, Accuracy: 374/451 (83%)\n",
      "\n",
      "Training so far 70.84204442501068 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 34.530617\n",
      "\n",
      "Test set: Average loss: 0.8436, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 72.89355855782827 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 42.588669\n",
      "\n",
      "Test set: Average loss: 0.8522, Accuracy: 375/451 (83%)\n",
      "\n",
      "Training so far 74.95452618996302 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 30.830500\n",
      "\n",
      "Test set: Average loss: 0.8051, Accuracy: 375/451 (83%)\n",
      "\n",
      "Training so far 77.03138041098913 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 24.172472\n",
      "\n",
      "Test set: Average loss: 0.8482, Accuracy: 375/451 (83%)\n",
      "\n",
      "Training so far 79.0664192477862 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 9.152660\n",
      "\n",
      "Test set: Average loss: 0.8843, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 81.07305533488592 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 33.918777\n",
      "\n",
      "Test set: Average loss: 0.8248, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 83.07851613362631 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 22.046429\n",
      "\n",
      "Test set: Average loss: 1.0780, Accuracy: 355/451 (79%)\n",
      "\n",
      "Training so far 85.08597931067149 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 36.232681\n",
      "\n",
      "Test set: Average loss: 0.8028, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 87.09335857232412 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 0.293563\n",
      "\n",
      "Test set: Average loss: 0.8516, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 89.11962534189225 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 13.924510\n",
      "\n",
      "Test set: Average loss: 0.9032, Accuracy: 373/451 (83%)\n",
      "\n",
      "Training so far 91.12777148485183 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 29.212185\n",
      "\n",
      "Test set: Average loss: 0.8883, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 93.16186077594757 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 17.959185\n",
      "\n",
      "Test set: Average loss: 0.8929, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 95.18996506134668 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 0.181173\n",
      "\n",
      "Test set: Average loss: 0.8895, Accuracy: 377/451 (84%)\n",
      "\n",
      "Epoch    48: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Training so far 97.21930040121079 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 7.513502\n",
      "\n",
      "Test set: Average loss: 0.8750, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 99.24838085969289 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 6.056810\n",
      "\n",
      "Test set: Average loss: 0.8424, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 101.34232129653294 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 18.433632\n",
      "\n",
      "Test set: Average loss: 0.8557, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 103.4232496658961 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 21.921089\n",
      "\n",
      "Test set: Average loss: 0.8202, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 105.45131081740061 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 19.630478\n",
      "\n",
      "Test set: Average loss: 0.8548, Accuracy: 381/451 (84%)\n",
      "\n",
      "Epoch    53: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Training so far 107.50827287038167 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 20.757307\n",
      "\n",
      "Test set: Average loss: 0.8605, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 109.58437938292822 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 35.943962\n",
      "\n",
      "Test set: Average loss: 0.8455, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 111.6693278392156 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.186679\n",
      "\n",
      "Test set: Average loss: 0.8317, Accuracy: 379/451 (84%)\n",
      "\n",
      "Training so far 113.7190669298172 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 0.158226\n",
      "\n",
      "Test set: Average loss: 0.8285, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 115.75369420448939 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.148952\n",
      "\n",
      "Test set: Average loss: 0.8198, Accuracy: 380/451 (84%)\n",
      "\n",
      "Epoch    58: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Training so far 117.82760004599889 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.046850\n",
      "\n",
      "Test set: Average loss: 0.8208, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 119.94218295017878 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.079060\n",
      "\n",
      "Test set: Average loss: 0.8160, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 122.05601488351822 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.064442\n",
      "\n",
      "Test set: Average loss: 0.8317, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 124.09891832272211 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.110766\n",
      "\n",
      "Test set: Average loss: 0.8340, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 126.16912871201833 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.046747\n",
      "\n",
      "Test set: Average loss: 0.8320, Accuracy: 383/451 (85%)\n",
      "\n",
      "Epoch    63: reducing learning rate of group 0 to 1.5000e-05.\n",
      "Training so far 128.20507068634032 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.069320\n",
      "\n",
      "Test set: Average loss: 0.8204, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 130.30880386829375 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.085652\n",
      "\n",
      "Test set: Average loss: 0.8456, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 132.39151761929193 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 0.050671\n",
      "\n",
      "Test set: Average loss: 0.8324, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 134.5510669986407 minutes\n",
      "====================\n",
      "time spent training: 134.55107080936432 minutes\n",
      "BEST LOSS: 0.8027779421626596\n",
      "BEST ACC: 84.70066518847007\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(label2code)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "results_folder = \"densenet161_mixup_repr_folds\"\n",
    "os.mkdir(f\"tmp/{results_folder}\")\n",
    "\n",
    "for FOLD in range(N_FOLDS):\n",
    "    print(\"PROCESSING FOLD\", FOLD)\n",
    "    model_ft = models.densenet161(pretrained=True)\n",
    "    model_ft.classifier = nn.Sequential(\n",
    "        nn.Linear(model_ft.classifier.in_features, n_classes)\n",
    "    )\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=5e-4)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer_ft, 'min', patience=4, factor=0.5, verbose=True, min_lr=1.5e-5)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(sdf_train_list[FOLD], batch_size=16, shuffle=True,\n",
    "                                               num_workers=6, drop_last=False, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(sdf_val_list[FOLD], batch_size=16, drop_last=False, pin_memory=True,\n",
    "                                        num_workers=6)\n",
    "    \n",
    "    # full train cycle with one fold for cross-val:\n",
    "    model_ft = train_on_fold(\n",
    "        model_ft,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        FOLD,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        lr_scheduler,\n",
    "        results_folder,\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    # predict from one model:\n",
    "    preds = predict_on_val(\n",
    "        model_ft,\n",
    "        val_loader\n",
    "    )\n",
    "    val_folds[FOLD][\"preds\"] = [code2label[c] for c in preds]\n",
    "    \n",
    "    # Save val predictions from one model:\n",
    "    val_out = get_val_outputs(model_ft, val_loader)\n",
    "    gt = val_folds[FOLD].label.map(label2code).values\n",
    "    val_losses = list()\n",
    "    \n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([val_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        val_losses.append(item_loss)\n",
    "    val_losses = np.array(val_losses)\n",
    "    val_folds[FOLD][\"loss\"] = val_losses\n",
    "    val_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=True).to_csv(\n",
    "        f\"tmp/{results_folder}/val_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## for debug, save predictions for train as well:\n",
    "    ### debug train audios:\n",
    "    train_debug_loader = torch.utils.data.DataLoader(sdf_train_list[FOLD], batch_size=32, shuffle=False,\n",
    "                                               num_workers=4, drop_last=False, pin_memory=True)\n",
    "    train_out = get_val_outputs(model_ft, train_debug_loader)\n",
    "    gt = train_folds[FOLD].label.map(label2code).values\n",
    "    train_losses = list()\n",
    "\n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([train_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        train_losses.append(item_loss)\n",
    "\n",
    "    train_losses = np.array(train_losses)\n",
    "    train_folds[FOLD][\"loss\"] = train_losses\n",
    "    train_folds[FOLD][\"preds\"] = [code2label[c] for c in np.argmax(train_out, axis=1)]\n",
    "    train_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=False).to_csv(\n",
    "        f\"tmp/{results_folder}/train_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## MAKE SUBMISSION (from one model):\n",
    "    sample_subm = pd.read_csv(\"Submission1.csv\")\n",
    "    sample_subm[\"image_fn\"] = sample_subm.fn.apply(get_image_path)\n",
    "    subm_dataset = SpectrogramTestDataset([[path, None] for path in sample_subm.image_fn.values ], conf)\n",
    "    subm_loader = torch.utils.data.DataLoader(subm_dataset, batch_size=16)\n",
    "\n",
    "    preds = get_predictions(model_ft, subm_loader)\n",
    "\n",
    "    for c in sample_subm.columns:\n",
    "        if c in {\"fn\", \"image_fn\"}:\n",
    "            continue\n",
    "        c_idx = label2code[c]\n",
    "        sample_subm[c] = preds[:, c_idx]\n",
    "\n",
    "    sample_subm.drop(\"image_fn\", axis=1).to_csv(f'tmp/{results_folder}/subm_{FOLD}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8713968957871397\n",
      "0.8669623059866963\n",
      "0.8470066518847007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i/100) for i in [87.13968957871397,86.69623059866963,84.70066518847007]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAKE ONE SUBMISSION FROM ALL SUBS:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s = f'tmp/{results_folder}/'\n",
    "preds_to_average = [\n",
    "    f\"subm_{f}.csv\"\n",
    "    for f in range(N_FOLDS)\n",
    "]\n",
    "\n",
    "all_subs = list()\n",
    "source_pred = pd.read_csv(s + preds_to_average[0])\n",
    "pred_cols = source_pred.drop(\"fn\", axis=1).columns.values\n",
    "for file in preds_to_average[1:]:\n",
    "    tmp = pd.read_csv(s + file)\n",
    "    source_pred[pred_cols] += tmp[pred_cols]\n",
    "\n",
    "source_pred[pred_cols] /= len(preds_to_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isclose((source_pred[pred_cols].sum(axis=1)).values, np.ones(source_pred.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_pred.to_csv(f'tmp/{results_folder}/{results_folder}_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
