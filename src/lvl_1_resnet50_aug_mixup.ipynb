{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import typing as tp\n",
    "import pathlib\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import IPython.display as ipd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_speech_features as psf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"../data\")\n",
    "audios_path = data_path / \"all_audio_resampled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path/'Train.csv')\n",
    "train_extra = pd.read_csv(data_path/'train_add.csv')\n",
    "train_extra_2 = pd.read_csv(data_path/'train_add_20201029.csv')\n",
    "\n",
    "label2code = {word: idx for idx, word in enumerate(train.label.unique().tolist())}\n",
    "code2label = {v:k for k,v in label2code.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(audio_path):\n",
    "    file_name = audio_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    ip = str(audios_path.resolve() / f\"{file_name}.wav\")\n",
    "    return ip\n",
    "\n",
    "train[\"image_fn\"] = train.fn.apply(get_image_path)\n",
    "train_extra[\"image_fn\"] = train_extra.fn.apply(get_image_path)\n",
    "train_extra_2[\"image_fn\"] = train_extra_2.fn.apply(get_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train, train_extra, train_extra_2], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build validation that includes all classes:\n",
    "\n",
    "\n",
    "vcs = train_df.label.value_counts()\n",
    "\n",
    "## possible schema:\n",
    "# 25+ - take 3\n",
    "# 12-25 - take 2\n",
    "# 12- - take 1\n",
    "\n",
    "def num_for_val(num_examples):\n",
    "    if num_examples >= 25:\n",
    "        return 3\n",
    "    if num_examples >= 12:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "train_df[\"num_examples\"] = train_df.label.map(vcs.to_dict())\n",
    "train_df[\"num_for_val\"] = train_df.num_examples.apply(num_for_val)\n",
    "\n",
    "random.seed(12)\n",
    "train_df_new = pd.DataFrame()\n",
    "for label in train_df.label.unique():\n",
    "    tmp = train_df.loc[train_df.label == label].copy()\n",
    "    tmp[\"dummy\"] = tmp.label.apply(lambda _: random.random())\n",
    "    tmp.sort_values(by=\"dummy\", inplace=True)\n",
    "    tmp[\"rank\"] = range(tmp.shape[0])\n",
    "    train_df_new = pd.concat([train_df_new, tmp])\n",
    "\n",
    "train_df_new.reset_index(drop=True, inplace=True)\n",
    "train_df_new[\"val_subset\"] = train_df_new.num_for_val > train_df_new[\"rank\"]\n",
    "train_df_new.drop(\"dummy\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    set(train_df_new.loc[train_df_new.val_subset].label.unique()) == \n",
    "    set(train_df_new.loc[~train_df_new.val_subset].label.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_new[\"val_fold\"] = train_df_new[\"rank\"] // train_df_new.num_for_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 451\n",
      "1 451\n",
      "2 451\n",
      "3 430\n",
      "4 413\n",
      "5 410\n",
      "6 407\n",
      "7 391\n",
      "8 381\n",
      "9 337\n"
     ]
    }
   ],
   "source": [
    "for f in range(10):\n",
    "    print(f, (train_df_new.val_fold == f).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 3\n",
    "train_folds = list()\n",
    "val_folds = list()\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    valf = train_df_new.loc[train_df_new.val_fold == i].copy()\n",
    "    trf = train_df_new.loc[train_df_new.val_fold != i].copy()\n",
    "    \n",
    "    train_folds.append(trf)\n",
    "    val_folds.append(valf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproduce.src.data_processing import new_generate_spec, new_build_image, normalize, MEAN, STD\n",
    "from reproduce.src.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    n_fft = 512\n",
    "    hop_size = 32\n",
    "    pad_center = True\n",
    "    trim = True\n",
    "    max_len_sec = 2.6\n",
    "    sr = 22050\n",
    "    img_size = 299\n",
    "    \n",
    "conf = AudioConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_noise = AddNoise(0, 0.07)\n",
    "# aug_ts = TimeStretch((0.5, 2))\n",
    "aug_pitch = PitchShift((-5, 5), sr=conf.sr)\n",
    "\n",
    "train_transforms = UseWithProb(\n",
    "    OneOf([\n",
    "        aug_noise,\n",
    "        aug_pitch\n",
    "    ]),\n",
    "    prob=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: tp.List[tp.List[str]],\n",
    "        config,\n",
    "        transform=None,\n",
    "        normalize=True\n",
    "    ):\n",
    "        self.file_list = file_list  # list of list: [file_path, label]\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn, word = self.file_list[idx]\n",
    "        audio, _ = librosa.core.load(fn, sr=SR)\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        image = new_build_image(audio, self.config)\n",
    "        \n",
    "        if self.normalize:\n",
    "            norm_image = normalize(np.array(image), mean=MEAN, std=STD)\n",
    "        else:\n",
    "            norm_image = image\n",
    "        \n",
    "        return np.moveaxis(norm_image, 2, 0), label2code[word]\n",
    "    \n",
    "    \n",
    "class SpectrogramTestDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: tp.List[tp.List[str]],\n",
    "        config,\n",
    "        transform=None,\n",
    "        normalize=True\n",
    "    ):\n",
    "        self.file_list = file_list  # list of list: [file_path, label]\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn, word = self.file_list[idx]\n",
    "        audio, _ = librosa.core.load(fn, sr=SR)\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        image = new_build_image(audio, self.config)\n",
    "        \n",
    "        if self.normalize:\n",
    "            norm_image = normalize(np.array(image), mean=MEAN, std=STD)\n",
    "        else:\n",
    "            norm_image = image\n",
    "        \n",
    "        return np.moveaxis(norm_image, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4258\n",
      "4258\n",
      "4258\n",
      "=========================\n",
      "451\n",
      "451\n",
      "451\n"
     ]
    }
   ],
   "source": [
    "sdf_train_list = [\n",
    "    SpectrogramDataset(t[[\"image_fn\", \"label\"]].values.tolist(), conf,\n",
    "                       transform=train_transforms, normalize=True)\n",
    "    for t in train_folds\n",
    "]\n",
    "\n",
    "sdf_val_list = [\n",
    "    SpectrogramDataset(v[[\"image_fn\", \"label\"]].values.tolist(), conf, normalize=True)\n",
    "    for v in val_folds\n",
    "]\n",
    "\n",
    "for s in sdf_train_list:\n",
    "    print(len(s))\n",
    "    \n",
    "print(\"=========================\")\n",
    "\n",
    "for s in sdf_val_list:\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def train_mixup_epoch(log_interval, mixup_prob, model, device, criterion, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        use_mixup = False\n",
    "        if random.random() < mixup_prob:\n",
    "            use_mixup = True\n",
    "        data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "        \n",
    "        if use_mixup:\n",
    "            data, y_a, y_b, lam = mixup_data(data, target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        if use_mixup:\n",
    "            loss = mixup_criterion(criterion, output, y_a, y_b, lam) #criterion(output, target)\n",
    "        else:\n",
    "            loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "\n",
    "            \n",
    "def test(model, device, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.type(torch.FloatTensor).to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def train_on_fold(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    fold,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    results_folder,\n",
    "    epochs\n",
    "):\n",
    "    seed_dict = {\n",
    "        0: 9,\n",
    "        1: 99,\n",
    "        2: 999\n",
    "    }\n",
    "    set_seed(seed_dict[fold])\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    best_loss = 1e5\n",
    "    best_acc = 0\n",
    "\n",
    "    max_patience = 20\n",
    "    patience = 0\n",
    "\n",
    "    train_loss_hist = list()\n",
    "    val_loss_hist = list()\n",
    "    val_acc_hist = list()\n",
    "\n",
    "    save_each_epoch = False\n",
    "\n",
    "    for ep in range(65):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.667, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{fold}_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "\n",
    "        lr_scheduler.step(cur_loss) \n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "        \n",
    "    for ep in range(10):\n",
    "        train_loss = train_mixup_epoch(1e10, 0.0, model, device, criterion, train_loader, optimizer, ep)\n",
    "        cur_loss, cur_acc = test(model, device, criterion, val_loader)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(cur_loss)\n",
    "        val_acc_hist.append(cur_acc)\n",
    "\n",
    "        if save_each_epoch:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/model_ep_{fold}_{ep}.pth\")\n",
    "\n",
    "        if cur_loss < best_loss:\n",
    "            torch.save(model.state_dict(), f\"tmp/{results_folder}/best_run_{fold}.pth\")\n",
    "            best_loss = cur_loss\n",
    "            best_acc = cur_acc\n",
    "            patience = 0\n",
    "\n",
    "        lr_scheduler.step(cur_loss)\n",
    "        print(\"Training so far {} minutes\".format((time.time() - t0) / 60))\n",
    "        print(\"=\"*20)\n",
    "\n",
    "    print(\"time spent training: {} minutes\".format((time.time() - t0) / 60))\n",
    "    print(\"BEST LOSS:\", best_loss)\n",
    "    print(\"BEST ACC:\", best_acc)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"tmp/{results_folder}/best_run_{fold}.pth\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_on_val(\n",
    "    model,\n",
    "    val_loader,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, (inputs, _) in enumerate(val_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs))\n",
    "\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_val_outputs(model, test_loader, device=\"cuda\"):\n",
    "    outputs_list = list()\n",
    "    for batch_idx, (inputs, target) in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            outputs_list.append(outputs)\n",
    "    outputs_list = np.concatenate([t.cpu().numpy() for t in outputs_list])\n",
    "    return outputs_list\n",
    "\n",
    "def get_predictions(model, test_loader, device=\"cuda\"):\n",
    "    sfm = nn.Softmax()\n",
    "    predictions = list()\n",
    "    for batch_idx, inputs in enumerate(test_loader):\n",
    "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(sfm(outputs)) ## ADD SOFTMAX\n",
    "    predictions = np.concatenate([t.cpu().numpy() for t in predictions])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft.load_state_dict(torch.load(f\"tmp/{tmp_folder_name}/best_run_{FOLD}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FOLD 0\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 172.038345\n",
      "\n",
      "Test set: Average loss: 4.6556, Accuracy: 24/451 (5%)\n",
      "\n",
      "Training so far 1.7080100456873575 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 148.428726\n",
      "\n",
      "Test set: Average loss: 3.7117, Accuracy: 59/451 (13%)\n",
      "\n",
      "Training so far 3.382240867614746 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 141.543137\n",
      "\n",
      "Test set: Average loss: 2.8740, Accuracy: 127/451 (28%)\n",
      "\n",
      "Training so far 5.105073436101278 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 121.851776\n",
      "\n",
      "Test set: Average loss: 2.5296, Accuracy: 170/451 (38%)\n",
      "\n",
      "Training so far 6.736630479494731 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 125.744942\n",
      "\n",
      "Test set: Average loss: 2.5422, Accuracy: 169/451 (37%)\n",
      "\n",
      "Training so far 8.372699813048046 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 50.592739\n",
      "\n",
      "Test set: Average loss: 1.7072, Accuracy: 272/451 (60%)\n",
      "\n",
      "Training so far 10.03559097846349 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 36.970451\n",
      "\n",
      "Test set: Average loss: 1.4853, Accuracy: 295/451 (65%)\n",
      "\n",
      "Training so far 11.696160852909088 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 37.478428\n",
      "\n",
      "Test set: Average loss: 1.3484, Accuracy: 300/451 (67%)\n",
      "\n",
      "Training so far 13.345548248291015 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 114.886246\n",
      "\n",
      "Test set: Average loss: 1.3104, Accuracy: 304/451 (67%)\n",
      "\n",
      "Training so far 15.029002888997395 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 87.003632\n",
      "\n",
      "Test set: Average loss: 1.1643, Accuracy: 331/451 (73%)\n",
      "\n",
      "Training so far 16.684023284912108 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 52.120789\n",
      "\n",
      "Test set: Average loss: 1.1857, Accuracy: 332/451 (74%)\n",
      "\n",
      "Training so far 18.36038145224253 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 108.213913\n",
      "\n",
      "Test set: Average loss: 1.1154, Accuracy: 330/451 (73%)\n",
      "\n",
      "Training so far 20.073738129933677 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 96.296112\n",
      "\n",
      "Test set: Average loss: 1.0942, Accuracy: 332/451 (74%)\n",
      "\n",
      "Training so far 21.700553182760874 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 14.217259\n",
      "\n",
      "Test set: Average loss: 1.0182, Accuracy: 333/451 (74%)\n",
      "\n",
      "Training so far 22.696223803361256 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 24.243729\n",
      "\n",
      "Test set: Average loss: 1.0751, Accuracy: 327/451 (73%)\n",
      "\n",
      "Training so far 23.68105376958847 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 36.483055\n",
      "\n",
      "Test set: Average loss: 0.9920, Accuracy: 340/451 (75%)\n",
      "\n",
      "Training so far 24.699104976654052 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 76.635521\n",
      "\n",
      "Test set: Average loss: 0.9840, Accuracy: 349/451 (77%)\n",
      "\n",
      "Training so far 25.7319598476092 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 15.917395\n",
      "\n",
      "Test set: Average loss: 0.8156, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 26.734108543395998 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 91.136353\n",
      "\n",
      "Test set: Average loss: 0.7176, Accuracy: 374/451 (83%)\n",
      "\n",
      "Training so far 27.714291683832805 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 15.744477\n",
      "\n",
      "Test set: Average loss: 0.7283, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 28.705119065443675 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 10.902434\n",
      "\n",
      "Test set: Average loss: 0.7403, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 29.81861107349396 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 47.826118\n",
      "\n",
      "Test set: Average loss: 0.7494, Accuracy: 371/451 (82%)\n",
      "\n",
      "Training so far 30.882875935236612 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 6.938597\n",
      "\n",
      "Test set: Average loss: 0.7941, Accuracy: 373/451 (83%)\n",
      "\n",
      "Training so far 31.885371069113415 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 14.524812\n",
      "\n",
      "Test set: Average loss: 0.6840, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 32.894370492299394 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 78.019287\n",
      "\n",
      "Test set: Average loss: 0.8262, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 33.90696423451106 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 15.487539\n",
      "\n",
      "Test set: Average loss: 0.7356, Accuracy: 367/451 (81%)\n",
      "\n",
      "Training so far 34.884780085086824 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 17.089155\n",
      "\n",
      "Test set: Average loss: 0.8419, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 35.866625249385834 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 76.157158\n",
      "\n",
      "Test set: Average loss: 0.7055, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 36.883608027299246 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 8.865323\n",
      "\n",
      "Test set: Average loss: 0.6929, Accuracy: 377/451 (84%)\n",
      "\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training so far 37.86148808399836 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 25.268078\n",
      "\n",
      "Test set: Average loss: 0.6243, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 38.83895509640376 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 7.707797\n",
      "\n",
      "Test set: Average loss: 0.5828, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 39.80187289317449 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 4.771245\n",
      "\n",
      "Test set: Average loss: 0.5842, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 40.782210393746695 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 80.474129\n",
      "\n",
      "Test set: Average loss: 0.5821, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 41.78319851954778 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 5.125002\n",
      "\n",
      "Test set: Average loss: 0.6056, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 42.80846353769302 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 41.270786\n",
      "\n",
      "Test set: Average loss: 0.5867, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 43.79570642709732 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 29.806326\n",
      "\n",
      "Test set: Average loss: 0.5717, Accuracy: 395/451 (88%)\n",
      "\n",
      "Training so far 44.77113511164983 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 68.884010\n",
      "\n",
      "Test set: Average loss: 0.6526, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 45.771878226598105 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 74.731155\n",
      "\n",
      "Test set: Average loss: 0.6243, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 46.787872807184854 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 50.517010\n",
      "\n",
      "Test set: Average loss: 0.6114, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 47.76685908238093 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 75.838440\n",
      "\n",
      "Test set: Average loss: 0.5507, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 48.76098751624425 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 82.775421\n",
      "\n",
      "Test set: Average loss: 0.6021, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 49.72309780518214 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 6.277062\n",
      "\n",
      "Test set: Average loss: 0.5467, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 50.697625736395516 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 27.311203\n",
      "\n",
      "Test set: Average loss: 0.6133, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 51.709804487228396 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 61.744427\n",
      "\n",
      "Test set: Average loss: 0.5804, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 52.73103550672531 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 81.391632\n",
      "\n",
      "Test set: Average loss: 0.5930, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 53.73719399372737 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 17.793997\n",
      "\n",
      "Test set: Average loss: 0.5926, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 54.787868189811704 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 76.839241\n",
      "\n",
      "Test set: Average loss: 0.5777, Accuracy: 393/451 (87%)\n",
      "\n",
      "Epoch    47: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Training so far 55.82181142568588 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 1.560896\n",
      "\n",
      "Test set: Average loss: 0.5405, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 56.81316380898158 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 86.444916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5439, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 57.820076155662534 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 39.251205\n",
      "\n",
      "Test set: Average loss: 0.5314, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 58.81813518206278 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 4.157947\n",
      "\n",
      "Test set: Average loss: 0.5339, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 59.805141270160675 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 8.452828\n",
      "\n",
      "Test set: Average loss: 0.5477, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 60.807098321119945 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 0.558549\n",
      "\n",
      "Test set: Average loss: 0.5112, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 61.79184664487839 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 0.246121\n",
      "\n",
      "Test set: Average loss: 0.5312, Accuracy: 398/451 (88%)\n",
      "\n",
      "Training so far 62.77208450635274 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 15.258808\n",
      "\n",
      "Test set: Average loss: 0.5318, Accuracy: 401/451 (89%)\n",
      "\n",
      "Training so far 63.744934697945915 minutes\n",
      "====================\n",
      "Train Epoch: 55 [0/4258 (0%)]\tLoss: 0.458131\n",
      "\n",
      "Test set: Average loss: 0.5563, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 64.71224003235498 minutes\n",
      "====================\n",
      "Train Epoch: 56 [0/4258 (0%)]\tLoss: 10.613818\n",
      "\n",
      "Test set: Average loss: 0.5605, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 65.70421505769094 minutes\n",
      "====================\n",
      "Train Epoch: 57 [0/4258 (0%)]\tLoss: 60.505737\n",
      "\n",
      "Test set: Average loss: 0.5741, Accuracy: 393/451 (87%)\n",
      "\n",
      "Epoch    58: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Training so far 66.7002385020256 minutes\n",
      "====================\n",
      "Train Epoch: 58 [0/4258 (0%)]\tLoss: 71.942490\n",
      "\n",
      "Test set: Average loss: 0.5101, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 67.7043319384257 minutes\n",
      "====================\n",
      "Train Epoch: 59 [0/4258 (0%)]\tLoss: 38.717194\n",
      "\n",
      "Test set: Average loss: 0.5456, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 68.71508922576905 minutes\n",
      "====================\n",
      "Train Epoch: 60 [0/4258 (0%)]\tLoss: 52.092903\n",
      "\n",
      "Test set: Average loss: 0.5129, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 69.69863683780035 minutes\n",
      "====================\n",
      "Train Epoch: 61 [0/4258 (0%)]\tLoss: 25.330215\n",
      "\n",
      "Test set: Average loss: 0.5245, Accuracy: 401/451 (89%)\n",
      "\n",
      "Training so far 70.67781618436177 minutes\n",
      "====================\n",
      "Train Epoch: 62 [0/4258 (0%)]\tLoss: 0.225956\n",
      "\n",
      "Test set: Average loss: 0.5580, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 71.69428972403209 minutes\n",
      "====================\n",
      "Train Epoch: 63 [0/4258 (0%)]\tLoss: 58.020332\n",
      "\n",
      "Test set: Average loss: 0.5378, Accuracy: 392/451 (87%)\n",
      "\n",
      "Epoch    64: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Training so far 72.68769925038019 minutes\n",
      "====================\n",
      "Train Epoch: 64 [0/4258 (0%)]\tLoss: 62.295208\n",
      "\n",
      "Test set: Average loss: 0.5310, Accuracy: 399/451 (88%)\n",
      "\n",
      "Training so far 73.67190720240275 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.364915\n",
      "\n",
      "Test set: Average loss: 0.5197, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 74.66318972508112 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 4.409121\n",
      "\n",
      "Test set: Average loss: 0.5199, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 75.63981300989786 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.212933\n",
      "\n",
      "Test set: Average loss: 0.5014, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 76.6177281777064 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.997158\n",
      "\n",
      "Test set: Average loss: 0.5402, Accuracy: 397/451 (88%)\n",
      "\n",
      "Training so far 77.58730882803599 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 1.416547\n",
      "\n",
      "Test set: Average loss: 0.5254, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 78.54949561357498 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.145014\n",
      "\n",
      "Test set: Average loss: 0.4972, Accuracy: 400/451 (89%)\n",
      "\n",
      "Training so far 79.5195852557818 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 1.038404\n",
      "\n",
      "Test set: Average loss: 0.5363, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 80.48544342517853 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.221452\n",
      "\n",
      "Test set: Average loss: 0.5538, Accuracy: 393/451 (87%)\n",
      "\n",
      "Training so far 81.45112549861273 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 0.203109\n",
      "\n",
      "Test set: Average loss: 0.5473, Accuracy: 396/451 (88%)\n",
      "\n",
      "Training so far 82.44009555180868 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.231478\n",
      "\n",
      "Test set: Average loss: 0.5332, Accuracy: 394/451 (87%)\n",
      "\n",
      "Training so far 83.41664089361826 minutes\n",
      "====================\n",
      "time spent training: 83.41664451758066 minutes\n",
      "BEST LOSS: 0.4971971903037602\n",
      "BEST ACC: 88.69179600886918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/letfoolsdie/virtual_envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/letfoolsdie/virtual_envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:134: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FOLD 1\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 170.355942\n",
      "\n",
      "Test set: Average loss: 4.7667, Accuracy: 26/451 (6%)\n",
      "\n",
      "Training so far 0.9675414681434631 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 141.894745\n",
      "\n",
      "Test set: Average loss: 3.5472, Accuracy: 78/451 (17%)\n",
      "\n",
      "Training so far 1.9283783237139385 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 98.352989\n",
      "\n",
      "Test set: Average loss: 2.7103, Accuracy: 149/451 (33%)\n",
      "\n",
      "Training so far 2.9087892333666483 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 105.105301\n",
      "\n",
      "Test set: Average loss: 2.4682, Accuracy: 193/451 (43%)\n",
      "\n",
      "Training so far 3.8816919763882956 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 70.317078\n",
      "\n",
      "Test set: Average loss: 1.9149, Accuracy: 248/451 (55%)\n",
      "\n",
      "Training so far 4.8583781560262045 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 57.089363\n",
      "\n",
      "Test set: Average loss: 1.6211, Accuracy: 288/451 (64%)\n",
      "\n",
      "Training so far 5.82958261569341 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 39.443642\n",
      "\n",
      "Test set: Average loss: 1.6764, Accuracy: 274/451 (61%)\n",
      "\n",
      "Training so far 6.798398792743683 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 114.442642\n",
      "\n",
      "Test set: Average loss: 1.4621, Accuracy: 302/451 (67%)\n",
      "\n",
      "Training so far 7.766607638200124 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 109.655624\n",
      "\n",
      "Test set: Average loss: 1.3189, Accuracy: 326/451 (72%)\n",
      "\n",
      "Training so far 8.73309117158254 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 80.597572\n",
      "\n",
      "Test set: Average loss: 1.3775, Accuracy: 306/451 (68%)\n",
      "\n",
      "Training so far 9.699399614334107 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 101.166359\n",
      "\n",
      "Test set: Average loss: 1.1388, Accuracy: 328/451 (73%)\n",
      "\n",
      "Training so far 10.680608900388082 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 26.281343\n",
      "\n",
      "Test set: Average loss: 1.1354, Accuracy: 333/451 (74%)\n",
      "\n",
      "Training so far 11.652982950210571 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 20.062551\n",
      "\n",
      "Test set: Average loss: 1.0428, Accuracy: 339/451 (75%)\n",
      "\n",
      "Training so far 12.62316388686498 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 99.077896\n",
      "\n",
      "Test set: Average loss: 1.1242, Accuracy: 344/451 (76%)\n",
      "\n",
      "Training so far 13.595500977834066 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 38.041435\n",
      "\n",
      "Test set: Average loss: 1.0682, Accuracy: 337/451 (75%)\n",
      "\n",
      "Training so far 14.594504002730051 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 17.955915\n",
      "\n",
      "Test set: Average loss: 1.0685, Accuracy: 338/451 (75%)\n",
      "\n",
      "Training so far 15.571496188640594 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 12.583733\n",
      "\n",
      "Test set: Average loss: 0.9406, Accuracy: 353/451 (78%)\n",
      "\n",
      "Training so far 16.539887789885203 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 9.391705\n",
      "\n",
      "Test set: Average loss: 0.8959, Accuracy: 360/451 (80%)\n",
      "\n",
      "Training so far 17.51885406970978 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 66.471344\n",
      "\n",
      "Test set: Average loss: 0.9073, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 18.502291623751322 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 12.952223\n",
      "\n",
      "Test set: Average loss: 0.9183, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 19.479376757144927 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 30.715471\n",
      "\n",
      "Test set: Average loss: 0.8716, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 20.443151410420736 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 104.762253\n",
      "\n",
      "Test set: Average loss: 1.0230, Accuracy: 348/451 (77%)\n",
      "\n",
      "Training so far 21.421297311782837 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 21.297281\n",
      "\n",
      "Test set: Average loss: 0.8503, Accuracy: 372/451 (82%)\n",
      "\n",
      "Training so far 22.391984883944193 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 102.591553\n",
      "\n",
      "Test set: Average loss: 0.8712, Accuracy: 362/451 (80%)\n",
      "\n",
      "Training so far 23.359682619571686 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 59.538071\n",
      "\n",
      "Test set: Average loss: 1.0362, Accuracy: 345/451 (76%)\n",
      "\n",
      "Training so far 24.31850323677063 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 15.080817\n",
      "\n",
      "Test set: Average loss: 0.8124, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 25.28936138947805 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 86.324455\n",
      "\n",
      "Test set: Average loss: 0.7937, Accuracy: 367/451 (81%)\n",
      "\n",
      "Training so far 26.25369162162145 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 55.472500\n",
      "\n",
      "Test set: Average loss: 0.9816, Accuracy: 350/451 (78%)\n",
      "\n",
      "Training so far 27.212530060609183 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 8.572381\n",
      "\n",
      "Test set: Average loss: 0.9075, Accuracy: 361/451 (80%)\n",
      "\n",
      "Training so far 28.179165116945903 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 8.302482\n",
      "\n",
      "Test set: Average loss: 0.7636, Accuracy: 376/451 (83%)\n",
      "\n",
      "Training so far 29.15461498896281 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 63.231010\n",
      "\n",
      "Test set: Average loss: 0.7771, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 30.122038388252257 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 52.625008\n",
      "\n",
      "Test set: Average loss: 0.8561, Accuracy: 368/451 (82%)\n",
      "\n",
      "Training so far 31.08050463994344 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 72.475800\n",
      "\n",
      "Test set: Average loss: 0.8403, Accuracy: 371/451 (82%)\n",
      "\n",
      "Training so far 32.050849517186485 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 69.082581\n",
      "\n",
      "Test set: Average loss: 0.9207, Accuracy: 360/451 (80%)\n",
      "\n",
      "Training so far 33.01462827126185 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 66.254135\n",
      "\n",
      "Test set: Average loss: 0.8255, Accuracy: 376/451 (83%)\n",
      "\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training so far 33.98507870435715 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 39.724682\n",
      "\n",
      "Test set: Average loss: 0.7230, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 34.96252781947454 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 62.771515\n",
      "\n",
      "Test set: Average loss: 0.7136, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 35.92972498734792 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 87.066757\n",
      "\n",
      "Test set: Average loss: 0.7279, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 36.89724298715591 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 1.123790\n",
      "\n",
      "Test set: Average loss: 0.7094, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 37.86840170621872 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 55.817146\n",
      "\n",
      "Test set: Average loss: 0.7549, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 38.83737466335297 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 6.265886\n",
      "\n",
      "Test set: Average loss: 0.7306, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 39.81137935717901 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 2.720491\n",
      "\n",
      "Test set: Average loss: 0.7435, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 40.77696882883708 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 67.869698\n",
      "\n",
      "Test set: Average loss: 0.7378, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 41.750013494491576 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 36.622726\n",
      "\n",
      "Test set: Average loss: 0.7338, Accuracy: 380/451 (84%)\n",
      "\n",
      "Epoch    44: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Training so far 42.71896371841431 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 43.094757\n",
      "\n",
      "Test set: Average loss: 0.6995, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 43.6936490893364 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 4.709995\n",
      "\n",
      "Test set: Average loss: 0.6990, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 44.677670152982074 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 5.040723\n",
      "\n",
      "Test set: Average loss: 0.7116, Accuracy: 392/451 (87%)\n",
      "\n",
      "Training so far 45.669361503918964 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 1.169586\n",
      "\n",
      "Test set: Average loss: 0.7243, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 46.639412542184196 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 4.808378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7213, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 47.63714360793431 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 7.981059\n",
      "\n",
      "Test set: Average loss: 0.7046, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 48.61775028308232 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 0.382614\n",
      "\n",
      "Test set: Average loss: 0.6903, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 49.60714798768361 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 0.265543\n",
      "\n",
      "Test set: Average loss: 0.7037, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 50.61866446336111 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 51.378540\n",
      "\n",
      "Test set: Average loss: 0.7044, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 51.60544813474019 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 69.463409\n",
      "\n",
      "Test set: Average loss: 0.7043, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 52.58300413688024 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 27.468594\n",
      "\n",
      "Test set: Average loss: 0.7268, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 53.552595523993176 minutes\n",
      "====================\n",
      "Train Epoch: 55 [0/4258 (0%)]\tLoss: 38.133652\n",
      "\n",
      "Test set: Average loss: 0.7030, Accuracy: 384/451 (85%)\n",
      "\n",
      "Epoch    56: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Training so far 54.534022851785025 minutes\n",
      "====================\n",
      "Train Epoch: 56 [0/4258 (0%)]\tLoss: 3.686496\n",
      "\n",
      "Test set: Average loss: 0.6888, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 55.521805584430695 minutes\n",
      "====================\n",
      "Train Epoch: 57 [0/4258 (0%)]\tLoss: 8.978678\n",
      "\n",
      "Test set: Average loss: 0.7129, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 56.49632066488266 minutes\n",
      "====================\n",
      "Train Epoch: 58 [0/4258 (0%)]\tLoss: 1.160921\n",
      "\n",
      "Test set: Average loss: 0.7175, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 57.50016983747482 minutes\n",
      "====================\n",
      "Train Epoch: 59 [0/4258 (0%)]\tLoss: 22.370266\n",
      "\n",
      "Test set: Average loss: 0.6942, Accuracy: 390/451 (86%)\n",
      "\n",
      "Training so far 58.49694284200668 minutes\n",
      "====================\n",
      "Train Epoch: 60 [0/4258 (0%)]\tLoss: 0.189637\n",
      "\n",
      "Test set: Average loss: 0.6932, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 59.48539821306864 minutes\n",
      "====================\n",
      "Train Epoch: 61 [0/4258 (0%)]\tLoss: 29.380045\n",
      "\n",
      "Test set: Average loss: 0.7183, Accuracy: 385/451 (85%)\n",
      "\n",
      "Epoch    62: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Training so far 60.4660497546196 minutes\n",
      "====================\n",
      "Train Epoch: 62 [0/4258 (0%)]\tLoss: 76.748169\n",
      "\n",
      "Test set: Average loss: 0.6942, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 61.453389366467796 minutes\n",
      "====================\n",
      "Train Epoch: 63 [0/4258 (0%)]\tLoss: 5.186990\n",
      "\n",
      "Test set: Average loss: 0.7034, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 62.43510024944941 minutes\n",
      "====================\n",
      "Train Epoch: 64 [0/4258 (0%)]\tLoss: 81.965691\n",
      "\n",
      "Test set: Average loss: 0.6871, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 63.443954014778136 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 4.363387\n",
      "\n",
      "Test set: Average loss: 0.6972, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 64.41787174940109 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 4.406197\n",
      "\n",
      "Test set: Average loss: 0.7250, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 65.41065572102865 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 6.179677\n",
      "\n",
      "Test set: Average loss: 0.7034, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 66.40190177758535 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.768869\n",
      "\n",
      "Test set: Average loss: 0.7313, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 67.40419148604074 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 3.519120\n",
      "\n",
      "Test set: Average loss: 0.7584, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 68.40112278461456 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.156984\n",
      "\n",
      "Test set: Average loss: 0.7335, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 69.40525919993719 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.182642\n",
      "\n",
      "Test set: Average loss: 0.7152, Accuracy: 389/451 (86%)\n",
      "\n",
      "Training so far 70.39569522539774 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.899341\n",
      "\n",
      "Test set: Average loss: 0.7455, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 71.3765791217486 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 1.474787\n",
      "\n",
      "Test set: Average loss: 0.7441, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 72.35560086170833 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.724135\n",
      "\n",
      "Test set: Average loss: 0.7247, Accuracy: 388/451 (86%)\n",
      "\n",
      "Training so far 73.35899879535039 minutes\n",
      "====================\n",
      "time spent training: 73.3589990814527 minutes\n",
      "BEST LOSS: 0.6871348310204144\n",
      "BEST ACC: 85.80931263858093\n",
      "PROCESSING FOLD 2\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 167.397614\n",
      "\n",
      "Test set: Average loss: 5.0456, Accuracy: 3/451 (1%)\n",
      "\n",
      "Training so far 0.9861269076665242 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 156.455414\n",
      "\n",
      "Test set: Average loss: 4.6574, Accuracy: 21/451 (5%)\n",
      "\n",
      "Training so far 1.964431063334147 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 152.660294\n",
      "\n",
      "Test set: Average loss: 3.6754, Accuracy: 60/451 (13%)\n",
      "\n",
      "Training so far 2.9422337412834167 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 118.242851\n",
      "\n",
      "Test set: Average loss: 3.1105, Accuracy: 102/451 (23%)\n",
      "\n",
      "Training so far 3.929017460346222 minutes\n",
      "====================\n",
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 136.114532\n",
      "\n",
      "Test set: Average loss: 2.3667, Accuracy: 203/451 (45%)\n",
      "\n",
      "Training so far 4.910580555597941 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 132.577484\n",
      "\n",
      "Test set: Average loss: 2.0696, Accuracy: 239/451 (53%)\n",
      "\n",
      "Training so far 5.9134454925855 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 131.346970\n",
      "\n",
      "Test set: Average loss: 2.0706, Accuracy: 244/451 (54%)\n",
      "\n",
      "Training so far 6.887784735361735 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 58.108467\n",
      "\n",
      "Test set: Average loss: 1.8633, Accuracy: 272/451 (60%)\n",
      "\n",
      "Training so far 7.848135427633921 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 42.358978\n",
      "\n",
      "Test set: Average loss: 1.5720, Accuracy: 293/451 (65%)\n",
      "\n",
      "Training so far 8.833020321528117 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 104.465874\n",
      "\n",
      "Test set: Average loss: 1.9298, Accuracy: 245/451 (54%)\n",
      "\n",
      "Training so far 9.820480020840963 minutes\n",
      "====================\n",
      "Train Epoch: 10 [0/4258 (0%)]\tLoss: 35.036385\n",
      "\n",
      "Test set: Average loss: 1.3673, Accuracy: 317/451 (70%)\n",
      "\n",
      "Training so far 10.8136381427447 minutes\n",
      "====================\n",
      "Train Epoch: 11 [0/4258 (0%)]\tLoss: 106.105392\n",
      "\n",
      "Test set: Average loss: 1.3538, Accuracy: 301/451 (67%)\n",
      "\n",
      "Training so far 11.808946983019512 minutes\n",
      "====================\n",
      "Train Epoch: 12 [0/4258 (0%)]\tLoss: 61.804451\n",
      "\n",
      "Test set: Average loss: 1.1828, Accuracy: 330/451 (73%)\n",
      "\n",
      "Training so far 12.800717147191365 minutes\n",
      "====================\n",
      "Train Epoch: 13 [0/4258 (0%)]\tLoss: 17.028536\n",
      "\n",
      "Test set: Average loss: 1.3139, Accuracy: 320/451 (71%)\n",
      "\n",
      "Training so far 13.781838297843933 minutes\n",
      "====================\n",
      "Train Epoch: 14 [0/4258 (0%)]\tLoss: 70.948914\n",
      "\n",
      "Test set: Average loss: 1.2204, Accuracy: 335/451 (74%)\n",
      "\n",
      "Training so far 14.748937940597534 minutes\n",
      "====================\n",
      "Train Epoch: 15 [0/4258 (0%)]\tLoss: 54.295818\n",
      "\n",
      "Test set: Average loss: 1.1879, Accuracy: 328/451 (73%)\n",
      "\n",
      "Training so far 15.720397488276165 minutes\n",
      "====================\n",
      "Train Epoch: 16 [0/4258 (0%)]\tLoss: 95.678680\n",
      "\n",
      "Test set: Average loss: 1.0888, Accuracy: 342/451 (76%)\n",
      "\n",
      "Training so far 16.691695030530294 minutes\n",
      "====================\n",
      "Train Epoch: 17 [0/4258 (0%)]\tLoss: 14.342079\n",
      "\n",
      "Test set: Average loss: 1.0863, Accuracy: 342/451 (76%)\n",
      "\n",
      "Training so far 17.6753830909729 minutes\n",
      "====================\n",
      "Train Epoch: 18 [0/4258 (0%)]\tLoss: 65.413353\n",
      "\n",
      "Test set: Average loss: 1.0225, Accuracy: 351/451 (78%)\n",
      "\n",
      "Training so far 18.62971749305725 minutes\n",
      "====================\n",
      "Train Epoch: 19 [0/4258 (0%)]\tLoss: 18.606932\n",
      "\n",
      "Test set: Average loss: 1.1005, Accuracy: 341/451 (76%)\n",
      "\n",
      "Training so far 19.60878905852636 minutes\n",
      "====================\n",
      "Train Epoch: 20 [0/4258 (0%)]\tLoss: 60.657181\n",
      "\n",
      "Test set: Average loss: 1.0402, Accuracy: 338/451 (75%)\n",
      "\n",
      "Training so far 20.60804496606191 minutes\n",
      "====================\n",
      "Train Epoch: 21 [0/4258 (0%)]\tLoss: 46.050240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0255, Accuracy: 348/451 (77%)\n",
      "\n",
      "Training so far 21.58089808622996 minutes\n",
      "====================\n",
      "Train Epoch: 22 [0/4258 (0%)]\tLoss: 7.753327\n",
      "\n",
      "Test set: Average loss: 1.0149, Accuracy: 353/451 (78%)\n",
      "\n",
      "Training so far 22.567681992053984 minutes\n",
      "====================\n",
      "Train Epoch: 23 [0/4258 (0%)]\tLoss: 7.176729\n",
      "\n",
      "Test set: Average loss: 0.9204, Accuracy: 365/451 (81%)\n",
      "\n",
      "Training so far 23.547632376352947 minutes\n",
      "====================\n",
      "Train Epoch: 24 [0/4258 (0%)]\tLoss: 32.886639\n",
      "\n",
      "Test set: Average loss: 1.0416, Accuracy: 353/451 (78%)\n",
      "\n",
      "Training so far 24.521318459510802 minutes\n",
      "====================\n",
      "Train Epoch: 25 [0/4258 (0%)]\tLoss: 24.747412\n",
      "\n",
      "Test set: Average loss: 1.0447, Accuracy: 349/451 (77%)\n",
      "\n",
      "Training so far 25.492129472891488 minutes\n",
      "====================\n",
      "Train Epoch: 26 [0/4258 (0%)]\tLoss: 38.221863\n",
      "\n",
      "Test set: Average loss: 0.8925, Accuracy: 360/451 (80%)\n",
      "\n",
      "Training so far 26.483438154061634 minutes\n",
      "====================\n",
      "Train Epoch: 27 [0/4258 (0%)]\tLoss: 102.722504\n",
      "\n",
      "Test set: Average loss: 1.0416, Accuracy: 353/451 (78%)\n",
      "\n",
      "Training so far 27.45859580039978 minutes\n",
      "====================\n",
      "Train Epoch: 28 [0/4258 (0%)]\tLoss: 40.502403\n",
      "\n",
      "Test set: Average loss: 0.9136, Accuracy: 369/451 (82%)\n",
      "\n",
      "Training so far 28.429204698403677 minutes\n",
      "====================\n",
      "Train Epoch: 29 [0/4258 (0%)]\tLoss: 31.683628\n",
      "\n",
      "Test set: Average loss: 0.9879, Accuracy: 357/451 (79%)\n",
      "\n",
      "Training so far 29.406712047259013 minutes\n",
      "====================\n",
      "Train Epoch: 30 [0/4258 (0%)]\tLoss: 89.448105\n",
      "\n",
      "Test set: Average loss: 1.0579, Accuracy: 354/451 (78%)\n",
      "\n",
      "Training so far 30.38686068058014 minutes\n",
      "====================\n",
      "Train Epoch: 31 [0/4258 (0%)]\tLoss: 9.455215\n",
      "\n",
      "Test set: Average loss: 0.9576, Accuracy: 358/451 (79%)\n",
      "\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training so far 31.36072505712509 minutes\n",
      "====================\n",
      "Train Epoch: 32 [0/4258 (0%)]\tLoss: 70.308403\n",
      "\n",
      "Test set: Average loss: 0.7981, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 32.33540230989456 minutes\n",
      "====================\n",
      "Train Epoch: 33 [0/4258 (0%)]\tLoss: 21.620499\n",
      "\n",
      "Test set: Average loss: 0.7768, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 33.305437644322716 minutes\n",
      "====================\n",
      "Train Epoch: 34 [0/4258 (0%)]\tLoss: 6.338502\n",
      "\n",
      "Test set: Average loss: 0.7514, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 34.2784317612648 minutes\n",
      "====================\n",
      "Train Epoch: 35 [0/4258 (0%)]\tLoss: 6.346249\n",
      "\n",
      "Test set: Average loss: 0.7788, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 35.2476198832194 minutes\n",
      "====================\n",
      "Train Epoch: 36 [0/4258 (0%)]\tLoss: 4.208596\n",
      "\n",
      "Test set: Average loss: 0.8253, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 36.218038245042166 minutes\n",
      "====================\n",
      "Train Epoch: 37 [0/4258 (0%)]\tLoss: 86.378357\n",
      "\n",
      "Test set: Average loss: 0.7977, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 37.20826558669408 minutes\n",
      "====================\n",
      "Train Epoch: 38 [0/4258 (0%)]\tLoss: 32.828362\n",
      "\n",
      "Test set: Average loss: 0.8169, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 38.179142200946806 minutes\n",
      "====================\n",
      "Train Epoch: 39 [0/4258 (0%)]\tLoss: 6.109022\n",
      "\n",
      "Test set: Average loss: 0.7386, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 39.143158769607545 minutes\n",
      "====================\n",
      "Train Epoch: 40 [0/4258 (0%)]\tLoss: 12.491465\n",
      "\n",
      "Test set: Average loss: 0.8611, Accuracy: 379/451 (84%)\n",
      "\n",
      "Training so far 40.12818689346314 minutes\n",
      "====================\n",
      "Train Epoch: 41 [0/4258 (0%)]\tLoss: 59.614910\n",
      "\n",
      "Test set: Average loss: 0.8054, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 41.10768646796544 minutes\n",
      "====================\n",
      "Train Epoch: 42 [0/4258 (0%)]\tLoss: 78.974655\n",
      "\n",
      "Test set: Average loss: 0.7945, Accuracy: 377/451 (84%)\n",
      "\n",
      "Training so far 42.080457746982574 minutes\n",
      "====================\n",
      "Train Epoch: 43 [0/4258 (0%)]\tLoss: 6.242343\n",
      "\n",
      "Test set: Average loss: 0.8074, Accuracy: 374/451 (83%)\n",
      "\n",
      "Training so far 43.061737501621245 minutes\n",
      "====================\n",
      "Train Epoch: 44 [0/4258 (0%)]\tLoss: 57.562428\n",
      "\n",
      "Test set: Average loss: 0.7760, Accuracy: 373/451 (83%)\n",
      "\n",
      "Epoch    45: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Training so far 44.019811220963796 minutes\n",
      "====================\n",
      "Train Epoch: 45 [0/4258 (0%)]\tLoss: 5.244913\n",
      "\n",
      "Test set: Average loss: 0.7594, Accuracy: 378/451 (84%)\n",
      "\n",
      "Training so far 44.99608085950216 minutes\n",
      "====================\n",
      "Train Epoch: 46 [0/4258 (0%)]\tLoss: 5.642362\n",
      "\n",
      "Test set: Average loss: 0.7753, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 45.97955628633499 minutes\n",
      "====================\n",
      "Train Epoch: 47 [0/4258 (0%)]\tLoss: 14.083076\n",
      "\n",
      "Test set: Average loss: 0.7813, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 46.938783486684166 minutes\n",
      "====================\n",
      "Train Epoch: 48 [0/4258 (0%)]\tLoss: 23.405245\n",
      "\n",
      "Test set: Average loss: 0.7500, Accuracy: 379/451 (84%)\n",
      "\n",
      "Training so far 47.91785994370778 minutes\n",
      "====================\n",
      "Train Epoch: 49 [0/4258 (0%)]\tLoss: 5.465185\n",
      "\n",
      "Test set: Average loss: 0.7351, Accuracy: 379/451 (84%)\n",
      "\n",
      "Training so far 48.893617304166156 minutes\n",
      "====================\n",
      "Train Epoch: 50 [0/4258 (0%)]\tLoss: 5.832149\n",
      "\n",
      "Test set: Average loss: 0.7737, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 49.87691464424133 minutes\n",
      "====================\n",
      "Train Epoch: 51 [0/4258 (0%)]\tLoss: 40.565498\n",
      "\n",
      "Test set: Average loss: 0.7675, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 50.85529216925303 minutes\n",
      "====================\n",
      "Train Epoch: 52 [0/4258 (0%)]\tLoss: 2.499943\n",
      "\n",
      "Test set: Average loss: 0.7522, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 51.8147287607193 minutes\n",
      "====================\n",
      "Train Epoch: 53 [0/4258 (0%)]\tLoss: 0.579108\n",
      "\n",
      "Test set: Average loss: 0.7465, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 52.78675277630488 minutes\n",
      "====================\n",
      "Train Epoch: 54 [0/4258 (0%)]\tLoss: 80.362442\n",
      "\n",
      "Test set: Average loss: 0.7330, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 53.761902987957 minutes\n",
      "====================\n",
      "Train Epoch: 55 [0/4258 (0%)]\tLoss: 3.807446\n",
      "\n",
      "Test set: Average loss: 0.7636, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 54.733916982014975 minutes\n",
      "====================\n",
      "Train Epoch: 56 [0/4258 (0%)]\tLoss: 0.282891\n",
      "\n",
      "Test set: Average loss: 0.7977, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 55.69857587814331 minutes\n",
      "====================\n",
      "Train Epoch: 57 [0/4258 (0%)]\tLoss: 52.641083\n",
      "\n",
      "Test set: Average loss: 0.7697, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 56.669533725579576 minutes\n",
      "====================\n",
      "Train Epoch: 58 [0/4258 (0%)]\tLoss: 0.641236\n",
      "\n",
      "Test set: Average loss: 0.7502, Accuracy: 385/451 (85%)\n",
      "\n",
      "Training so far 57.64588649670283 minutes\n",
      "====================\n",
      "Train Epoch: 59 [0/4258 (0%)]\tLoss: 71.582489\n",
      "\n",
      "Test set: Average loss: 0.7650, Accuracy: 380/451 (84%)\n",
      "\n",
      "Epoch    60: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Training so far 58.615181080500285 minutes\n",
      "====================\n",
      "Train Epoch: 60 [0/4258 (0%)]\tLoss: 2.770400\n",
      "\n",
      "Test set: Average loss: 0.7403, Accuracy: 381/451 (84%)\n",
      "\n",
      "Training so far 59.59005659023921 minutes\n",
      "====================\n",
      "Train Epoch: 61 [0/4258 (0%)]\tLoss: 53.557495\n",
      "\n",
      "Test set: Average loss: 0.7443, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 60.56236083904902 minutes\n",
      "====================\n",
      "Train Epoch: 62 [0/4258 (0%)]\tLoss: 49.968811\n",
      "\n",
      "Test set: Average loss: 0.7594, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 61.53294891913732 minutes\n",
      "====================\n",
      "Train Epoch: 63 [0/4258 (0%)]\tLoss: 0.399995\n",
      "\n",
      "Test set: Average loss: 0.7374, Accuracy: 380/451 (84%)\n",
      "\n",
      "Training so far 62.507277222474414 minutes\n",
      "====================\n",
      "Train Epoch: 64 [0/4258 (0%)]\tLoss: 2.988984\n",
      "\n",
      "Test set: Average loss: 0.7617, Accuracy: 379/451 (84%)\n",
      "\n",
      "Epoch    65: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Training so far 63.47457201480866 minutes\n",
      "====================\n",
      "Train Epoch: 0 [0/4258 (0%)]\tLoss: 0.342350\n",
      "\n",
      "Test set: Average loss: 0.7208, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 64.44733785788218 minutes\n",
      "====================\n",
      "Train Epoch: 1 [0/4258 (0%)]\tLoss: 0.574960\n",
      "\n",
      "Test set: Average loss: 0.7256, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 65.42863851388296 minutes\n",
      "====================\n",
      "Train Epoch: 2 [0/4258 (0%)]\tLoss: 0.370431\n",
      "\n",
      "Test set: Average loss: 0.7358, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 66.39006483157476 minutes\n",
      "====================\n",
      "Train Epoch: 3 [0/4258 (0%)]\tLoss: 0.919729\n",
      "\n",
      "Test set: Average loss: 0.7314, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 67.36987274090448 minutes\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/4258 (0%)]\tLoss: 0.689750\n",
      "\n",
      "Test set: Average loss: 0.7377, Accuracy: 383/451 (85%)\n",
      "\n",
      "Training so far 68.34244406620661 minutes\n",
      "====================\n",
      "Train Epoch: 5 [0/4258 (0%)]\tLoss: 0.355623\n",
      "\n",
      "Test set: Average loss: 0.7261, Accuracy: 386/451 (86%)\n",
      "\n",
      "Training so far 69.30972306728363 minutes\n",
      "====================\n",
      "Train Epoch: 6 [0/4258 (0%)]\tLoss: 0.722595\n",
      "\n",
      "Test set: Average loss: 0.7243, Accuracy: 382/451 (85%)\n",
      "\n",
      "Training so far 70.28663562138875 minutes\n",
      "====================\n",
      "Train Epoch: 7 [0/4258 (0%)]\tLoss: 0.491879\n",
      "\n",
      "Test set: Average loss: 0.7205, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 71.26583449045818 minutes\n",
      "====================\n",
      "Train Epoch: 8 [0/4258 (0%)]\tLoss: 1.451376\n",
      "\n",
      "Test set: Average loss: 0.7502, Accuracy: 384/451 (85%)\n",
      "\n",
      "Training so far 72.23256327708562 minutes\n",
      "====================\n",
      "Train Epoch: 9 [0/4258 (0%)]\tLoss: 0.143068\n",
      "\n",
      "Test set: Average loss: 0.7463, Accuracy: 387/451 (86%)\n",
      "\n",
      "Training so far 73.21324704885483 minutes\n",
      "====================\n",
      "time spent training: 73.2132474541664 minutes\n",
      "BEST LOSS: 0.7204920280799104\n",
      "BEST ACC: 85.80931263858093\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(label2code)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "results_folder = \"resnet50_aug_mixup_repr_folds\"\n",
    "# os.mkdir(f\"tmp/{results_folder}\")\n",
    "BS = 32\n",
    "for FOLD in range(N_FOLDS):\n",
    "    print(\"PROCESSING FOLD\", FOLD)\n",
    "    \n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "    model_ft.fc = nn.Sequential(\n",
    "        nn.Linear(model_ft.fc.in_features, n_classes)\n",
    "    )\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=2e-4)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer_ft, 'min', patience=4, factor=0.5, verbose=True, min_lr=2e-5)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(sdf_train_list[FOLD], batch_size=BS, shuffle=True,\n",
    "                                               num_workers=6, drop_last=False, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(sdf_val_list[FOLD], batch_size=BS, drop_last=False, pin_memory=True,\n",
    "                                        num_workers=6)\n",
    "    \n",
    "    # full train cycle with one fold for cross-val:\n",
    "    model_ft = train_on_fold(\n",
    "        model_ft,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        FOLD,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        lr_scheduler,\n",
    "        results_folder,\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    # predict from one model:\n",
    "    preds = predict_on_val(\n",
    "        model_ft,\n",
    "        val_loader\n",
    "    )\n",
    "    val_folds[FOLD][\"preds\"] = [code2label[c] for c in preds]\n",
    "    \n",
    "    # Save val predictions from one model:\n",
    "    val_out = get_val_outputs(model_ft, val_loader)\n",
    "    gt = val_folds[FOLD].label.map(label2code).values\n",
    "    val_losses = list()\n",
    "    \n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([val_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        val_losses.append(item_loss)\n",
    "    val_losses = np.array(val_losses)\n",
    "    val_folds[FOLD][\"loss\"] = val_losses\n",
    "    val_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=True).to_csv(\n",
    "        f\"tmp/{results_folder}/val_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## for debug, save predictions for train as well:\n",
    "    ### debug train audios:\n",
    "    train_debug_loader = torch.utils.data.DataLoader(sdf_train_list[FOLD], batch_size=BS, shuffle=False,\n",
    "                                               num_workers=6, drop_last=False, pin_memory=True)\n",
    "    train_out = get_val_outputs(model_ft, train_debug_loader)\n",
    "    gt = train_folds[FOLD].label.map(label2code).values\n",
    "    train_losses = list()\n",
    "\n",
    "    for idx in range(len(gt)):\n",
    "        item_loss = criterion(torch.Tensor([train_out[idx]]), torch.LongTensor([gt[idx]])).numpy()\n",
    "        train_losses.append(item_loss)\n",
    "\n",
    "    train_losses = np.array(train_losses)\n",
    "    train_folds[FOLD][\"loss\"] = train_losses\n",
    "    train_folds[FOLD][\"preds\"] = [code2label[c] for c in np.argmax(train_out, axis=1)]\n",
    "    train_folds[FOLD].reset_index(drop=True).sort_values(by=\"loss\", ascending=False).to_csv(\n",
    "        f\"tmp/{results_folder}/train_loss_{FOLD}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    ## MAKE SUBMISSION (from one model):\n",
    "    sample_subm = pd.read_csv(\"Submission1.csv\")\n",
    "    sample_subm[\"image_fn\"] = sample_subm.fn.apply(get_image_path)\n",
    "    subm_dataset = SpectrogramTestDataset([[path, None] for path in sample_subm.image_fn.values ], conf)\n",
    "    subm_loader = torch.utils.data.DataLoader(subm_dataset, batch_size=16)\n",
    "\n",
    "    preds = get_predictions(model_ft, subm_loader)\n",
    "\n",
    "    for c in sample_subm.columns:\n",
    "        if c in {\"fn\", \"image_fn\"}:\n",
    "            continue\n",
    "        c_idx = label2code[c]\n",
    "        sample_subm[c] = preds[:, c_idx]\n",
    "\n",
    "    sample_subm.drop(\"image_fn\", axis=1).to_csv(f'tmp/{results_folder}/subm_{FOLD}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8869179600886917\n",
      "0.8580931263858094\n",
      "0.8580931263858094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i/100) for i in [88.69179600886918,85.80931263858093,85.80931263858093]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAKE ONE SUBMISSION FROM ALL SUBS:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s = f'tmp/{results_folder}/'\n",
    "preds_to_average = [\n",
    "    f\"subm_{f}.csv\"\n",
    "    for f in range(N_FOLDS)\n",
    "]\n",
    "\n",
    "all_subs = list()\n",
    "source_pred = pd.read_csv(s + preds_to_average[0])\n",
    "pred_cols = source_pred.drop(\"fn\", axis=1).columns.values\n",
    "for file in preds_to_average[1:]:\n",
    "    tmp = pd.read_csv(s + file)\n",
    "    source_pred[pred_cols] += tmp[pred_cols]\n",
    "\n",
    "source_pred[pred_cols] /= len(preds_to_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isclose((source_pred[pred_cols].sum(axis=1)).values, np.ones(source_pred.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_pred.to_csv(f'tmp/{results_folder}/{results_folder}_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
